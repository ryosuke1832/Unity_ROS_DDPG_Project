#!/usr/bin/env python3
"""
DDPGæŠŠæŒåŠ›å¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 

ãƒ‘ã‚¿ãƒ¼ãƒ³1: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ - LSL/TCP ãƒ‡ãƒ¼ã‚¿ã‚’å—ä¿¡ã—ã¦ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§å­¦ç¿’
ãƒ‘ã‚¿ãƒ¼ãƒ³2: é•·æœŸå­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ - äº‹å‰ã«ä½œæˆã—ãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§è‡ªå·±å­¦ç¿’

æ©Ÿèƒ½:
1. åˆ†é¡æ©Ÿï¼ˆgrip_force_classifier.pyï¼‰ã§EEGãƒ‡ãƒ¼ã‚¿ã‚’3ã‚¯ãƒ©ã‚¹åˆ†é¡
2. DDPGã§TCP GripForceã‚’æœ€é©åŒ–
3. unity_tcp_interface.pyã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«æŠŠæŒåŠ›ã‚’å¿œç­”
4. tcp_lsl_sync_systemã§ãƒ‡ãƒ¼ã‚¿åŒæœŸãƒ»åé›†
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import random
import time
import threading
import queue
import json
import os
from collections import deque, namedtuple
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
import pickle

# æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã®æ´»ç”¨
from tcp_lsl_sync_system import LSLTCPEpisodeCollector
from unity_tcp_interface import EEGTCPInterface
from grip_force_classifier import RealtimeGripForceClassifier, load_csv_data

# PyTorchè¨­å®š
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ”§ ãƒ‡ãƒã‚¤ã‚¹: {device}")

# çµŒé¨“ãƒãƒƒãƒ•ã‚¡ç”¨ã®ãƒŠãƒƒãƒ—ãƒ«ã‚¿ã‚¤ãƒ—
Experience = namedtuple('Experience', 
                       ['state', 'action', 'reward', 'next_state', 'done'])

class Actor(nn.Module):
    """DDPG Actor ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆæŠŠæŒåŠ›å‡ºåŠ›ï¼‰"""
    
    def __init__(self, state_dim=5, action_dim=1, hidden_dim=128):
        super(Actor, self).__init__()
        
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)
        
        # HeåˆæœŸåŒ–
        nn.init.kaiming_normal_(self.fc1.weight)
        nn.init.kaiming_normal_(self.fc2.weight)
        nn.init.uniform_(self.fc3.weight, -3e-3, 3e-3)
    
    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        x = torch.tanh(self.fc3(x))  # [-1, 1]
        return x


class Critic(nn.Module):
    """DDPG Critic ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆQå€¤å‡ºåŠ›ï¼‰"""
    
    def __init__(self, state_dim=5, action_dim=1, hidden_dim=128):
        super(Critic, self).__init__()
        
        # State pathway
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        
        # Combined pathway (state + action)
        self.fc2 = nn.Linear(hidden_dim + action_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 1)
        
        # HeåˆæœŸåŒ–
        nn.init.kaiming_normal_(self.fc1.weight)
        nn.init.kaiming_normal_(self.fc2.weight)
        nn.init.uniform_(self.fc3.weight, -3e-3, 3e-3)
    
    def forward(self, state, action):
        x = F.relu(self.fc1(state))
        x = torch.cat([x, action], dim=1)
        x = F.relu(self.fc2(x))
        q_value = self.fc3(x)
        return q_value


class ReplayBuffer:
    """çµŒé¨“å†ç”Ÿãƒãƒƒãƒ•ã‚¡"""
    
    def __init__(self, capacity=100000):
        self.buffer = deque(maxlen=capacity)
        self.capacity = capacity
    
    def push(self, state, action, reward, next_state, done):
        """çµŒé¨“ã‚’è¿½åŠ """
        experience = Experience(state, action, reward, next_state, done)
        self.buffer.append(experience)
    
    def sample(self, batch_size):
        """ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        experiences = random.sample(self.buffer, batch_size)
        
        states = torch.FloatTensor([e.state for e in experiences]).to(device)
        actions = torch.FloatTensor([e.action for e in experiences]).to(device)
        rewards = torch.FloatTensor([e.reward for e in experiences]).to(device)
        next_states = torch.FloatTensor([e.next_state for e in experiences]).to(device)
        dones = torch.BoolTensor([e.done for e in experiences]).to(device)
        
        return states, actions, rewards, next_states, dones
    
    def __len__(self):
        return len(self.buffer)


class DDPGAgent:
    """DDPG ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self, 
                 state_dim=5, 
                 action_dim=1, 
                 lr_actor=1e-4, 
                 lr_critic=1e-3,
                 gamma=0.99,
                 tau=0.001,
                 noise_std=0.2,
                 buffer_capacity=100000):
        
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.tau = tau
        self.noise_std = noise_std
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.actor = Actor(state_dim, action_dim).to(device)
        self.actor_target = Actor(state_dim, action_dim).to(device)
        self.critic = Critic(state_dim, action_dim).to(device)
        self.critic_target = Critic(state_dim, action_dim).to(device)
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®åˆæœŸåŒ–
        self.actor_target.load_state_dict(self.actor.state_dict())
        self.critic_target.load_state_dict(self.critic.state_dict())
        
        # æœ€é©åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)
        
        # çµŒé¨“ãƒãƒƒãƒ•ã‚¡
        self.replay_buffer = ReplayBuffer(buffer_capacity)
        
        # çµ±è¨ˆæƒ…å ±
        self.training_step = 0
        self.episode_count = 0
        
        print(f"ğŸ¤– DDPG ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†")
        print(f"   çŠ¶æ…‹æ¬¡å…ƒ: {state_dim}, è¡Œå‹•æ¬¡å…ƒ: {action_dim}")
        print(f"   å­¦ç¿’ç‡: Actor={lr_actor}, Critic={lr_critic}")
    
    def get_action(self, state, add_noise=True):
        """è¡Œå‹•é¸æŠï¼ˆæŠŠæŒåŠ›å‡ºåŠ›ï¼‰"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
        
        with torch.no_grad():
            action = self.actor(state_tensor).cpu().numpy().flatten()
        
        if add_noise:
            noise = np.random.normal(0, self.noise_std, size=action.shape)
            action = action + noise
        
        # [-1, 1] â†’ [2, 30] ã«å¤‰æ›ï¼ˆæŠŠæŒåŠ›ç¯„å›²ï¼‰
        grip_force = self._convert_action_to_grip_force(action[0])
        return grip_force, action[0]
    
    def _convert_action_to_grip_force(self, action_value):
        """è¡Œå‹•å€¤ã‚’æŠŠæŒåŠ›ã«å¤‰æ›"""
        # action_value: [-1, 1] â†’ grip_force: [2, 30]
        grip_force = 2.0 + (action_value + 1.0) * (30.0 - 2.0) / 2.0
        return np.clip(grip_force, 2.0, 30.0)
    
    def _convert_grip_force_to_action(self, grip_force):
        """æŠŠæŒåŠ›ã‚’è¡Œå‹•å€¤ã«å¤‰æ›"""
        # grip_force: [2, 30] â†’ action_value: [-1, 1]
        action_value = 2.0 * (grip_force - 2.0) / (30.0 - 2.0) - 1.0
        return np.clip(action_value, -1.0, 1.0)
    
    def update(self, batch_size=64):
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ›´æ–°"""
        if len(self.replay_buffer) < batch_size:
            return None, None
        
        # ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)
        
        # Criticæ›´æ–°
        with torch.no_grad():
            next_actions = self.actor_target(next_states)
            target_q = self.critic_target(next_states, next_actions)
            target_q = rewards.unsqueeze(1) + self.gamma * target_q * (~dones).unsqueeze(1)
        
        current_q = self.critic(states, actions.unsqueeze(1))
        critic_loss = F.mse_loss(current_q, target_q)
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()
        
        # Actoræ›´æ–°
        actor_loss = -self.critic(states, self.actor(states)).mean()
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ›´æ–°
        self._soft_update(self.actor_target, self.actor)
        self._soft_update(self.critic_target, self.critic)
        
        self.training_step += 1
        
        return critic_loss.item(), actor_loss.item()
    
    def _soft_update(self, target, source):
        """ã‚½ãƒ•ãƒˆã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ"""
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(target_param.data * (1.0 - self.tau) + param.data * self.tau)
    
    def save_model(self, filepath):
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'actor_optimizer': self.actor_optimizer.state_dict(),
            'critic_optimizer': self.critic_optimizer.state_dict(),
            'training_step': self.training_step,
            'episode_count': self.episode_count
        }, filepath)
        print(f"ğŸ’¾ DDPGãƒ¢ãƒ‡ãƒ«ä¿å­˜: {filepath}")
    
    def load_model(self, filepath):
        """ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        checkpoint = torch.load(filepath, map_location=device)
        
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer'])
        self.training_step = checkpoint.get('training_step', 0)
        self.episode_count = checkpoint.get('episode_count', 0)
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ›´æ–°
        self.actor_target.load_state_dict(self.actor.state_dict())
        self.critic_target.load_state_dict(self.critic.state_dict())
        
        print(f"ğŸ“‚ DDPGãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {filepath}")
        print(f"   å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—: {self.training_step}, ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {self.episode_count}")


class GripForceEnvironmentState:
    """æŠŠæŒåŠ›ç’°å¢ƒã®çŠ¶æ…‹ç®¡ç†"""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        """çŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆ"""
        self.eeg_classification = 1  # 0: UnderGrip, 1: Success, 2: OverGrip
        self.previous_grip_force = 10.0
        self.tcp_grip_force = 0.0
        self.contact = False
        self.episode_step = 0
        return self.get_state()
    
    def get_state(self):
        """ç¾åœ¨ã®çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—"""
        state = np.array([
            self.eeg_classification / 2.0,  # [0, 1] ã«æ­£è¦åŒ–
            self.previous_grip_force / 30.0,  # [0, 1] ã«æ­£è¦åŒ–
            self.tcp_grip_force / 30.0,  # [0, 1] ã«æ­£è¦åŒ–
            1.0 if self.contact else 0.0,
            self.episode_step / 100.0  # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é€²è¡Œåº¦
        ])
        return state
    
    def update(self, eeg_class, grip_force, tcp_data):
        """çŠ¶æ…‹æ›´æ–°"""
        self.eeg_classification = eeg_class
        self.previous_grip_force = grip_force
        self.tcp_grip_force = tcp_data.get('grip_force', 0.0)
        self.contact = tcp_data.get('contact', False)
        self.episode_step += 1
        
        return self.get_state()
    
    def calculate_reward(self, eeg_class, grip_force, tcp_data):
        """å ±é…¬è¨ˆç®—"""
        reward = 0.0
        
        # EEGåˆ†é¡ã«åŸºã¥ãå ±é…¬
        if eeg_class == 1:  # Success
            reward += 10.0
        elif eeg_class == 0:  # UnderGrip
            reward -= 5.0
        elif eeg_class == 2:  # OverGrip
            reward -= 8.0
        
        # TCPæŠŠæŒåŠ›ã¨ã®å·®ã«åŸºã¥ãå ±é…¬
        tcp_force = tcp_data.get('grip_force', 10.0)
        force_diff = abs(grip_force - tcp_force)
        reward -= force_diff * 0.5  # å·®ãŒå¤§ãã„ã»ã©ãƒšãƒŠãƒ«ãƒ†ã‚£
        
        # æ¥è§¦çŠ¶æ…‹ã®å ±é…¬
        if tcp_data.get('contact', False):
            reward += 2.0
        
        # ç ´æãƒšãƒŠãƒ«ãƒ†ã‚£
        if tcp_data.get('broken', False):
            reward -= 15.0
        
        return reward


class DDPGGripForceSystem:
    """DDPGæŠŠæŒåŠ›å¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self,
                 classifier_model_path='models/best_grip_force_classifier.pth',
                 lsl_stream_name='MockEEG',
                 tcp_host='127.0.0.1',
                 tcp_port=12345):
        
        self.classifier_model_path = classifier_model_path
        self.lsl_stream_name = lsl_stream_name
        self.tcp_host = tcp_host
        self.tcp_port = tcp_port
        
        # åˆ†é¡å™¨åˆæœŸåŒ–
        self.classifier = None
        self.init_classifier()
        
        # DDPGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
        self.agent = DDPGAgent(state_dim=5, action_dim=1)
        
        # ç’°å¢ƒçŠ¶æ…‹
        self.env_state = GripForceEnvironmentState()
        
        # ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ 
        self.data_collector = None
        self.tcp_interface = None
        
        # å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰
        self.learning_mode = None  # 'realtime' or 'self_training'
        self.is_running = False
        
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            'total_episodes': 0,
            'total_rewards': [],
            'classification_accuracy': [],
            'grip_force_history': [],
            'start_time': None
        }
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‘ã‚¹
        self.model_save_dir = "models/ddpg"
        os.makedirs(self.model_save_dir, exist_ok=True)
        
        print(f"ğŸš€ DDPGæŠŠæŒåŠ›å¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def init_classifier(self):
        """åˆ†é¡å™¨ã®åˆæœŸåŒ–"""
        try:
            if os.path.exists(self.classifier_model_path):
                self.classifier = RealtimeGripForceClassifier(
                    model_path=self.classifier_model_path,
                    lsl_stream_name=self.lsl_stream_name,
                    tcp_host=self.tcp_host,
                    tcp_port=self.tcp_port
                )
                print(f"âœ… åˆ†é¡å™¨èª­ã¿è¾¼ã¿æˆåŠŸ: {self.classifier_model_path}")
            else:
                print(f"âš ï¸ åˆ†é¡å™¨ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.classifier_model_path}")
                print(f"   grip_force_classifier.py ã§äº‹å‰ã«å­¦ç¿’ã—ã¦ãã ã•ã„")
        except Exception as e:
            print(f"âŒ åˆ†é¡å™¨åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    def start_realtime_learning_mode(self):
        """ãƒ‘ã‚¿ãƒ¼ãƒ³1: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰"""
        print(f"ğŸ”´ ãƒ‘ã‚¿ãƒ¼ãƒ³1: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰é–‹å§‹")
        
        if not self.classifier:
            print(f"âŒ åˆ†é¡å™¨ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return False
        
        self.learning_mode = 'realtime'
        self.is_running = True
        self.stats['start_time'] = time.time()
        
        # ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        self.data_collector = LSLTCPEpisodeCollector(
            lsl_stream_name=self.lsl_stream_name,
            tcp_host=self.tcp_host,
            tcp_port=self.tcp_port,
            save_to_csv=True
        )
        
        # TCPå¿œç­”ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        self.tcp_interface = EEGTCPInterface(
            host=self.tcp_host,
            port=self.tcp_port + 1  # åˆ¥ãƒãƒ¼ãƒˆã§å¿œç­”
        )
        
        # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
        self.tcp_interface.add_message_callback(self._on_grip_force_request)
        
        # ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹
        if not self.data_collector.start_collection():
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹å¤±æ•—")
            return False
        
        if not self.tcp_interface.start_server():
            print(f"âŒ TCPå¿œç­”ã‚µãƒ¼ãƒãƒ¼é–‹å§‹å¤±æ•—")
            return False
        
        # åˆ†é¡å™¨é–‹å§‹
        if not self.classifier.start_classification():
            print(f"âŒ åˆ†é¡å™¨é–‹å§‹å¤±æ•—")
            return False
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’ã‚¹ãƒ¬ãƒƒãƒ‰
        learning_thread = threading.Thread(target=self._realtime_learning_loop, daemon=True)
        learning_thread.start()
        
        print(f"âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰é–‹å§‹å®Œäº†")
        return True
    
    def start_self_training_mode(self, pretrained_model_path=None):
        """ãƒ‘ã‚¿ãƒ¼ãƒ³2: é•·æœŸå­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ï¼ˆè‡ªå·±å­¦ç¿’ï¼‰"""
        print(f"ğŸ”µ ãƒ‘ã‚¿ãƒ¼ãƒ³2: é•·æœŸå­¦ç¿’ãƒ¢ãƒ¼ãƒ‰é–‹å§‹")
        
        self.learning_mode = 'self_training'
        self.is_running = True
        self.stats['start_time'] = time.time()
        
        # äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        if pretrained_model_path and os.path.exists(pretrained_model_path):
            self.agent.load_model(pretrained_model_path)
            print(f"ğŸ“‚ äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {pretrained_model_path}")
        
        # è‡ªå·±å­¦ç¿’ã‚¹ãƒ¬ãƒƒãƒ‰
        self_training_thread = threading.Thread(target=self._self_training_loop, daemon=True)
        self_training_thread.start()
        
        print(f"âœ… é•·æœŸå­¦ç¿’ãƒ¢ãƒ¼ãƒ‰é–‹å§‹å®Œäº†")
        return True
    
    def _on_grip_force_request(self, message_data):
        """æŠŠæŒåŠ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¸ã®å¿œç­”"""
        try:
            # ç¾åœ¨ã®çŠ¶æ…‹ã‚’å–å¾—
            current_state = self.env_state.get_state()
            
            # DDPGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‹ã‚‰æŠŠæŒåŠ›ã‚’å–å¾—
            grip_force, action_value = self.agent.get_action(current_state, add_noise=False)
            
            # å¿œç­”é€ä¿¡
            response = {
                'type': 'grip_force_command',
                'target_force': round(grip_force, 2),
                'timestamp': time.time(),
                'agent_action': round(action_value, 3),
                'learning_mode': self.learning_mode
            }
            
            self.tcp_interface.send_message(response)
            
            # çµ±è¨ˆæ›´æ–°
            self.stats['grip_force_history'].append(grip_force)
            
            print(f"ğŸ¯ æŠŠæŒåŠ›å¿œç­”: {grip_force:.2f}N (è¡Œå‹•å€¤: {action_value:.3f})")
            
        except Exception as e:
            print(f"âŒ æŠŠæŒåŠ›å¿œç­”ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _realtime_learning_loop(self):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’ãƒ«ãƒ¼ãƒ—"""
        print(f"ğŸ”„ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’ãƒ«ãƒ¼ãƒ—é–‹å§‹")
        
        while self.is_running:
            try:
                # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åé›†å¾…æ©Ÿ
                if self.data_collector.episodes:
                    # æ–°ã—ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å‡¦ç†
                    episode = self.data_collector.episodes.pop(0)
                    self._process_episode_for_learning(episode)
                
                time.sleep(0.1)  # 100msé–“éš”
                
            except Exception as e:
                print(f"âš ï¸ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
                time.sleep(1.0)
        
        print(f"ğŸ”„ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’ãƒ«ãƒ¼ãƒ—çµ‚äº†")
    
    def _process_episode_for_learning(self, episode):
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å­¦ç¿’ã«ä½¿ç”¨"""
        try:
            # EEGåˆ†é¡
            eeg_data = episode.lsl_data  # (300, 32)
            eeg_class, confidence = self._classify_eeg_data(eeg_data)
            
            # çŠ¶æ…‹æ›´æ–°
            tcp_data = episode.tcp_data
            grip_force = tcp_data.get('grip_force', 10.0)
            
            # å‰ã®çŠ¶æ…‹
            prev_state = self.env_state.get_state()
            
            # çŠ¶æ…‹æ›´æ–°
            current_state = self.env_state.update(eeg_class, grip_force, tcp_data)
            
            # å ±é…¬è¨ˆç®—
            reward = self.env_state.calculate_reward(eeg_class, grip_force, tcp_data)
            
            # è¡Œå‹•å€¤å¤‰æ›
            action_value = self.agent._convert_grip_force_to_action(grip_force)
            
            # çµŒé¨“ã‚’ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
            done = tcp_data.get('broken', False)
            self.agent.replay_buffer.push(
                prev_state, action_value, reward, current_state, done
            )
            
            # å­¦ç¿’å®Ÿè¡Œ
            if len(self.agent.replay_buffer) > 64:
                critic_loss, actor_loss = self.agent.update()
                
                if critic_loss is not None:
                    print(f"ğŸ“ˆ å­¦ç¿’: EP={episode.episode_id}, "
                          f"åˆ†é¡={['Under', 'Success', 'Over'][eeg_class]}, "
                          f"å ±é…¬={reward:.2f}, "
                          f"Criticæå¤±={critic_loss:.4f}")
            
            # çµ±è¨ˆæ›´æ–°
            self.stats['total_episodes'] += 1
            self.stats['total_rewards'].append(reward)
            self.stats['classification_accuracy'].append(1.0 if eeg_class == 1 else 0.0)
            
            # å®šæœŸçš„ãªãƒ¢ãƒ‡ãƒ«ä¿å­˜
            if self.stats['total_episodes'] % 50 == 0:
                self._save_checkpoint()
                
        except Exception as e:
            print(f"âš ï¸ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _classify_eeg_data(self, eeg_data):
        """EEGãƒ‡ãƒ¼ã‚¿ã®åˆ†é¡"""
        try:
            if self.classifier and hasattr(self.classifier, 'classify_epoch'):
                return self.classifier.classify_epoch(eeg_data)
            else:
                # ãƒ€ãƒŸãƒ¼åˆ†é¡ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
                return random.randint(0, 2), 0.33
        except Exception as e:
            print(f"âš ï¸ EEGåˆ†é¡ã‚¨ãƒ©ãƒ¼: {e}")
            return 1, 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Success
    
    def _self_training_loop(self):
        """è‡ªå·±å­¦ç¿’ãƒ«ãƒ¼ãƒ—ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç’°å¢ƒï¼‰"""
        print(f"ğŸ”„ è‡ªå·±å­¦ç¿’ãƒ«ãƒ¼ãƒ—é–‹å§‹")
        
        episode_count = 0
        
        while self.is_running:
            try:
                episode_count += 1
                episode_reward = 0.0
                episode_steps = 0
                
                # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åˆæœŸåŒ–
                state = self.env_state.reset()
                
                # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Ÿè¡Œ
                for step in range(100):  # æœ€å¤§100ã‚¹ãƒ†ãƒƒãƒ—
                    # è¡Œå‹•é¸æŠ
                    grip_force, action_value = self.agent.get_action(state, add_noise=True)
                    
                    # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç’°å¢ƒã§ã®ã‚¹ãƒ†ãƒƒãƒ—
                    next_state, reward, done = self._simulate_environment_step(
                        state, grip_force, action_value
                    )
                    
                    # çµŒé¨“ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
                    self.agent.replay_buffer.push(
                        state, action_value, reward, next_state, done
                    )
                    
                    episode_reward += reward
                    episode_steps += 1
                    state = next_state
                    
                    # å­¦ç¿’å®Ÿè¡Œ
                    if len(self.agent.replay_buffer) > 64:
                        critic_loss, actor_loss = self.agent.update()
                    
                    if done:
                        break
                
                # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Œäº†
                self.stats['total_episodes'] += 1
                self.stats['total_rewards'].append(episode_reward)
                
                if episode_count % 10 == 0:
                    avg_reward = np.mean(self.stats['total_rewards'][-10:])
                    print(f"ğŸ® è‡ªå·±å­¦ç¿’ EP={episode_count}, "
                          f"å ±é…¬={episode_reward:.2f}, "
                          f"å¹³å‡å ±é…¬={avg_reward:.2f}, "
                          f"ã‚¹ãƒ†ãƒƒãƒ—={episode_steps}")
                
                # å®šæœŸçš„ãªãƒ¢ãƒ‡ãƒ«ä¿å­˜
                if episode_count % 100 == 0:
                    self._save_checkpoint()
                
                time.sleep(0.1)  # çŸ­ã„ä¼‘æ†©
                
            except Exception as e:
                print(f"âš ï¸ è‡ªå·±å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
                time.sleep(1.0)
        
        print(f"ğŸ”„ è‡ªå·±å­¦ç¿’ãƒ«ãƒ¼ãƒ—çµ‚äº†")
    
    def _simulate_environment_step(self, state, grip_force, action_value):
        """ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç’°å¢ƒã§ã®ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ"""
        # ãƒ€ãƒŸãƒ¼ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå®Ÿéš›ã®ç’°å¢ƒã§ã¯ç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
        
        # EEGåˆ†é¡ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        target_force = 12.0  # ç›®æ¨™æŠŠæŒåŠ›
        force_error = abs(grip_force - target_force)
        
        if force_error < 2.0:
            eeg_class = 1  # Success
        elif grip_force < target_force:
            eeg_class = 0  # UnderGrip
        else:
            eeg_class = 2  # OverGrip
        
        # ãƒ€ãƒŸãƒ¼TCPãƒ‡ãƒ¼ã‚¿
        tcp_data = {
            'grip_force': grip_force + random.uniform(-1.0, 1.0),
            'contact': random.random() > 0.3,
            'broken': force_error > 8.0
        }
        
        # çŠ¶æ…‹æ›´æ–°
        next_state = self.env_state.update(eeg_class, grip_force, tcp_data)
        
        # å ±é…¬è¨ˆç®—
        reward = self.env_state.calculate_reward(eeg_class, grip_force, tcp_data)
        
        # çµ‚äº†æ¡ä»¶
        done = tcp_data['broken'] or self.env_state.episode_step > 50
        
        return next_state, reward, done
    
    def _save_checkpoint(self):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # DDPGãƒ¢ãƒ‡ãƒ«ä¿å­˜
        model_path = os.path.join(self.model_save_dir, f"ddpg_agent_{timestamp}.pth")
        self.agent.save_model(model_path)
        
        # çµ±è¨ˆæƒ…å ±ä¿å­˜
        stats_path = os.path.join(self.model_save_dir, f"training_stats_{timestamp}.pkl")
        with open(stats_path, 'wb') as f:
            pickle.dump(self.stats, f)
        
        print(f"ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: {model_path}")
    
    def stop_learning(self):
        """å­¦ç¿’åœæ­¢"""
        print(f"â¹ï¸ DDPGå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ åœæ­¢ä¸­...")
        
        self.is_running = False
        
        # ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ åœæ­¢
        if self.data_collector:
            self.data_collector.stop_collection()
        
        # TCPå¿œç­”ã‚·ã‚¹ãƒ†ãƒ åœæ­¢
        if self.tcp_interface:
            self.tcp_interface.stop_server()
        
        # åˆ†é¡å™¨åœæ­¢
        if self.classifier:
            self.classifier.stop_classification()
        
        # æœ€çµ‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
        self._save_checkpoint()
        
        # çµ±è¨ˆè¡¨ç¤º
        self._print_final_statistics()
        
        print(f"âœ… DDPGå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ åœæ­¢å®Œäº†")
    
    def _print_final_statistics(self):
        """æœ€çµ‚çµ±è¨ˆè¡¨ç¤º"""
        print(f"\nğŸ“Š DDPGå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆ:")
        print(f"=" * 60)
        print(f"   å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰: {self.learning_mode}")
        print(f"   ç·ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {self.stats['total_episodes']}")
        
        if self.stats['total_rewards']:
            print(f"   å¹³å‡å ±é…¬: {np.mean(self.stats['total_rewards']):.3f}")
            print(f"   æœ€é«˜å ±é…¬: {np.max(self.stats['total_rewards']):.3f}")
            print(f"   æœ€æ–°10ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å¹³å‡: {np.mean(self.stats['total_rewards'][-10:]):.3f}")
        
        if self.stats['classification_accuracy']:
            accuracy = np.mean(self.stats['classification_accuracy']) * 100
            print(f"   EEGåˆ†é¡ç²¾åº¦: {accuracy:.1f}%")
        
        if self.stats['grip_force_history']:
            print(f"   å¹³å‡æŠŠæŒåŠ›: {np.mean(self.stats['grip_force_history']):.2f}N")
            print(f"   æŠŠæŒåŠ›ç¯„å›²: {np.min(self.stats['grip_force_history']):.2f} - {np.max(self.stats['grip_force_history']):.2f}N")
        
        if self.stats['start_time']:
            training_time = time.time() - self.stats['start_time']
            print(f"   å­¦ç¿’æ™‚é–“: {training_time:.1f}ç§’")
        
        print(f"   DDPGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—: {self.agent.training_step}")
    
    def test_agent(self, num_episodes=10):
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        print(f"ğŸ§ª DDPG ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ†ã‚¹ãƒˆé–‹å§‹ ({num_episodes}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰)")
        
        test_rewards = []
        
        for episode in range(num_episodes):
            state = self.env_state.reset()
            episode_reward = 0.0
            
            for step in range(50):
                # ãƒã‚¤ã‚ºãªã—ã§è¡Œå‹•é¸æŠ
                grip_force, action_value = self.agent.get_action(state, add_noise=False)
                
                # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒƒãƒ—
                next_state, reward, done = self._simulate_environment_step(
                    state, grip_force, action_value
                )
                
                episode_reward += reward
                state = next_state
                
                if done:
                    break
            
            test_rewards.append(episode_reward)
            print(f"   ãƒ†ã‚¹ãƒˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ {episode+1}: å ±é…¬={episode_reward:.2f}")
        
        avg_test_reward = np.mean(test_rewards)
        print(f"âœ… ãƒ†ã‚¹ãƒˆå®Œäº† - å¹³å‡å ±é…¬: {avg_test_reward:.3f}")
        
        return avg_test_reward


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print(f"ğŸš€ DDPGæŠŠæŒåŠ›å¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ")
    print(f"=" * 60)
    print(f"ãƒ‘ã‚¿ãƒ¼ãƒ³1: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ï¼ˆLSL/TCPé€£æºï¼‰")
    print(f"ãƒ‘ã‚¿ãƒ¼ãƒ³2: é•·æœŸå­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ï¼ˆè‡ªå·±å­¦ç¿’ï¼‰")
    print(f"")
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    ddpg_system = DDPGGripForceSystem(
        classifier_model_path='models/best_grip_force_classifier.pth',
        lsl_stream_name='MockEEG',
        tcp_host='127.0.0.1',
        tcp_port=12345
    )
    
    # ãƒ¢ãƒ¼ãƒ‰é¸æŠ
    print(f"å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„:")
    print(f"1. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ï¼ˆLSL/TCPé€£æºï¼‰")
    print(f"2. é•·æœŸå­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ï¼ˆè‡ªå·±å­¦ç¿’ï¼‰")
    print(f"3. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ†ã‚¹ãƒˆ")
    print(f"4. æ—¢å­˜ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ + ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’")
    print(f"5. æ—¢å­˜ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ + é•·æœŸå­¦ç¿’")
    
    choice = input(f"é¸æŠ (1-5): ").strip()
    
    try:
        if choice == "1":
            # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’
            print(f"\nğŸ”´ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰é–‹å§‹")
            
            if ddpg_system.start_realtime_learning_mode():
                print(f"ğŸ’¡ ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒä¸­...")
                print(f"   LSL/TCPãƒ‡ãƒ¼ã‚¿ã‚’å—ä¿¡ã—ã¦å­¦ç¿’å®Ÿè¡Œ")
                print(f"   Unity ã‹ã‚‰ã®æŠŠæŒåŠ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«è‡ªå‹•å¿œç­”")
                print(f"   Ctrl+C ã§çµ‚äº†")
                
                try:
                    while ddpg_system.is_running:
                        time.sleep(1.0)
                        
                        # å®šæœŸçš„ãªçµ±è¨ˆè¡¨ç¤º
                        if ddpg_system.stats['total_episodes'] > 0 and ddpg_system.stats['total_episodes'] % 20 == 0:
                            recent_rewards = ddpg_system.stats['total_rewards'][-10:]
                            if recent_rewards:
                                avg_reward = np.mean(recent_rewards)
                                print(f"ğŸ“ˆ æœ€æ–°10ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å¹³å‡å ±é…¬: {avg_reward:.3f}")
                        
                except KeyboardInterrupt:
                    print(f"\nâ¹ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼åœæ­¢")
                finally:
                    ddpg_system.stop_learning()
            else:
                print(f"âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰é–‹å§‹å¤±æ•—")
        
        elif choice == "2":
            # ãƒ‘ã‚¿ãƒ¼ãƒ³2: é•·æœŸå­¦ç¿’ï¼ˆè‡ªå·±å­¦ç¿’ï¼‰
            print(f"\nğŸ”µ é•·æœŸå­¦ç¿’ãƒ¢ãƒ¼ãƒ‰é–‹å§‹")
            
            if ddpg_system.start_self_training_mode():
                print(f"ğŸ’¡ è‡ªå·±å­¦ç¿’å®Ÿè¡Œä¸­...")
                print(f"   ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç’°å¢ƒã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå­¦ç¿’")
                print(f"   Ctrl+C ã§çµ‚äº†")
                
                try:
                    while ddpg_system.is_running:
                        time.sleep(5.0)
                        
                        # å®šæœŸçš„ãªé€²æ—è¡¨ç¤º
                        if ddpg_system.stats['total_episodes'] > 0:
                            recent_rewards = ddpg_system.stats['total_rewards'][-20:]
                            if recent_rewards:
                                avg_reward = np.mean(recent_rewards)
                                print(f"ğŸ“ˆ æœ€æ–°20ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å¹³å‡å ±é…¬: {avg_reward:.3f} "
                                      f"(ç·ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {ddpg_system.stats['total_episodes']})")
                        
                except KeyboardInterrupt:
                    print(f"\nâ¹ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼åœæ­¢")
                finally:
                    ddpg_system.stop_learning()
            else:
                print(f"âŒ é•·æœŸå­¦ç¿’ãƒ¢ãƒ¼ãƒ‰é–‹å§‹å¤±æ•—")
        
        elif choice == "3":
            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ†ã‚¹ãƒˆ
            print(f"\nğŸ§ª ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ†ã‚¹ãƒˆ")
            model_path = input(f"ãƒ†ã‚¹ãƒˆç”¨ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ (ç©ºã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ): ").strip()
            
            if model_path and os.path.exists(model_path):
                ddpg_system.agent.load_model(model_path)
                print(f"ğŸ“‚ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {model_path}")
            
            ddpg_system.test_agent(num_episodes=20)
        
        elif choice == "4":
            # æ—¢å­˜ãƒ¢ãƒ‡ãƒ« + ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’
            print(f"\nğŸ”´ æ—¢å­˜ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ + ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’")
            model_path = input(f"æ—¢å­˜DDPGãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: ").strip()
            
            if not model_path or not os.path.exists(model_path):
                print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
                return
            
            ddpg_system.agent.load_model(model_path)
            
            if ddpg_system.start_realtime_learning_mode():
                try:
                    while ddpg_system.is_running:
                        time.sleep(1.0)
                except KeyboardInterrupt:
                    print(f"\nâ¹ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼åœæ­¢")
                finally:
                    ddpg_system.stop_learning()
        
        elif choice == "5":
            # æ—¢å­˜ãƒ¢ãƒ‡ãƒ« + é•·æœŸå­¦ç¿’
            print(f"\nğŸ”µ æ—¢å­˜ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ + é•·æœŸå­¦ç¿’")
            model_path = input(f"æ—¢å­˜DDPGãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: ").strip()
            
            if not model_path or not os.path.exists(model_path):
                print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
                return
            
            if ddpg_system.start_self_training_mode(pretrained_model_path=model_path):
                try:
                    while ddpg_system.is_running:
                        time.sleep(5.0)
                except KeyboardInterrupt:
                    print(f"\nâ¹ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼åœæ­¢")
                finally:
                    ddpg_system.stop_learning()
        
        else:
            print(f"âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")
    
    except Exception as e:
        print(f"âŒ ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"\nğŸ‘‹ DDPGæŠŠæŒåŠ›å¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ çµ‚äº†")


# å˜ä½“ãƒ†ã‚¹ãƒˆç”¨é–¢æ•°
def test_ddpg_components():
    """DDPGã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ãƒ†ã‚¹ãƒˆ"""
    print(f"ğŸ§ª DDPGã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒ†ã‚¹ãƒˆ")
    
    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ†ã‚¹ãƒˆ
    print(f"1. DDPGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ†ã‚¹ãƒˆ")
    agent = DDPGAgent(state_dim=5, action_dim=1)
    
    # ãƒ€ãƒŸãƒ¼çŠ¶æ…‹ã§è¡Œå‹•é¸æŠ
    dummy_state = np.random.randn(5)
    grip_force, action_value = agent.get_action(dummy_state)
    print(f"   ãƒ€ãƒŸãƒ¼çŠ¶æ…‹: {dummy_state}")
    print(f"   å‡ºåŠ›æŠŠæŒåŠ›: {grip_force:.2f}N")
    print(f"   è¡Œå‹•å€¤: {action_value:.3f}")
    
    # çµŒé¨“ãƒãƒƒãƒ•ã‚¡ãƒ†ã‚¹ãƒˆ
    print(f"2. çµŒé¨“ãƒãƒƒãƒ•ã‚¡ãƒ†ã‚¹ãƒˆ")
    for i in range(10):
        state = np.random.randn(5)
        action = np.random.randn()
        reward = np.random.randn()
        next_state = np.random.randn(5)
        done = random.random() > 0.8
        
        agent.replay_buffer.push(state, action, reward, next_state, done)
    
    print(f"   ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º: {len(agent.replay_buffer)}")
    
    # å­¦ç¿’ãƒ†ã‚¹ãƒˆ
    if len(agent.replay_buffer) >= 8:
        print(f"3. å­¦ç¿’ãƒ†ã‚¹ãƒˆ")
        critic_loss, actor_loss = agent.update(batch_size=8)
        print(f"   Criticæå¤±: {critic_loss:.4f}")
        print(f"   Actoræå¤±: {actor_loss:.4f}")
    
    # ç’°å¢ƒçŠ¶æ…‹ãƒ†ã‚¹ãƒˆ
    print(f"4. ç’°å¢ƒçŠ¶æ…‹ãƒ†ã‚¹ãƒˆ")
    env_state = GripForceEnvironmentState()
    state = env_state.reset()
    print(f"   åˆæœŸçŠ¶æ…‹: {state}")
    
    # çŠ¶æ…‹æ›´æ–°ãƒ†ã‚¹ãƒˆ
    dummy_tcp_data = {
        'grip_force': 12.5,
        'contact': True,
        'broken': False
    }
    new_state = env_state.update(1, 11.0, dummy_tcp_data)
    reward = env_state.calculate_reward(1, 11.0, dummy_tcp_data)
    print(f"   æ›´æ–°å¾ŒçŠ¶æ…‹: {new_state}")
    print(f"   è¨ˆç®—å ±é…¬: {reward:.2f}")
    
    print(f"âœ… DDPGã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒ†ã‚¹ãƒˆå®Œäº†")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        test_ddpg_components()
    else:
        main()