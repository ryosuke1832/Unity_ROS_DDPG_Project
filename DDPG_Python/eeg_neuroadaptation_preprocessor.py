#!/usr/bin/env python3
"""
EEGãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

è«–æ–‡ã®å‰å‡¦ç†æ‰‹é †:
1. 250Hz ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
2. 2-50Hz ãƒãƒ³ãƒ‰ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°  
3. Artifact Subspace Reconstruction (ASR)
4. Independent Component Analysis (ICA)
5. ADJUST ã«ã‚ˆã‚‹è‡ªå‹•ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆé™¤å»
6. ã‚¨ãƒãƒƒã‚¯æŠ½å‡º (ã‚¤ãƒ™ãƒ³ãƒˆã‹ã‚‰400mså¾Œã¾ã§)
7. Power Spectral Density (PSD) è¨ˆç®—
8. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸›ç®—

æœ¬å®Ÿè£…ã§ã¯ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ã«é©ã—ãŸç°¡æ˜“ç‰ˆã‚’æä¾›
"""

import numpy as np
import warnings
from scipy import signal
from sklearn.decomposition import FastICA
from collections import deque
import time

class NeuroadaptationEEGPreprocessor:
    """
    è«–æ–‡æº–æ‹ EEGå‰å‡¦ç†ã‚¯ãƒ©ã‚¹ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯¾å¿œç‰ˆï¼‰
    """
    
    def __init__(self, 
                 sampling_rate=250,
                 n_channels=32,
                 epoch_duration=1.2,
                 filter_lowcut=2.0,
                 filter_highcut=50.0,
                 enable_asr=True,
                 enable_ica=False,  # é‡ã„å‡¦ç†ã®ãŸã‚é€šå¸¸ã¯False
                 asr_threshold=5.0):
        """
        åˆæœŸåŒ–
        
        Args:
            sampling_rate: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‘¨æ³¢æ•° (250Hz, è«–æ–‡æº–æ‹ )
            n_channels: ãƒãƒ£ãƒ³ãƒãƒ«æ•° (32, è«–æ–‡æº–æ‹ )  
            epoch_duration: ã‚¨ãƒãƒƒã‚¯é•· (1.2ç§’)
            filter_lowcut: ãƒãƒ³ãƒ‰ãƒ‘ã‚¹ä½åŸŸã‚«ãƒƒãƒˆã‚ªãƒ• (2Hz, è«–æ–‡æº–æ‹ )
            filter_highcut: ãƒãƒ³ãƒ‰ãƒ‘ã‚¹é«˜åŸŸã‚«ãƒƒãƒˆã‚ªãƒ• (50Hz, è«–æ–‡æº–æ‹ )
            enable_asr: ASRæœ‰åŠ¹ãƒ•ãƒ©ã‚°
            enable_ica: ICAæœ‰åŠ¹ãƒ•ãƒ©ã‚° (é‡ã„å‡¦ç†)
            asr_threshold: ASRé–¾å€¤ (sigmaæ•°)
        """
        self.sampling_rate = sampling_rate
        self.n_channels = n_channels
        self.epoch_duration = epoch_duration
        self.epoch_samples = int(epoch_duration * sampling_rate)
        
        # ãƒ•ã‚£ãƒ«ã‚¿è¨­å®š
        self.filter_lowcut = filter_lowcut
        self.filter_highcut = filter_highcut
        
        # å‰å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        self.enable_asr = enable_asr
        self.enable_ica = enable_ica
        self.asr_threshold = asr_threshold
        
        # ãƒãƒ³ãƒ‰ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ä¿‚æ•°ã‚’äº‹å‰è¨ˆç®—
        self._setup_bandpass_filter()
        
        # ICAè¨­å®šï¼ˆæœ‰åŠ¹æ™‚ï¼‰
        if self.enable_ica:
            self.ica = FastICA(n_components=n_channels, random_state=42, max_iter=1000)
            self.ica_fitted = False
        
        # çµ±è¨ˆæƒ…å ±
        self.processing_stats = {
            'total_epochs': 0,
            'asr_rejected_channels': 0,
            'ica_applications': 0,
            'avg_processing_time_ms': 0.0
        }
        
        print(f"ğŸ§  Neuroadaptation EEGå‰å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–:")
        print(f"   ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: {sampling_rate}Hz")
        print(f"   ãƒãƒ£ãƒ³ãƒãƒ«æ•°: {n_channels}ch")
        print(f"   ã‚¨ãƒãƒƒã‚¯é•·: {epoch_duration}s ({self.epoch_samples}samples)")
        print(f"   ãƒãƒ³ãƒ‰ãƒ‘ã‚¹: {filter_lowcut}-{filter_highcut}Hz")
        print(f"   ASRæœ‰åŠ¹: {enable_asr}")
        print(f"   ICAæœ‰åŠ¹: {enable_ica}")
        
    def _setup_bandpass_filter(self):
        """ãƒãƒ³ãƒ‰ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ã®è¨­è¨ˆï¼ˆè«–æ–‡æº–æ‹ : 2-50Hzï¼‰"""
        nyquist = self.sampling_rate / 2
        low = self.filter_lowcut / nyquist
        high = self.filter_highcut / nyquist
        
        # è«–æ–‡ã§ã¯æ˜è¨˜ã•ã‚Œã¦ã„ãªã„ãŒã€5æ¬¡Butterworthã‚’ä½¿ç”¨
        self.filter_order = 5
        self.sos = signal.butter(self.filter_order, [low, high], 
                                btype='band', output='sos')
        
        print(f"   ãƒ•ã‚£ãƒ«ã‚¿è¨­è¨ˆå®Œäº†: {self.filter_order}æ¬¡Butterworth")
    
    def preprocess_epoch(self, epoch_data: np.ndarray) -> dict:
        """
        å˜ä¸€ã‚¨ãƒãƒƒã‚¯ã®å®Œå…¨å‰å‡¦ç†ï¼ˆè«–æ–‡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼‰
        
        Args:
            epoch_data: (samples, channels) or (channels, samples) ã®EEGã‚¨ãƒãƒƒã‚¯
            
        Returns:
            dict: {
                'processed_epoch': å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ (samples, channels),
                'processing_info': å‡¦ç†æƒ…å ±,
                'quality_metrics': å“è³ªæŒ‡æ¨™,
                'rejected_channels': é™¤å»ã•ã‚ŒãŸãƒãƒ£ãƒ³ãƒãƒ«ID,
                'processing_time_ms': å‡¦ç†æ™‚é–“
            }
        """
        start_time = time.time()
        
        # ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ã®æ¨™æº–åŒ–: (samples, channels)
        if epoch_data.ndim != 2:
            raise ValueError(f"ã‚¨ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã¯2Dã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™: {epoch_data.shape}")
        
        # (channels, samples) â†’ (samples, channels) å¤‰æ›ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
        if epoch_data.shape[1] == self.n_channels and epoch_data.shape[0] != self.n_channels:
            # æ—¢ã« (samples, channels) å½¢å¼
            processed_data = epoch_data.copy()
        elif epoch_data.shape[0] == self.n_channels:
            # (channels, samples) â†’ (samples, channels) å¤‰æ›
            processed_data = epoch_data.T.copy()
        else:
            raise ValueError(f"ãƒãƒ£ãƒ³ãƒãƒ«æ•°ãŒä¸æ­£ã§ã™: {epoch_data.shape}, æœŸå¾…å€¤: {self.n_channels}")
        
        processing_info = {
            'original_shape': epoch_data.shape,
            'standardized_shape': processed_data.shape,
            'steps_applied': []
        }
        
        rejected_channels = []
        
        # Step 1: ãƒãƒ³ãƒ‰ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (2-50Hz)
        processed_data = self._apply_bandpass_filter(processed_data)
        processing_info['steps_applied'].append('bandpass_filter')
        
        # Step 2: Artifact Subspace Reconstruction (ASR)
        if self.enable_asr:
            processed_data, asr_rejected = self._apply_asr(processed_data)
            rejected_channels.extend(asr_rejected)
            processing_info['steps_applied'].append('asr')
        
        # Step 3: Independent Component Analysis (ICA) - ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        if self.enable_ica:
            processed_data = self._apply_ica(processed_data)
            processing_info['steps_applied'].append('ica')
        
        # Step 4: æ­£è¦åŒ– (Z-score)
        processed_data = self._apply_zscore_normalization(processed_data)
        processing_info['steps_applied'].append('zscore_normalization')
        
        # Step 5: å“è³ªè©•ä¾¡
        quality_metrics = self._assess_epoch_quality(processed_data)
        
        # çµ±è¨ˆæ›´æ–°
        processing_time_ms = (time.time() - start_time) * 1000
        self.processing_stats['total_epochs'] += 1
        self.processing_stats['asr_rejected_channels'] += len(rejected_channels)
        
        # å‡¦ç†æ™‚é–“ã®ç§»å‹•å¹³å‡
        prev_avg = self.processing_stats['avg_processing_time_ms']
        n = self.processing_stats['total_epochs']
        self.processing_stats['avg_processing_time_ms'] = (prev_avg * (n-1) + processing_time_ms) / n
        
        return {
            'processed_epoch': processed_data,  # (samples, channels)
            'processing_info': processing_info,
            'quality_metrics': quality_metrics,
            'rejected_channels': rejected_channels,
            'processing_time_ms': processing_time_ms
        }
    
    def _apply_bandpass_filter(self, data: np.ndarray) -> np.ndarray:
        """ãƒãƒ³ãƒ‰ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨ (2-50Hz)"""
        filtered_data = np.zeros_like(data)
        
        for ch in range(data.shape[1]):
            try:
                # sosfilt ã‚’ä½¿ç”¨ï¼ˆã‚ˆã‚Šæ•°å€¤çš„ã«å®‰å®šï¼‰
                filtered_data[:, ch] = signal.sosfilt(self.sos, data[:, ch])
            except Exception as e:
                warnings.warn(f"Channel {ch}: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¤±æ•— - {e}")
                filtered_data[:, ch] = data[:, ch]  # å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒ
                
        return filtered_data
    
    def _apply_asr(self, data: np.ndarray) -> tuple[np.ndarray, list]:
        """
        Artifact Subspace Reconstruction (ASR) ã®ç°¡æ˜“å®Ÿè£…
        æ¥µç«¯ãªå¤–ã‚Œå€¤ãƒãƒ£ãƒ³ãƒãƒ«ã®æ¤œå‡ºã¨ä¿®æ­£
        """
        processed_data = data.copy()
        rejected_channels = []
        
        for ch in range(data.shape[1]):
            ch_data = data[:, ch]
            
            if np.std(ch_data) == 0:
                # ç„¡ä¿¡å·ãƒãƒ£ãƒ³ãƒãƒ«
                processed_data[:, ch] = 0
                rejected_channels.append(ch)
                continue
                
            # Z-score ã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º
            z_scores = np.abs((ch_data - np.mean(ch_data)) / np.std(ch_data))
            max_z_score = np.max(z_scores)
            
            if max_z_score > self.asr_threshold:
                # é–¾å€¤ã‚’è¶…ãˆã‚‹å¤–ã‚Œå€¤ãŒå­˜åœ¨
                if max_z_score > self.asr_threshold * 2:
                    # éå¸¸ã«å¤§ããªå¤–ã‚Œå€¤ â†’ ãƒãƒ£ãƒ³ãƒãƒ«å…¨ä½“ã‚’é™¤å»
                    processed_data[:, ch] = 0
                    rejected_channels.append(ch)
                else:
                    # ä¸­ç¨‹åº¦ã®å¤–ã‚Œå€¤ â†’ å¤–ã‚Œå€¤ã‚µãƒ³ãƒ—ãƒ«ã®ã¿é™¤å»
                    outlier_mask = z_scores > self.asr_threshold
                    processed_data[outlier_mask, ch] = np.median(ch_data)
        
        return processed_data, rejected_channels
    
    def _apply_ica(self, data: np.ndarray) -> np.ndarray:
        """Independent Component Analysis (ICA) é©ç”¨"""
        try:
            if not self.ica_fitted:
                # åˆå›å­¦ç¿’
                self.ica.fit(data.T)  # ICAã¯ (features, samples) ã‚’æœŸå¾…
                self.ica_fitted = True
                self.processing_stats['ica_applications'] += 1
            
            # ICAå¤‰æ›ã¨é€†å¤‰æ›ï¼ˆã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆé™¤å»ã®ç°¡æ˜“ç‰ˆï¼‰
            sources = self.ica.transform(data.T)
            
            # ç°¡æ˜“çš„ãªã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆé™¤å»ï¼šæ¥µç«¯ãªæˆåˆ†ã‚’æ¸›è¡°
            for i in range(sources.shape[0]):
                if np.std(sources[i, :]) > 3 * np.mean([np.std(sources[j, :]) for j in range(sources.shape[0])]):
                    sources[i, :] *= 0.1  # 90%æ¸›è¡°
            
            # é€†å¤‰æ›
            cleaned_data = self.ica.inverse_transform(sources).T
            return cleaned_data
            
        except Exception as e:
            warnings.warn(f"ICAå‡¦ç†å¤±æ•—: {e}")
            return data  # ICAå¤±æ•—æ™‚ã¯å…ƒãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
    
    def _apply_zscore_normalization(self, data: np.ndarray) -> np.ndarray:
        """ãƒãƒ£ãƒ³ãƒãƒ«ã”ã¨Z-scoreæ­£è¦åŒ–"""
        normalized_data = np.zeros_like(data)
        
        for ch in range(data.shape[1]):
            ch_data = data[:, ch]
            ch_mean = np.mean(ch_data)
            ch_std = np.std(ch_data)
            
            if ch_std > 1e-10:  # ã‚¼ãƒ­é™¤ç®—å›é¿
                normalized_data[:, ch] = (ch_data - ch_mean) / ch_std
            else:
                normalized_data[:, ch] = ch_data
                
        return normalized_data
    
    def _assess_epoch_quality(self, data: np.ndarray) -> dict:
        """ã‚¨ãƒãƒƒã‚¯å“è³ªè©•ä¾¡"""
        return {
            'snr_db': self._estimate_snr(data),
            'artifact_ratio': self._estimate_artifact_ratio(data),
            'channel_correlation': self._estimate_channel_correlation(data),
            'spectral_quality': self._estimate_spectral_quality(data)
        }
    
    def _estimate_snr(self, data: np.ndarray) -> float:
        """Signal-to-Noise Ratio æ¨å®š"""
        signal_power = np.mean(np.var(data, axis=0))
        noise_floor = np.mean([np.var(np.diff(data[:, ch])) for ch in range(data.shape[1])])
        
        if noise_floor > 0:
            snr_db = 10 * np.log10(signal_power / noise_floor)
        else:
            snr_db = float('inf')
            
        return min(snr_db, 40.0)  # ä¸Šé™è¨­å®š
    
    def _estimate_artifact_ratio(self, data: np.ndarray) -> float:
        """ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆæ¯”ç‡æ¨å®š"""
        total_samples = data.shape[0] * data.shape[1]
        artifact_samples = 0
        
        for ch in range(data.shape[1]):
            z_scores = np.abs((data[:, ch] - np.mean(data[:, ch])) / np.std(data[:, ch]))
            artifact_samples += np.sum(z_scores > 3)
            
        return artifact_samples / total_samples
    
    def _estimate_channel_correlation(self, data: np.ndarray) -> float:
        """ãƒãƒ£ãƒ³ãƒãƒ«é–“ç›¸é–¢ã®å¹³å‡"""
        try:
            corr_matrix = np.corrcoef(data.T)
            # å¯¾è§’æˆåˆ†ã‚’é™¤ã
            mask = ~np.eye(corr_matrix.shape[0], dtype=bool)
            avg_correlation = np.mean(np.abs(corr_matrix[mask]))
            return avg_correlation
        except:
            return 0.0
    
    def _estimate_spectral_quality(self, data: np.ndarray) -> float:
        """ã‚¹ãƒšã‚¯ãƒˆãƒ«å“è³ªè©•ä¾¡"""
        try:
            # å„ãƒãƒ£ãƒ³ãƒãƒ«ã®2-50Hzå¸¯åŸŸãƒ‘ãƒ¯ãƒ¼ã®ä¸€æ§˜æ€§
            freqs, psd = signal.welch(data, fs=self.sampling_rate, axis=0)
            target_band_mask = (freqs >= 2) & (freqs <= 50)
            
            band_powers = np.mean(psd[target_band_mask, :], axis=0)
            spectral_uniformity = 1.0 / (1.0 + np.std(band_powers) / np.mean(band_powers))
            
            return spectral_uniformity
        except:
            return 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    def get_processing_statistics(self) -> dict:
        """å‰å‡¦ç†çµ±è¨ˆæƒ…å ±ã®å–å¾—"""
        return self.processing_stats.copy()
    
    def reset_statistics(self):
        """çµ±è¨ˆæƒ…å ±ã®ãƒªã‚»ãƒƒãƒˆ"""
        self.processing_stats = {
            'total_epochs': 0,
            'asr_rejected_channels': 0,
            'ica_applications': 0,
            'avg_processing_time_ms': 0.0
        }


class StreamingEEGPreprocessor(NeuroadaptationEEGPreprocessor):
    """
    ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”¨å‰å‡¦ç†ã‚¯ãƒ©ã‚¹
    é€£ç¶šãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒªãƒ¼ãƒ ã«å¯¾å¿œ
    """
    
    def __init__(self, *args, buffer_duration=5.0, **kwargs):
        super().__init__(*args, **kwargs)
        
        self.buffer_duration = buffer_duration
        self.buffer_samples = int(buffer_duration * self.sampling_rate)
        
        # é€£ç¶šãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ•ã‚¡
        self.continuous_buffer = deque(maxlen=self.buffer_samples)
        self.timestamps_buffer = deque(maxlen=self.buffer_samples)
        
        print(f"   ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒƒãƒ•ã‚¡: {buffer_duration}s ({self.buffer_samples}samples)")
    
    def add_sample(self, sample: np.ndarray, timestamp: float):
        """
        æ–°ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
        
        Args:
            sample: (n_channels,) ã®å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«
            timestamp: ã‚µãƒ³ãƒ—ãƒ«ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
        """
        if sample.shape[0] != self.n_channels:
            raise ValueError(f"ã‚µãƒ³ãƒ—ãƒ«ã®ãƒãƒ£ãƒ³ãƒãƒ«æ•°ãŒä¸æ­£: {sample.shape[0]}, æœŸå¾…å€¤: {self.n_channels}")
            
        self.continuous_buffer.append(sample.copy())
        self.timestamps_buffer.append(timestamp)
    
    def extract_and_preprocess_epoch(self, center_timestamp: float) -> dict:
        """
        æŒ‡å®šã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å‘¨è¾ºã®ã‚¨ãƒãƒƒã‚¯ã‚’æŠ½å‡ºã—ã¦å‰å‡¦ç†
        
        Args:
            center_timestamp: ã‚¨ãƒãƒƒã‚¯ä¸­å¿ƒã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
            
        Returns:
            å‰å‡¦ç†çµæœ (preprocess_epochã¨åŒã˜å½¢å¼) ã¾ãŸã¯ None
        """
        if len(self.continuous_buffer) < self.epoch_samples:
            return None
        
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‹ã‚‰æœ€é©ãªã‚¨ãƒãƒƒã‚¯ç¯„å›²ã‚’æ±ºå®š
        timestamps = list(self.timestamps_buffer)
        time_diffs = [abs(ts - center_timestamp) for ts in timestamps]
        
        if not time_diffs:
            return None
        
        center_idx = time_diffs.index(min(time_diffs))
        half_epoch = self.epoch_samples // 2
        
        start_idx = max(0, center_idx - half_epoch)
        end_idx = min(len(self.continuous_buffer), start_idx + self.epoch_samples)
        
        # ã‚¨ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        if end_idx - start_idx < self.epoch_samples:
            return None  # ãƒ‡ãƒ¼ã‚¿ä¸è¶³
        
        epoch_data = np.array([self.continuous_buffer[i] for i in range(start_idx, end_idx)])
        
        # å‰å‡¦ç†å®Ÿè¡Œ
        result = self.preprocess_epoch(epoch_data)
        result['extraction_info'] = {
            'center_timestamp': center_timestamp,
            'center_idx': center_idx,
            'epoch_range': (start_idx, end_idx),
            'sync_latency': min(time_diffs)
        }
        
        return result


# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆé–¢æ•°
def demo_preprocessing():
    """å‰å‡¦ç†ãƒ‡ãƒ¢"""
    print("ğŸ§  EEG Neuroadaptation å‰å‡¦ç†ãƒ‡ãƒ¢")
    
    # æ¨¡æ“¬EEGãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ (1.2ç§’ã€32ãƒãƒ£ãƒ³ãƒãƒ«ã€250Hz)
    np.random.seed(42)
    samples = 300  # 1.2ç§’ Ã— 250Hz
    channels = 32
    
    # æ¨¡æ“¬EEGãƒ‡ãƒ¼ã‚¿ï¼ˆãƒã‚¤ã‚º + ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆå«ã‚€ï¼‰
    mock_eeg = np.random.randn(samples, channels) * 10
    
    # ã„ãã¤ã‹ã®ãƒãƒ£ãƒ³ãƒãƒ«ã«ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆè¿½åŠ 
    mock_eeg[:, 5] += np.random.randn(samples) * 50  # å¤§ããªãƒã‚¤ã‚º
    mock_eeg[100:150, 10] = 100  # ã‚¹ãƒ‘ã‚¤ã‚¯ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ
    
    # å‰å‡¦ç†å®Ÿè¡Œ
    preprocessor = NeuroadaptationEEGPreprocessor(
        enable_asr=True,
        enable_ica=False  # ãƒ‡ãƒ¢ã§ã¯ç„¡åŠ¹
    )
    
    result = preprocessor.preprocess_epoch(mock_eeg)
    
    print(f"\nğŸ“Š å‡¦ç†çµæœ:")
    print(f"   å‡¦ç†æ™‚é–“: {result['processing_time_ms']:.2f}ms")
    print(f"   é™¤å»ãƒãƒ£ãƒ³ãƒãƒ«: {result['rejected_channels']}")
    print(f"   å“è³ªæŒ‡æ¨™:")
    for metric, value in result['quality_metrics'].items():
        print(f"     {metric}: {value:.3f}")
    
    print(f"\nğŸ“ˆ çµ±è¨ˆæƒ…å ±:")
    stats = preprocessor.get_processing_statistics()
    for key, value in stats.items():
        print(f"   {key}: {value}")


if __name__ == "__main__":
    demo_preprocessing()