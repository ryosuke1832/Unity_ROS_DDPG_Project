#!/usr/bin/env python3
"""
TypeA DDPGå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ï¼ˆåŒ…æ‹¬çš„ä¿®æ­£ç‰ˆï¼‰

ä¿®æ­£ç‚¹ï¼š
1. æŠŠæŒåŠ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆå—ç†æ¡ä»¶ã®å¤§å¹…æ‹¡å¼µ
2. Collector(12345)â†’DDPG(12346)ã®çŠ¶æ…‹æ©‹æ¸¡ã—æ©Ÿèƒ½è¿½åŠ 
3. å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½å®Ÿè£…
4. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»ãƒ­ã‚°æ©Ÿèƒ½å¼·åŒ–
5. Unityäº’æ›æ€§å‘ä¸Š
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import matplotlib.pyplot as plt
import os
import json
import time
import threading
import queue
import argparse
from datetime import datetime
from collections import deque, namedtuple
from pathlib import Path
from scipy import integrate
import warnings
warnings.filterwarnings('ignore')

# æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from e_tcp_lsl_sync_system import LSLTCPEpisodeCollector, Episode
from c_unity_tcp_interface import EEGTCPInterface

# PyTorchè¨­å®š
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ¯ ãƒ‡ãƒã‚¤ã‚¹: {device}")

# DDPGç”¨ã®çµŒé¨“ãƒãƒƒãƒ•ã‚¡
Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])

class Actor(nn.Module):
    """DDPG Actorãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"""
    def __init__(self, state_dim=4, action_dim=1, hidden_dim=256):
        super().__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)
        
        # HeåˆæœŸåŒ–
        nn.init.kaiming_normal_(self.fc1.weight)
        nn.init.kaiming_normal_(self.fc2.weight)
        nn.init.uniform_(self.fc3.weight, -3e-3, 3e-3)
    
    def forward(self, state):
        x = torch.relu(self.fc1(state))
        x = torch.relu(self.fc2(x))
        x = torch.tanh(self.fc3(x))  # [-1, 1]ã®ç¯„å›²
        return x

class Critic(nn.Module):
    """DDPG Criticãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"""
    def __init__(self, state_dim=4, action_dim=1, hidden_dim=256):
        super().__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim + action_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 1)
        
        nn.init.kaiming_normal_(self.fc1.weight)
        nn.init.kaiming_normal_(self.fc2.weight)
        nn.init.uniform_(self.fc3.weight, -3e-3, 3e-3)
    
    def forward(self, state, action):
        x = torch.relu(self.fc1(state))
        x = torch.cat([x, action], dim=1)
        x = torch.relu(self.fc2(x))
        q_value = self.fc3(x)
        return q_value

class OUNoise:
    """Ornstein-Uhlenbeck ãƒã‚¤ã‚º"""
    def __init__(self, action_dim, mu=0.0, theta=0.15, sigma=0.2):
        self.action_dim = action_dim
        self.mu = mu
        self.theta = theta
        self.sigma = sigma
        self.state = np.ones(self.action_dim) * self.mu
        self.reset()
    
    def reset(self):
        self.state = np.ones(self.action_dim) * self.mu
    
    def sample(self):
        x = self.state
        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))
        self.state = x + dx
        return self.state

class ReplayBuffer:
    """çµŒé¨“å†ç”Ÿãƒãƒƒãƒ•ã‚¡"""
    def __init__(self, capacity=100000):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        experience = Experience(state, action, reward, next_state, done)
        self.buffer.append(experience)
    
    def sample(self, batch_size):
        import random
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = map(np.stack, zip(*batch))
        return state, action, reward, next_state, done
    
    def __len__(self):
        return len(self.buffer)

class AnalysisUtils:
    """åˆ†æç”¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£"""
    
    @staticmethod
    def calculate_auc(y_values):
        """AUCè¨ˆç®—ï¼ˆå°å½¢å‰‡ï¼‰"""
        return float(np.trapz(y_values, dx=1.0))
    
    @staticmethod
    def moving_average(x, window=100):
        """ç§»å‹•å¹³å‡"""
        series = pd.Series(x)
        return series.rolling(window, min_periods=1).mean().values
    
    @staticmethod
    def detect_plateau(y_smooth, window=200, eps=1e-3):
        """plateauæ¤œå‡º"""
        if len(y_smooth) < window:
            return None, None
        
        segment = y_smooth[-window:]
        gradient = np.abs(np.diff(segment)).mean()
        
        if gradient < eps:
            plateau_value = float(np.mean(segment))
            plateau_episode = len(y_smooth) - window
            return plateau_value, plateau_episode
        
        return None, None
    
    @staticmethod
    def find_time_to_threshold(y_values, threshold=0.70):
        """é–¾å€¤åˆ°é”æ™‚é–“"""
        indices = np.where(y_values >= threshold)[0]
        return int(indices[0]) if len(indices) > 0 else None

class SystemHealthChecker:
    """ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯ã‚¯ãƒ©ã‚¹ï¼ˆæ–°è¿½åŠ ï¼‰"""
    
    def __init__(self):
        self.checks = {}
        self.last_check_time = time.time()
        self.check_interval = 10.0  # 10ç§’é–“éš”
    
    def register_check(self, name: str, check_func, critical: bool = False):
        """ãƒã‚§ãƒƒã‚¯é …ç›®ã‚’ç™»éŒ²"""
        self.checks[name] = {
            'func': check_func,
            'critical': critical,
            'last_result': None,
            'last_check': None
        }
    
    def run_health_checks(self) -> dict:
        """å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ"""
        current_time = time.time()
        if current_time - self.last_check_time < self.check_interval:
            return None
        
        results = {}
        critical_failures = []
        
        for name, check_info in self.checks.items():
            try:
                result = check_info['func']()
                check_info['last_result'] = result
                check_info['last_check'] = current_time
                results[name] = result
                
                if check_info['critical'] and not result.get('success', False):
                    critical_failures.append(name)
                    
            except Exception as e:
                error_result = {'success': False, 'error': str(e)}
                check_info['last_result'] = error_result
                results[name] = error_result
                
                if check_info['critical']:
                    critical_failures.append(name)
        
        self.last_check_time = current_time
        
        # é‡è¦ãªå•é¡ŒãŒã‚ã‚‹å ´åˆã¯è­¦å‘Š
        if critical_failures:
            print(f"âš ï¸ é‡è¦ãªå¥å…¨æ€§ãƒã‚§ãƒƒã‚¯å¤±æ•—: {critical_failures}")
        
        return results
    
    def get_status_summary(self) -> str:
        """çŠ¶æ…‹ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        if not self.checks:
            return "â“ ãƒã‚§ãƒƒã‚¯é …ç›®æœªç™»éŒ²"
        
        total = len(self.checks)
        success = sum(1 for check in self.checks.values() 
                     if check['last_result'] and check['last_result'].get('success', False))
        
        if success == total:
            return f"âœ… å¥å…¨ ({success}/{total})"
        elif success > total * 0.7:
            return f"âš ï¸ æ³¨æ„ ({success}/{total})"
        else:
            return f"âŒ å•é¡Œ ({success}/{total})"

class TypeADDPGSystem:
    """TypeA DDPGå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ï¼ˆåŒ…æ‹¬çš„ä¿®æ­£ç‰ˆï¼‰"""
    
    def __init__(self, experiment_type="A_400", seed=42):
        """
        Args:
            experiment_type: "A_400" or "A_long"
            seed: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰
        """
        # ã‚·ãƒ¼ãƒ‰è¨­å®š
        self.seed = seed
        np.random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(seed)
        
        self.experiment_type = experiment_type
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir = f"DDPG_Python/logs/typea_{experiment_type}_seed{seed}_{self.session_id}"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°è¨­å®š
        if experiment_type == "A_400":
            self.target_episodes = 400
        elif experiment_type == "A_long":
            self.target_episodes = 5000
        else:
            self.target_episodes = 400
        
        # å¯¾ç§°æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆFä¸­å¿ƒ11.5Nã€åŠå¹…3.5Nï¼‰
        self.force_center = 11.5  # N
        self.force_halfwidth = 3.5  # N
        self.force_min = self.force_center - self.force_halfwidth  # 8N
        self.force_max = self.force_center + self.force_halfwidth  # 15N
        
        # çŠ¶æ…‹ç©ºé–“è¨­è¨ˆ [force_norm, contact, broken, prev_action]
        self.state_dim = 4
        self.action_dim = 1
        
        # DDPGãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.actor = Actor(self.state_dim, self.action_dim).to(device)
        self.actor_target = Actor(self.state_dim, self.action_dim).to(device)
        self.critic = Critic(self.state_dim, self.action_dim).to(device)
        self.critic_target = Critic(self.state_dim, self.action_dim).to(device)
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆæœŸåŒ–
        self.actor_target.load_state_dict(self.actor.state_dict())
        self.critic_target.load_state_dict(self.critic.state_dict())
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-3)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)
        
        # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.gamma = 0.99
        self.tau = 0.005
        self.batch_size = 64
        
        # çµŒé¨“ãƒãƒƒãƒ•ã‚¡
        self.replay_buffer = ReplayBuffer(capacity=100000)
        
        # æ¢ç´¢ãƒã‚¤ã‚º
        self.noise = OUNoise(self.action_dim, sigma=0.2)
        self.noise_decay = 0.995
        
        # ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ 
        self.episode_collector = None
        
        # TCPé€šä¿¡ï¼ˆä¿®æ­£: auto_reply=False ã§è‡ªå‹•å¿œç­”ã‚’ç„¡åŠ¹åŒ–ï¼‰
        self.tcp_interface = EEGTCPInterface(host='127.0.0.1', port=12346, auto_reply=False)
        
        # å®Ÿè¡Œåˆ¶å¾¡
        self.is_running = False
        self.learning_thread = None
        
        # Pendingæ–¹å¼ç”¨ã®çŠ¶æ…‹ç®¡ç†
        self.pending_state = None
        self.pending_action = None
        self.episode_count = 0  # è¡¨ç¤ºãƒ»é€²æ—ç”¨ã®ã‚«ã‚¦ãƒ³ã‚¿
        
        # â˜… ä¿®æ­£: çŠ¶æ…‹ç®¡ç†å¼·åŒ–ï¼ˆç›´è¿‘ã®ãƒ­ãƒœãƒƒãƒˆçŠ¶æ…‹ã‚’ä¿æŒï¼‰
        self.last_tcp_data = None
        self.last_tcp_data_timestamp = None
        self.collector_state_cache = None  # Collectorå´ã®çŠ¶æ…‹ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        
        # çµ±è¨ˆï¼ˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã”ã¨ï¼‰
        self.episode_data = []  # å„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿
        
        # å ±é…¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.reward_params = {
            'success_reward': 10.0,
            'error_penalty_coeff': 1.0,
            'damage_penalty': 20.0,
            'contact_bonus': 2.0
        }
        
        # â˜… æ–°è¿½åŠ : å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ 
        self.health_checker = SystemHealthChecker()
        self._setup_health_checks()
        
        # â˜… æ–°è¿½åŠ : ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†çµ±è¨ˆ
        self.request_stats = {
            'total_requests': 0,
            'json_requests': 0,
            'text_requests': 0,
            'successful_responses': 0,
            'failed_responses': 0,
            'state_context_available': 0,
            'state_context_missing': 0,
            'collector_fallbacks': 0
        }
        
        print(f"ğŸ¤– TypeA DDPGå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†ï¼ˆåŒ…æ‹¬çš„ä¿®æ­£ç‰ˆï¼‰")
        print(f"   å®Ÿé¨“ã‚¿ã‚¤ãƒ—: {experiment_type}")
        print(f"   ã‚·ãƒ¼ãƒ‰: {seed}")
        print(f"   ç›®æ¨™ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {self.target_episodes}")
        print(f"   å¯¾ç§°æ­£è¦åŒ–: {self.force_min}-{self.force_max}N â†’ [-1,1]")
        print(f"   å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
        print(f"   å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯: æœ‰åŠ¹")
        print(f"   çŠ¶æ…‹æ©‹æ¸¡ã—æ©Ÿèƒ½: æœ‰åŠ¹")
    
    def _setup_health_checks(self):
        """å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯é …ç›®ã®è¨­å®š"""
        
        def check_tcp_connection():
            """TCPæ¥ç¶šçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯"""
            return {
                'success': self.tcp_interface.is_connected,
                'details': f"æ¥ç¶šæ¸ˆã¿: {self.tcp_interface.client_address}" if self.tcp_interface.is_connected else "æœªæ¥ç¶š"
            }
        
        def check_episode_collector():
            """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åé›†ã‚·ã‚¹ãƒ†ãƒ ãƒã‚§ãƒƒã‚¯"""
            if not self.episode_collector:
                return {'success': False, 'details': 'åé›†ã‚·ã‚¹ãƒ†ãƒ æœªåˆæœŸåŒ–'}
            
            return {
                'success': self.episode_collector.is_running,
                'details': f"ç¨¼åƒä¸­, ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {len(self.episode_collector.episodes)}"
            }
        
        def check_state_availability():
            """çŠ¶æ…‹ãƒ‡ãƒ¼ã‚¿å¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯"""
            ddpg_state_age = None
            collector_state_age = None
            
            if self.last_tcp_data_timestamp:
                ddpg_state_age = (time.time() - self.last_tcp_data_timestamp)
            
            if (self.episode_collector and 
                hasattr(self.episode_collector.tcp_interface, 'last_robot_state_timestamp') and
                self.episode_collector.tcp_interface.last_robot_state_timestamp):
                collector_state_age = (time.time() - self.episode_collector.tcp_interface.last_robot_state_timestamp)
            
            has_recent_state = (ddpg_state_age and ddpg_state_age < 30) or (collector_state_age and collector_state_age < 30)
            
            return {
                'success': has_recent_state,
                'details': f"DDPGçŠ¶æ…‹: {ddpg_state_age:.1f}så‰, CollectorçŠ¶æ…‹: {collector_state_age:.1f}så‰" if ddpg_state_age or collector_state_age else "çŠ¶æ…‹ãƒ‡ãƒ¼ã‚¿ãªã—"
            }
        
        def check_learning_progress():
            """å­¦ç¿’é€²æ—ãƒã‚§ãƒƒã‚¯"""
            buffer_ratio = len(self.replay_buffer) / self.replay_buffer.capacity
            return {
                'success': True,
                'details': f"ãƒãƒƒãƒ•ã‚¡åˆ©ç”¨ç‡: {buffer_ratio:.1%}, ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {self.episode_count}/{self.target_episodes}"
            }
        
        # ãƒã‚§ãƒƒã‚¯é …ç›®ç™»éŒ²
        self.health_checker.register_check("tcp_connection", check_tcp_connection, critical=True)
        self.health_checker.register_check("episode_collector", check_episode_collector, critical=True)
        self.health_checker.register_check("state_availability", check_state_availability, critical=False)
        self.health_checker.register_check("learning_progress", check_learning_progress, critical=False)
    
    def _handle_tcp_state(self, message_data):
        """TCPçŠ¶æ…‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†ï¼ˆãƒ­ãƒœãƒƒãƒˆçŠ¶æ…‹ã‚’ä¿æŒï¼‰"""
        try:
            # JSONã®ãƒ­ãƒœãƒƒãƒˆçŠ¶æ…‹ï¼ˆepisode/grip_forceç­‰ï¼‰ãŒæ¥ãŸã¨ãã«æ›´æ–°
            if isinstance(message_data, dict) and 'episode' in message_data and 'grip_force' in message_data:
                self.last_tcp_data = message_data
                self.last_tcp_data_timestamp = time.time()
                print(f"ğŸ“Š DDPGå´ãƒ­ãƒœãƒƒãƒˆçŠ¶æ…‹æ›´æ–°: ep={message_data.get('episode')}, force={message_data.get('grip_force'):.2f}N")
        except Exception as e:
            print(f"âš ï¸ TCPçŠ¶æ…‹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _handle_collector_state_update(self, message_data):
        """Collectorå´çŠ¶æ…‹æ›´æ–°ãƒãƒ³ãƒ‰ãƒ©ï¼ˆæ–°æ©Ÿèƒ½ï¼‰"""
        try:
            if isinstance(message_data, dict) and 'episode' in message_data and 'grip_force' in message_data:
                self.collector_state_cache = message_data.copy()
                print(f"ğŸ”— CollectorçŠ¶æ…‹ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°: ep={message_data.get('episode')}, force={message_data.get('grip_force'):.2f}N")
        except Exception as e:
            print(f"âš ï¸ CollectorçŠ¶æ…‹æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _get_best_available_state(self):
        """æœ€é©ãªåˆ©ç”¨å¯èƒ½çŠ¶æ…‹ã‚’å–å¾—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ï¼‰"""
        current_time = time.time()
        
        # 1. DDPGå´ã®ç›´è¿‘çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯
        if self.last_tcp_data and self.last_tcp_data_timestamp:
            age = current_time - self.last_tcp_data_timestamp
            if age < 30:  # 30ç§’ä»¥å†…
                return self.last_tcp_data, 'ddpg_direct'
        
        # 2. Collectorå´ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯
        if self.collector_state_cache:
            return self.collector_state_cache, 'collector_cache'
        
        # 3. Collectorå´ã®æœ€æ–°çŠ¶æ…‹ã‚’ç›´æ¥å–å¾—
        if self.episode_collector:
            collector_state = self.episode_collector.tcp_interface.get_last_robot_state()
            if collector_state and collector_state['age_ms'] < 30000:  # 30ç§’ä»¥å†…
                self.request_stats['collector_fallbacks'] += 1
                return collector_state['data'], 'collector_direct'
        
        # 4. æœ€çµ‚çš„ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return None, 'none'
    
    def normalize_force(self, force):
        """æŠŠæŒåŠ›ã®å¯¾ç§°æ­£è¦åŒ– [8-15N] â†’ [-1,1]"""
        return (force - self.force_center) / self.force_halfwidth
    
    def denormalize_action(self, action):
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®é€†æ­£è¦åŒ– [-1,1] â†’ [8-15N]"""
        return action * self.force_halfwidth + self.force_center
    
    def calculate_explicit_reward(self, tcp_data):
        """æ˜ç¤ºçš„å ±é…¬ã®è¨ˆç®—"""
        actual_grip_force = tcp_data.get('grip_force', 0.0)
        contact = tcp_data.get('contact', False)
        broken = tcp_data.get('broken', False)
        
        reward = 0.0
        
        # æˆåŠŸå ±é…¬ï¼ˆ8-15Nç¯„å›²å†…ï¼‰
        if self.force_min <= actual_grip_force <= self.force_max:
            reward += self.reward_params['success_reward']
        
        # æŠŠæŒåŠ›èª¤å·®ãƒšãƒŠãƒ«ãƒ†ã‚£
        force_error = abs(actual_grip_force - self.force_center)
        reward -= self.reward_params['error_penalty_coeff'] * force_error
        
        # ç ´æãƒšãƒŠãƒ«ãƒ†ã‚£
        if broken:
            reward -= self.reward_params['damage_penalty']
        
        # æ¥è§¦ãƒœãƒ¼ãƒŠã‚¹
        if contact and not broken:
            reward += self.reward_params['contact_bonus']
        
        return reward
    
    def create_state(self, tcp_data, prev_action):
        """çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®ä½œæˆï¼ˆå¯¾ç§°æ­£è¦åŒ–ï¼‰"""
        grip_force = tcp_data.get('grip_force', self.force_center)
        force_norm = self.normalize_force(grip_force)
        
        contact = 1.0 if tcp_data.get('contact', False) else 0.0
        broken = 1.0 if tcp_data.get('broken', False) else 0.0
        
        state = np.array([
            force_norm,
            contact,
            broken,
            prev_action
        ], dtype=np.float32)
        
        return state
    
    def select_action(self, state, add_noise=True):
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³é¸æŠ"""
        state_tensor = torch.FloatTensor(state.reshape(1, -1)).to(device)
        
        self.actor.eval()
        with torch.no_grad():
            action = self.actor(state_tensor).cpu().data.numpy().flatten()
        
        if add_noise and self.is_running:
            noise_sample = self.noise.sample()
            action += noise_sample
        
        action = np.clip(action, -1.0, 1.0)
        return action
    
    def update_networks(self):
        """DDPGãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ›´æ–°"""
        if len(self.replay_buffer) < self.batch_size:
            return
        
        state, action, reward, next_state, done = self.replay_buffer.sample(self.batch_size)
        
        state = torch.FloatTensor(state).to(device)
        action = torch.FloatTensor(action).to(device)
        reward = torch.FloatTensor(reward).to(device)
        next_state = torch.FloatTensor(next_state).to(device)
        done = torch.FloatTensor(done).to(device)
        
        # Criticã®æ›´æ–°
        next_action = self.actor_target(next_state)
        target_q = self.critic_target(next_state, next_action)
        target_q = reward + (self.gamma * target_q * (1 - done))
        
        current_q = self.critic(state, action)
        critic_loss = F.mse_loss(current_q, target_q.detach())
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=1.0)
        self.critic_optimizer.step()
        
        # Actorã®æ›´æ–°
        actor_loss = -self.critic(state, self.actor(state)).mean()
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1.0)
        self.actor_optimizer.step()
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ã‚½ãƒ•ãƒˆã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ
        self.soft_update(self.actor_target, self.actor, self.tau)
        self.soft_update(self.critic_target, self.critic, self.tau)
        
        # ãƒã‚¤ã‚ºæ¸›è¡°
        self.noise.sigma *= self.noise_decay
        self.noise.sigma = max(self.noise.sigma, 0.01)
        
        return actor_loss.item(), critic_loss.item(), current_q.mean().item()
    
    def soft_update(self, target, source, tau):
        """ã‚½ãƒ•ãƒˆã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ"""
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)
    
    def handle_grip_force_request(self, message_data):
        """Unityã‹ã‚‰ã®æŠŠæŒåŠ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†ï¼ˆåŒ…æ‹¬çš„ä¿®æ­£ç‰ˆï¼‰"""
        try:
            self.request_stats['total_requests'] += 1
            
            # â˜… ä¿®æ­£1: å—ç†æ¡ä»¶ã®å¤§å¹…æ‹¡å¼µï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œï¼‰
            message_type = message_data.get('type', '').lower() if isinstance(message_data, dict) else ''
            
            # JSONå½¢å¼ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆåˆ¤å®šï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
            json_request_types = [
                'grip_force_request',     # æ¨™æº–
                'request_grip_force',     # é€†é †
                'grip_request',           # ç°¡ç•¥
                'force_request',          # ã•ã‚‰ã«ç°¡ç•¥
                'grip_force_command_request',  # è©³ç´°
                'robot_grip_request'      # ãƒ­ãƒœãƒƒãƒˆç”¨
            ]
            
            is_json_request = (isinstance(message_data, dict) and 
                             (message_type in json_request_types or
                              any(keyword in message_type for keyword in ['grip', 'force', 'request'])))
            
            # ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆåˆ¤å®š
            is_text_request = (isinstance(message_data, dict) and 
                             message_data.get('type') == 'text_message' and 
                             message_data.get('content', '').upper().strip() == 'REQUEST_GRIP_FORCE')
            
            # ç›´æ¥ãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
            is_direct_text = (isinstance(message_data, str) and 
                            message_data.upper().strip() == 'REQUEST_GRIP_FORCE')
            
            if not (is_json_request or is_text_request or is_direct_text):
                return  # æŠŠæŒåŠ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ã¯ãªã„
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¿ã‚¤ãƒ—çµ±è¨ˆ
            if is_json_request:
                self.request_stats['json_requests'] += 1
                print(f"ğŸ¯ JSONæŠŠæŒåŠ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ¤œå‡º: {message_type}")
            elif is_text_request or is_direct_text:
                self.request_stats['text_requests'] += 1
                print(f"ğŸ¯ ãƒ†ã‚­ã‚¹ãƒˆæŠŠæŒåŠ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ¤œå‡º: REQUEST_GRIP_FORCE")
            
            # â˜… ä¿®æ­£2: çŠ¶æ…‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å–å¾—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ä»˜ãï¼‰
            tcp_data, source = self._get_best_available_state()
            
            if tcp_data:
                self.request_stats['state_context_available'] += 1
                episode = tcp_data.get('episode', 'unknown')
                grip_force = tcp_data.get('grip_force', 'unknown')
                print(f"ğŸ“Š çŠ¶æ…‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—æˆåŠŸ [{source}]: ep={episode}, force={grip_force}")
            else:
                self.request_stats['state_context_missing'] += 1
                print(f"âš ï¸ çŠ¶æ…‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—å¤±æ•— - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆçŠ¶æ…‹ã‚’ä½¿ç”¨")
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆçŠ¶æ…‹ä½œæˆ
                tcp_data = {
                    'grip_force': self.force_center,
                    'contact': False,
                    'broken': False,
                    'episode': self.episode_count
                }
            
            # å‰å›ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å€¤
            prev_action = 0.0 if self.pending_action is None else self.pending_action[0]
            
            # çŠ¶æ…‹ä½œæˆ
            state = self.create_state(tcp_data, prev_action)
            
            # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³é¸æŠ
            action = self.select_action(state, add_noise=True)
            
            # PendingçŠ¶æ…‹ã‚’ä¿å­˜ï¼ˆK=1è¨­è¨ˆï¼‰
            self.pending_state = state
            self.pending_action = action
            
            # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠŠæŒåŠ›ã«å¤‰æ›
            grip_force = self.denormalize_action(action[0])
            grip_force = np.clip(grip_force, 5.0, 25.0)  # å®‰å…¨ã‚¯ãƒ©ãƒ³ãƒ—
            
            print(f"ğŸ¤– TypeAæŠŠæŒåŠ›æ±ºå®š: {grip_force:.2f}N (action: {action[0]:.3f}, noise_Ïƒ: {self.noise.sigma:.3f})")
            print(f"   çŠ¶æ…‹ã‚½ãƒ¼ã‚¹: {source}, ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {tcp_data.get('episode')}")
            
            # â˜… ä¿®æ­£3: Unityäº’æ›æ€§ã‚’è€ƒæ…®ã—ãŸTCPå¿œç­”é€ä¿¡
            response = self._create_grip_force_response(grip_force, tcp_data, source)
            
            success = self.tcp_interface.send_message(response)
            
            if success:
                self.request_stats['successful_responses'] += 1
                print(f"âœ… æŠŠæŒåŠ›ã‚³ãƒãƒ³ãƒ‰é€ä¿¡æˆåŠŸ")
            else:
                self.request_stats['failed_responses'] += 1
                print(f"âŒ æŠŠæŒåŠ›ã‚³ãƒãƒ³ãƒ‰é€ä¿¡å¤±æ•—")
            
        except Exception as e:
            self.request_stats['failed_responses'] += 1
            print(f"âŒ æŠŠæŒåŠ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
    
    def _create_grip_force_response(self, grip_force, tcp_data, source):
        """æŠŠæŒåŠ›å¿œç­”ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä½œæˆï¼ˆUnityäº’æ›æ€§è€ƒæ…®ï¼‰"""
        response = {
            'type': 'grip_force_command',
            'target_force': float(grip_force),     # Pythonæ¨™æº–
            'targetForce': float(grip_force),      # Unity C# ã‚­ãƒ£ãƒ¡ãƒ«ã‚±ãƒ¼ã‚¹
            'force': float(grip_force),            # ç°¡æ˜“å½¢å¼
            'timestamp': time.time(),
            'session_id': f"typea_{self.experiment_type}_seed{self.seed}_{int(time.time())}",
            'episode_number': tcp_data.get('episode', self.episode_count),
            'episodeNumber': tcp_data.get('episode', self.episode_count),  # ã‚­ãƒ£ãƒ¡ãƒ«ã‚±ãƒ¼ã‚¹
            'state_source': source,
            'learning_episode': len(self.episode_data),
            'experiment_type': self.experiment_type,
            'seed': self.seed
        }
        
        return response
    
    def run_learning(self):
        """TypeA DDPGå­¦ç¿’å®Ÿè¡Œï¼ˆåŒ…æ‹¬çš„ä¿®æ­£ç‰ˆï¼‰"""
        print(f"ğŸš€ TypeA DDPGå­¦ç¿’é–‹å§‹ï¼ˆåŒ…æ‹¬çš„ä¿®æ­£ç‰ˆï¼‰ ({self.experiment_type}, seed={self.seed})")
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åé›†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ï¼ˆauto_reply=False ã§è‡ªå‹•å¿œç­”ã‚’ç„¡åŠ¹åŒ–ï¼‰
        self.episode_collector = LSLTCPEpisodeCollector(
            lsl_stream_name='MockEEG',
            tcp_host='127.0.0.1',
            tcp_port=12345,
            save_to_csv=True
        )
        
        if not self.episode_collector.start_collection():
            print("âŒ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åé›†é–‹å§‹å¤±æ•—")
            return False
        
        # TCPé€šä¿¡é–‹å§‹ã¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
        if not self.tcp_interface.start_server():
            print("âŒ TCPé€šä¿¡é–‹å§‹å¤±æ•—")
            return False
        
        # â˜… ä¿®æ­£4: ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®šï¼ˆçŠ¶æ…‹æ©‹æ¸¡ã—æ©Ÿèƒ½ä»˜ãï¼‰
        print("ğŸ”— TCPã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®šä¸­...")
        
        # DDPGå´ï¼ˆ12346ï¼‰ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        self.tcp_interface.add_message_callback(self._handle_tcp_state)  # ãƒ­ãƒœãƒƒãƒˆçŠ¶æ…‹ä¿æŒç”¨
        self.tcp_interface.add_message_callback(self.handle_grip_force_request)  # æŠŠæŒåŠ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†ç”¨
        
        # â˜… æ–°æ©Ÿèƒ½: Collectorå´ï¼ˆ12345ï¼‰ã‹ã‚‰DDPGå´ã¸ã®çŠ¶æ…‹æ©‹æ¸¡ã—
        self.episode_collector.tcp_interface.add_state_update_callback(self._handle_collector_state_update)
        print("   ğŸŒ‰ Collectorâ†’DDPGçŠ¶æ…‹æ©‹æ¸¡ã—è¨­å®šå®Œäº†")
        
        print("âœ… TCPã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®šå®Œäº†")
        print("ğŸ“‹ ãƒãƒ¼ãƒˆè¨­å®š:")
        print("   ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åé›†: 127.0.0.1:12345 (è‡ªå‹•å¿œç­”ç„¡åŠ¹)")
        print("   å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ : 127.0.0.1:12346 (è‡ªå‹•å¿œç­”ç„¡åŠ¹)")
        print("ğŸ’¡ Unityæ¥ç¶šæ¨å¥¨: 12346ãƒãƒ¼ãƒˆã«æ¥ç¶šã—ã¦ãã ã•ã„")
        print("ğŸ”— çŠ¶æ…‹ãƒ‡ãƒ¼ã‚¿: 12345â†’12346è‡ªå‹•æ©‹æ¸¡ã—æœ‰åŠ¹")
        
        # å­¦ç¿’ãƒ«ãƒ¼ãƒ—é–‹å§‹ï¼ˆdaemon=Falseï¼‰
        self.is_running = True
        self.learning_thread = threading.Thread(target=self._learning_loop, daemon=False)
        self.learning_thread.start()
        
        print(f"âœ… TypeAå­¦ç¿’é–‹å§‹å®Œäº†ï¼ˆåŒ…æ‹¬çš„ä¿®æ­£ç‰ˆï¼‰")
        return True
    
    def _learning_loop(self):
        """å­¦ç¿’ãƒ«ãƒ¼ãƒ—ï¼ˆåŒ…æ‹¬çš„ä¿®æ­£ç‰ˆï¼‰"""
        print(f"ğŸ”„ TypeAå­¦ç¿’ãƒ«ãƒ¼ãƒ—é–‹å§‹ï¼ˆPendingæ–¹å¼ãƒ»åŒ…æ‹¬çš„ä¿®æ­£ç‰ˆï¼‰")
        
        last_episode_count = 0
        last_health_check = time.time()
        last_stats_print = time.time()
        
        while self.is_running:
            try:
                current_episode_count = len(self.episode_collector.episodes)
                
                # åœæ­¢æ¡ä»¶ãƒã‚§ãƒƒã‚¯
                if current_episode_count >= self.target_episodes:
                    print(f"ğŸ¯ ç›®æ¨™ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°åˆ°é”: {current_episode_count}/{self.target_episodes}")
                    break
                
                # æ–°ã—ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®å‡¦ç†
                if current_episode_count > last_episode_count:
                    for i in range(last_episode_count, current_episode_count):
                        episode = self.episode_collector.episodes[i]
                        
                        print(f"ğŸ†• ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ {episode.episode_id} å—ä¿¡ (pending: {'ã‚ã‚Š' if self.pending_state is not None else 'ãªã—'})")
                        
                        # PendingçŠ¶æ…‹ãŒã‚ã‚‹å ´åˆã®ã¿çµŒé¨“ã‚’è¿½åŠ ï¼ˆK=1è¨­è¨ˆï¼‰
                        if self.pending_state is not None and self.pending_action is not None:
                            # å ±é…¬è¨ˆç®—
                            reward = self.calculate_explicit_reward(episode.tcp_data)
                            
                            # æ¬¡çŠ¶æ…‹ä½œæˆ
                            next_state = self.create_state(episode.tcp_data, self.pending_action[0])
                            
                            # çµŒé¨“ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
                            self.replay_buffer.push(
                                self.pending_state,
                                self.pending_action,
                                reward,
                                next_state,
                                True  # K=1ãªã®ã§æ¯å›done=True
                            )
                            
                            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ›´æ–°
                            if len(self.replay_buffer) >= self.batch_size:
                                actor_loss, critic_loss, avg_q = self.update_networks()
                            else:
                                actor_loss, critic_loss, avg_q = 0, 0, 0
                            
                            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿è¨˜éŒ²
                            self._record_episode_data(episode, reward, actor_loss, critic_loss, avg_q)
                            
                            print(f"âœ… å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ: episode={len(self.episode_data)}, reward={reward:.2f}, buffer_size={len(self.replay_buffer)}")
                            
                            # é€²æ—è¡¨ç¤º
                            if len(self.episode_data) % 50 == 0:
                                self._print_learning_progress()
                            
                            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
                            if len(self.episode_data) % 100 == 0:
                                self._save_model()
                        else:
                            print(f"âš ï¸ PendingçŠ¶æ…‹ãªã—ã§EPISODE_ENDå—ä¿¡: episode={episode.episode_id}")
                        
                        # PendingçŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆ
                        self.pending_state = None
                        self.pending_action = None
                    
                    last_episode_count = current_episode_count
                
                # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚«ã‚¦ãƒ³ã‚¿åŒæœŸ
                self.episode_count = current_episode_count
                
                # â˜… æ–°æ©Ÿèƒ½: å®šæœŸçš„ãªå¥å…¨æ€§ãƒã‚§ãƒƒã‚¯
                current_time = time.time()
                if current_time - last_health_check > 30:  # 30ç§’ã”ã¨
                    health_results = self.health_checker.run_health_checks()
                    if health_results:
                        status = self.health_checker.get_status_summary()
                        print(f"ğŸ¥ ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§: {status}")
                    last_health_check = current_time
                
                # å®šæœŸçš„ãªçµ±è¨ˆè¡¨ç¤º
                if current_time - last_stats_print > 60:  # 60ç§’ã”ã¨
                    self._print_request_statistics()
                    last_stats_print = current_time
                
                time.sleep(0.1)
                
            except Exception as e:
                print(f"âŒ å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
                time.sleep(1.0)
        
        print(f"âœ… TypeAå­¦ç¿’ãƒ«ãƒ¼ãƒ—å®Œäº†: {self.episode_count}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰")
        self.is_running = False
        self._save_final_results()
    
    def _print_request_statistics(self):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†çµ±è¨ˆè¡¨ç¤º"""
        stats = self.request_stats
        total = stats['total_requests']
        
        if total > 0:
            success_rate = stats['successful_responses'] / total * 100
            context_rate = stats['state_context_available'] / total * 100
            
            print(f"\nğŸ“Š ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†çµ±è¨ˆ:")
            print(f"   ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: {total}")
            print(f"   JSON/ãƒ†ã‚­ã‚¹ãƒˆ: {stats['json_requests']}/{stats['text_requests']}")
            print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
            print(f"   çŠ¶æ…‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ç‡: {context_rate:.1f}%")
            print(f"   Collectorãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {stats['collector_fallbacks']}")
    
    def _record_episode_data(self, episode, reward, actor_loss, critic_loss, avg_q):
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®è¨˜éŒ²"""
        grip_force = episode.tcp_data.get('grip_force', 0.0)
        contact = episode.tcp_data.get('contact', False)
        broken = episode.tcp_data.get('broken', False)
        
        # æˆåŠŸåˆ¤å®š
        success = self.force_min <= grip_force <= self.force_max
        
        # æŠŠæŒåŠ›èª¤å·®
        force_error = abs(grip_force - self.force_center)
        
        episode_info = {
            'episode': len(self.episode_data) + 1,  # å­¦ç¿’ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç•ªå·
            'reward': reward,
            'grip_force': grip_force,
            'success': success,
            'force_error': force_error,
            'contact': contact,
            'broken': broken,
            'actor_loss': actor_loss,
            'critic_loss': critic_loss,
            'avg_q_value': avg_q,
            'noise_sigma': self.noise.sigma
        }
        
        self.episode_data.append(episode_info)
    
    def _print_learning_progress(self):
        """å­¦ç¿’é€²æ—è¡¨ç¤º"""
        if len(self.episode_data) == 0:
            return
        
        # æœ€æ–°50ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®çµ±è¨ˆ
        recent_data = self.episode_data[-50:]
        recent_rewards = [d['reward'] for d in recent_data]
        recent_successes = [d['success'] for d in recent_data]
        recent_errors = [d['force_error'] for d in recent_data]
        recent_damages = [d['broken'] for d in recent_data]
        
        success_rate = np.mean(recent_successes)
        avg_reward = np.mean(recent_rewards)
        avg_error = np.mean(recent_errors)
        damage_rate = np.mean(recent_damages)
        
        print(f"\nğŸ“Š TypeAé€²æ— (å­¦ç¿’æ¸ˆã¿ {len(self.episode_data)}/{self.target_episodes}):")
        print(f"   å¹³å‡å ±é…¬ï¼ˆæœ€æ–°50ï¼‰: {avg_reward:.2f}")
        print(f"   æˆåŠŸç‡ï¼ˆæœ€æ–°50ï¼‰: {success_rate:.1%}")
        print(f"   å¹³å‡åŠ›èª¤å·®ï¼ˆæœ€æ–°50ï¼‰: {avg_error:.2f}N")
        print(f"   ç ´æç‡ï¼ˆæœ€æ–°50ï¼‰: {damage_rate:.1%}")
        print(f"   ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º: {len(self.replay_buffer)}")
        print(f"   æ¢ç´¢ãƒã‚¤ã‚ºÏƒ: {self.noise.sigma:.3f}")
        print(f"   å¥å…¨æ€§: {self.health_checker.get_status_summary()}")
    
    def _save_model(self):
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        try:
            model_path = os.path.join(self.output_dir, f'typea_model_ep{len(self.episode_data)}.pth')
            torch.save({
                'actor_state_dict': self.actor.state_dict(),
                'critic_state_dict': self.critic.state_dict(),
                'actor_optimizer': self.actor_optimizer.state_dict(),
                'critic_optimizer': self.critic_optimizer.state_dict(),
                'episode_count': len(self.episode_data),
                'experiment_type': self.experiment_type,
                'seed': self.seed,
                'request_stats': self.request_stats
            }, model_path)
        except Exception as e:
            print(f"âš ï¸ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _calculate_advanced_metrics(self):
        """é«˜åº¦ãªæŒ‡æ¨™è¨ˆç®—"""
        if len(self.episode_data) == 0:
            return {}
        
        # DataFrameã«å¤‰æ›
        df = pd.DataFrame(self.episode_data)
        
        # ç§»å‹•å¹³å‡è¨ˆç®—ï¼ˆæˆåŠŸç‡ï¼‰
        success_ma = AnalysisUtils.moving_average(df['success'].values, window=100)
        
        # AUCè¨ˆç®—
        auc_all = AnalysisUtils.calculate_auc(success_ma)
        auc_0_400 = AnalysisUtils.calculate_auc(success_ma[:400]) if len(success_ma) >= 400 else auc_all
        
        # plateauæ¤œå‡º
        plateau_value, plateau_episode = AnalysisUtils.detect_plateau(success_ma, window=200, eps=1e-3)
        
        # time-to-70%
        time_to_70 = AnalysisUtils.find_time_to_threshold(success_ma, threshold=0.70)
        
        # æœ€çµ‚æ€§èƒ½ï¼ˆæœ€æ–°100ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å¹³å‡ï¼‰
        final_success_rate = np.mean(df['success'].iloc[-100:]) if len(df) >= 100 else np.mean(df['success'])
        final_reward = np.mean(df['reward'].iloc[-100:]) if len(df) >= 100 else np.mean(df['reward'])
        final_force_error = np.mean(df['force_error'].iloc[-100:]) if len(df) >= 100 else np.mean(df['force_error'])
        final_damage_rate = np.mean(df['broken'].iloc[-100:]) if len(df) >= 100 else np.mean(df['broken'])
        
        return {
            'auc_all': auc_all,
            'auc_0_400': auc_0_400,
            'plateau_value': plateau_value,
            'plateau_episode': plateau_episode,
            'time_to_70': time_to_70,
            'final_success_rate': final_success_rate,
            'final_reward': final_reward,
            'final_force_error': final_force_error,
            'final_damage_rate': final_damage_rate,
            'success_moving_average': success_ma.tolist()
        }
    
    def _save_final_results(self):
        """æœ€çµ‚çµæœä¿å­˜ï¼ˆåŒ…æ‹¬çš„ä¿®æ­£ç‰ˆï¼‰"""
        print(f"ğŸ’¾ æœ€çµ‚çµæœä¿å­˜ä¸­...")
        
        if len(self.episode_data) == 0:
            print(f"âš ï¸ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ãŒã€åŸºæœ¬æƒ…å ±ã‚’ä¿å­˜ã—ã¾ã™")
            basic_stats = {
                'experiment_type': self.experiment_type,
                'seed': self.seed,
                'total_episodes': self.episode_count,
                'target_episodes': self.target_episodes,
                'learning_episodes': 0,
                'request_stats': self.request_stats,
                'health_summary': self.health_checker.get_status_summary(),
                'message': 'No learning data collected'
            }
            json_path = os.path.join(self.output_dir, 'final_stats.json')
            with open(json_path, 'w') as f:
                json.dump(basic_stats, f, indent=2)
            print(f"ğŸ“„ åŸºæœ¬æƒ…å ±ä¿å­˜: {json_path}")
            return
        
        # DataFrameä½œæˆ
        df = pd.DataFrame(self.episode_data)
        
        # ç§»å‹•å¹³å‡è¿½åŠ 
        df['success_rate_ma100'] = AnalysisUtils.moving_average(df['success'].values, window=100)
        df['reward_ma50'] = AnalysisUtils.moving_average(df['reward'].values, window=50)
        
        # learning_results.csvä¿å­˜
        csv_data = {
            'episode': df['episode'],
            'reward': df['reward'],
            'success_rate': df['success_rate_ma100'],
            'force_error': df['force_error'],
            'damage_rate': df['broken'].astype(int)
        }
        results_df = pd.DataFrame(csv_data)
        csv_path = os.path.join(self.output_dir, 'learning_results.csv')
        results_df.to_csv(csv_path, index=False)
        
        # é«˜åº¦ãªæŒ‡æ¨™è¨ˆç®—
        advanced_metrics = self._calculate_advanced_metrics()
        
        # final_stats.jsonä¿å­˜ï¼ˆåŒ…æ‹¬çš„ä¿®æ­£ç‰ˆï¼‰
        final_stats = {
            'experiment_type': self.experiment_type,
            'seed': self.seed,
            'total_episodes': self.episode_count,
            'target_episodes': self.target_episodes,
            'learning_episodes': len(self.episode_data),
            **advanced_metrics,
            'request_stats': self.request_stats,
            'health_summary': self.health_checker.get_status_summary(),
            'reward_parameters': self.reward_params,
            'force_normalization': {
                'center': self.force_center,
                'halfwidth': self.force_halfwidth,
                'range': [self.force_min, self.force_max]
            }
        }
        
        json_path = os.path.join(self.output_dir, 'final_stats.json')
        with open(json_path, 'w') as f:
            json.dump(final_stats, f, indent=2)
        
        # master_auc.jsonä¿å­˜ï¼ˆé›†è¨ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆç”¨ï¼‰
        master_auc = {
            'auc_0_400': advanced_metrics['auc_0_400'],
            'auc_all': advanced_metrics['auc_all'],
            'time_to_70': advanced_metrics['time_to_70'],
            'plateau_value': advanced_metrics['plateau_value'],
            'plateau_at_episode': advanced_metrics['plateau_episode'],
            'final_success_rate_at_400': advanced_metrics['success_moving_average'][399] if len(advanced_metrics['success_moving_average']) > 399 else None,
            'final_success_rate': advanced_metrics['final_success_rate'],
            'request_success_rate': self.request_stats['successful_responses'] / max(self.request_stats['total_requests'], 1),
            'state_context_rate': self.request_stats['state_context_available'] / max(self.request_stats['total_requests'], 1)
        }
        
        master_auc_path = os.path.join(self.output_dir, 'master_auc.json')
        with open(master_auc_path, 'w') as f:
            json.dump(master_auc, f, indent=2)
        
        # å­¦ç¿’æ›²ç·šãƒ—ãƒ­ãƒƒãƒˆ
        self._plot_learning_curves(df)
        
        print(f"âœ… æœ€çµ‚çµæœä¿å­˜å®Œäº†:")
        print(f"   CSV: {csv_path}")
        print(f"   çµ±è¨ˆ: {json_path}")
        print(f"   Master AUC: {master_auc_path}")
        print(f"   å­¦ç¿’ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {len(self.episode_data)}/{self.episode_count}")
        print(f"   æœ€çµ‚æˆåŠŸç‡: {advanced_metrics['final_success_rate']:.1%}")
        print(f"   AUC(0-400): {advanced_metrics['auc_0_400']:.2f}")
        print(f"   AUC(å…¨åŸŸ): {advanced_metrics['auc_all']:.2f}")
        print(f"   ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†æˆåŠŸç‡: {master_auc['request_success_rate']:.1%}")
        print(f"   çŠ¶æ…‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ç‡: {master_auc['state_context_rate']:.1%}")
        if advanced_metrics['time_to_70'] is not None:
            print(f"   Time-to-70%: Episode {advanced_metrics['time_to_70']}")
        if advanced_metrics['plateau_value'] is not None:
            print(f"   Plateau: {advanced_metrics['plateau_value']:.3f} (Episode {advanced_metrics['plateau_episode']})")
    
    def _plot_learning_curves(self, df):
        """å­¦ç¿’æ›²ç·šã®ãƒ—ãƒ­ãƒƒãƒˆ"""
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle(f'TypeA DDPG Learning Curves (Fixed Ver. - {self.experiment_type}, seed={self.seed})')
            
            episodes = df['episode'].values
            
            # æˆåŠŸç‡ï¼ˆç§»å‹•å¹³å‡ï¼‰
            axes[0, 0].plot(episodes, df['success_rate_ma100'], 'g-', linewidth=2, label='Success Rate (MA100)')
            axes[0, 0].axhline(y=0.7, color='r', linestyle='--', alpha=0.7, label='70% threshold')
            axes[0, 0].set_title('Success Rate (8-15N)')
            axes[0, 0].set_xlabel('Episode')
            axes[0, 0].set_ylabel('Success Rate')
            axes[0, 0].set_ylim(0, 1)
            axes[0, 0].legend()
            axes[0, 0].grid(True)
            
            # å ±é…¬
            axes[0, 1].plot(episodes, df['reward'], alpha=0.3, label='Raw')
            axes[0, 1].plot(episodes, df['reward_ma50'], 'r-', linewidth=2, label='MA50')
            axes[0, 1].set_title('Episode Rewards')
            axes[0, 1].set_xlabel('Episode')
            axes[0, 1].set_ylabel('Reward')
            axes[0, 1].legend()
            axes[0, 1].grid(True)
            
            # æŠŠæŒåŠ›èª¤å·®
            axes[1, 0].plot(episodes, df['force_error'], alpha=0.3)
            force_error_ma = AnalysisUtils.moving_average(df['force_error'].values, 50)
            axes[1, 0].plot(episodes, force_error_ma, 'orange', linewidth=2)
            axes[1, 0].set_title('Grip Force Error |F - F*|')
            axes[1, 0].set_xlabel('Episode')
            axes[1, 0].set_ylabel('Error (N)')
            axes[1, 0].grid(True)
            
            # Qå€¤ã¨Loss
            if 'avg_q_value' in df.columns and df['avg_q_value'].notna().any():
                q_values = df['avg_q_value'].dropna()
                q_episodes = df.loc[df['avg_q_value'].notna(), 'episode']
                axes[1, 1].plot(q_episodes, q_values, 'b-', alpha=0.7, label='Avg Q-value')
                axes[1, 1].set_ylabel('Q-value', color='b')
                axes[1, 1].tick_params(axis='y', labelcolor='b')
                
                # Actor Lossï¼ˆå³è»¸ï¼‰
                if 'actor_loss' in df.columns and df['actor_loss'].notna().any():
                    ax2 = axes[1, 1].twinx()
                    actor_losses = df['actor_loss'].dropna()
                    loss_episodes = df.loc[df['actor_loss'].notna(), 'episode']
                    ax2.plot(loss_episodes, actor_losses, 'r-', alpha=0.7, label='Actor Loss')
                    ax2.set_ylabel('Actor Loss', color='r')
                    ax2.tick_params(axis='y', labelcolor='r')
            
            axes[1, 1].set_title('Q-values & Actor Loss')
            axes[1, 1].set_xlabel('Episode')
            axes[1, 1].grid(True)
            
            plt.tight_layout()
            
            plot_path = os.path.join(self.output_dir, 'learning_curves.png')
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"ğŸ“ˆ å­¦ç¿’æ›²ç·šä¿å­˜: {plot_path}")
            
        except Exception as e:
            print(f"âš ï¸ ãƒ—ãƒ­ãƒƒãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def stop_learning(self):
        """å­¦ç¿’åœæ­¢ï¼ˆåŒ…æ‹¬çš„ä¿®æ­£ç‰ˆï¼‰"""
        print(f"ğŸ›‘ TypeAå­¦ç¿’åœæ­¢ä¸­...")
        
        self.is_running = False
        
        if self.episode_collector:
            self.episode_collector.stop_collection()
        
        if self.tcp_interface:
            self.tcp_interface.stop_server()
        
        # æœ€çµ‚çµ±è¨ˆè¡¨ç¤º
        self._print_request_statistics()
        print(f"ğŸ¥ æœ€çµ‚å¥å…¨æ€§: {self.health_checker.get_status_summary()}")
        
        # ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿çµæœä¿å­˜
        if len(self.episode_data) > 0:
            self._save_final_results()
        
        # å­¦ç¿’ã‚¹ãƒ¬ãƒƒãƒ‰ã®çµ‚äº†ã‚’å¾…ã¤
        if self.learning_thread and self.learning_thread.is_alive():
            print(f"â³ å­¦ç¿’ã‚¹ãƒ¬ãƒƒãƒ‰çµ‚äº†å¾…æ©Ÿä¸­...")
            self.learning_thread.join(timeout=10)
            if self.learning_thread.is_alive():
                print(f"âš ï¸ å­¦ç¿’ã‚¹ãƒ¬ãƒƒãƒ‰ãŒ10ç§’ä»¥å†…ã«çµ‚äº†ã—ã¾ã›ã‚“ã§ã—ãŸ")
            else:
                print(f"âœ… å­¦ç¿’ã‚¹ãƒ¬ãƒƒãƒ‰æ­£å¸¸çµ‚äº†")
        
        print(f"âœ… TypeAå­¦ç¿’åœæ­¢å®Œäº†ï¼ˆåŒ…æ‹¬çš„ä¿®æ­£ç‰ˆï¼‰")

# é›†è¨ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæ—¢å­˜ã®ã¾ã¾ã§å•é¡Œãªã—ï¼‰
def aggregate_multiple_seeds(base_dir, experiment_type, seeds):
    """è¤‡æ•°ã‚·ãƒ¼ãƒ‰ã®çµæœã‚’é›†è¨ˆ"""
    print(f"ğŸ“Š è¤‡æ•°ã‚·ãƒ¼ãƒ‰çµæœé›†è¨ˆ: {experiment_type}, seeds={seeds}")
    
    all_results = []
    
    for seed in seeds:
        # ã‚·ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ¤œç´¢
        pattern = f"typea_{experiment_type}_seed{seed}_*"
        matching_dirs = list(Path(base_dir).glob(pattern))
        
        if not matching_dirs:
            print(f"âš ï¸ Seed {seed}ã®çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {pattern}")
            continue
        
        # æœ€æ–°ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨
        latest_dir = max(matching_dirs, key=lambda x: x.stat().st_mtime)
        master_auc_path = latest_dir / "master_auc.json"
        
        if master_auc_path.exists():
            with open(master_auc_path, 'r') as f:
                seed_result = json.load(f)
                seed_result['seed'] = seed
                all_results.append(seed_result)
            print(f"   Seed {seed}: èª­ã¿è¾¼ã¿å®Œäº†")
        else:
            print(f"âš ï¸ Seed {seed}: master_auc.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    if len(all_results) == 0:
        print(f"âŒ æœ‰åŠ¹ãªçµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # çµ±è¨ˆè¨ˆç®—
    metrics = ['auc_0_400', 'auc_all', 'final_success_rate', 'time_to_70', 'plateau_value', 
               'request_success_rate', 'state_context_rate']
    aggregated = {}
    
    for metric in metrics:
        values = [r[metric] for r in all_results if r.get(metric) is not None]
        if values:
            aggregated[f'{metric}_mean'] = np.mean(values)
            aggregated[f'{metric}_std'] = np.std(values)
            aggregated[f'{metric}_ci95'] = 1.96 * np.std(values) / np.sqrt(len(values))
            aggregated[f'{metric}_values'] = values
    
    aggregated['n_seeds'] = len(all_results)
    aggregated['seeds'] = [r['seed'] for r in all_results]
    aggregated['experiment_type'] = experiment_type
    
    # çµæœä¿å­˜
    output_path = Path(base_dir) / f"aggregated_{experiment_type}_fixed.json"
    with open(output_path, 'w') as f:
        json.dump(aggregated, f, indent=2)
    
    print(f"âœ… é›†è¨ˆçµæœä¿å­˜: {output_path}")
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print(f"\nğŸ“ˆ {experiment_type} é›†è¨ˆçµæœï¼ˆåŒ…æ‹¬çš„ä¿®æ­£ç‰ˆï¼‰ (n={len(all_results)}):")
    for metric in ['auc_0_400', 'auc_all', 'final_success_rate', 'request_success_rate']:
        if f'{metric}_mean' in aggregated:
            mean_val = aggregated[f'{metric}_mean']
            ci_val = aggregated[f'{metric}_ci95']
            print(f"   {metric}: {mean_val:.3f} Â± {ci_val:.3f}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description="TypeA DDPGå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ï¼ˆåŒ…æ‹¬çš„ä¿®æ­£ç‰ˆï¼‰")
    parser.add_argument("--type", choices=["A_400", "A_long"], default="A_400",
                       help="å®Ÿé¨“ã‚¿ã‚¤ãƒ—")
    parser.add_argument("--seed", type=int, default=42,
                       help="ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰")
    parser.add_argument("--multi-seed", nargs="+", type=int,
                       help="è¤‡æ•°ã‚·ãƒ¼ãƒ‰å®Ÿè¡Œ (ä¾‹: --multi-seed 1 2 3 4 5)")
    parser.add_argument("--aggregate", action="store_true",
                       help="æ—¢å­˜çµæœã®é›†è¨ˆã®ã¿å®Ÿè¡Œ")
    
    args = parser.parse_args()
    
    print(f"ğŸ¤– TypeA DDPGå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ï¼ˆåŒ…æ‹¬çš„ä¿®æ­£ç‰ˆï¼‰")
    print(f"=" * 70)
    print(f"ä¿®æ­£å†…å®¹:")
    print(f"  1. æŠŠæŒåŠ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆå—ç†æ¡ä»¶ã®å¤§å¹…æ‹¡å¼µ")
    print(f"  2. Collectorâ†’DDPGçŠ¶æ…‹æ©‹æ¸¡ã—æ©Ÿèƒ½è¿½åŠ ")
    print(f"  3. å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½å®Ÿè£…")
    print(f"  4. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»ãƒ­ã‚°æ©Ÿèƒ½å¼·åŒ–")
    print(f"  5. Unityäº’æ›æ€§å‘ä¸Šï¼ˆtarget_force + targetForceï¼‰")
    print(f"=" * 70)
    
    base_output_dir = "DDPG_Python/logs"
    
    if args.aggregate:
        # é›†è¨ˆã®ã¿å®Ÿè¡Œ
        if args.multi_seed:
            aggregate_multiple_seeds(base_output_dir, args.type, args.multi_seed)
        else:
            print(f"âŒ --aggregate ã«ã¯ --multi-seed ãŒå¿…è¦ã§ã™")
        return
    
    if args.multi_seed:
        # è¤‡æ•°ã‚·ãƒ¼ãƒ‰å®Ÿè¡Œ
        print(f"ğŸ”„ è¤‡æ•°ã‚·ãƒ¼ãƒ‰å®Ÿè¡Œ: {args.type}, seeds={args.multi_seed}")
        
        for seed in args.multi_seed:
            print(f"\nğŸŒ± Seed {seed} é–‹å§‹...")
            
            system = TypeADDPGSystem(experiment_type=args.type, seed=seed)
            
            if system.run_learning():
                try:
                    # å­¦ç¿’å®Œäº†ã¾ã§å¾…æ©Ÿ
                    while system.is_running and system.episode_count < system.target_episodes:
                        time.sleep(10)
                        if system.episode_count % 100 == 0 and system.episode_count > 0:
                            health = system.health_checker.get_status_summary()
                            print(f"   Seed {seed}: {system.episode_count}/{system.target_episodes}ep - {health}")
                    
                    if system.episode_count >= system.target_episodes:
                        print(f"âœ… Seed {seed} å®Œäº†!")
                    
                except KeyboardInterrupt:
                    print(f"â¹ï¸ Seed {seed} ä¸­æ–­")
                finally:
                    system.stop_learning()
            else:
                print(f"âŒ Seed {seed} é–‹å§‹å¤±æ•—")
        
        # è‡ªå‹•é›†è¨ˆ
        print(f"\nğŸ“Š è‡ªå‹•é›†è¨ˆå®Ÿè¡Œ...")
        aggregate_multiple_seeds(base_output_dir, args.type, args.multi_seed)
        
    else:
        # å˜ä¸€ã‚·ãƒ¼ãƒ‰å®Ÿè¡Œ
        system = TypeADDPGSystem(experiment_type=args.type, seed=args.seed)
        
        if system.run_learning():
            try:
                print(f"\nğŸ’¡ TypeAå­¦ç¿’å®Ÿè¡Œä¸­ï¼ˆåŒ…æ‹¬çš„ä¿®æ­£ç‰ˆï¼‰:")
                print(f"   å®Ÿé¨“ã‚¿ã‚¤ãƒ—: {args.type}")
                print(f"   ã‚·ãƒ¼ãƒ‰: {args.seed}")
                print(f"   ç›®æ¨™ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {system.target_episodes}")
                print(f"   æŠŠæŒåŠ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆå—ç†: æ‹¡å¼µå¯¾å¿œ")
                print(f"   çŠ¶æ…‹æ©‹æ¸¡ã—: Collectorâ†’DDPGè‡ªå‹•")
                print(f"   å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯: 30ç§’é–“éš”")
                print(f"   Ctrl+C ã§çµ‚äº†")
                
                # é€²æ—ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
                last_progress_time = time.time()
                while system.is_running and system.episode_count < system.target_episodes:
                    time.sleep(5)
                    
                    # 30ç§’ã”ã¨ã«é€²æ—è¡¨ç¤º
                    current_time = time.time()
                    if current_time - last_progress_time >= 30:
                        health = system.health_checker.get_status_summary()
                        print(f"ğŸ”„ é€²æ—: {system.episode_count}/{system.target_episodes}ep - {health}")
                        last_progress_time = current_time
                
                if system.episode_count >= system.target_episodes:
                    print(f"ğŸ‰ ç›®æ¨™ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°é”æˆï¼")
                
            except KeyboardInterrupt:
                print(f"\nâ¹ï¸ å­¦ç¿’ä¸­æ–­")
            finally:
                system.stop_learning()
        else:
            print(f"âŒ å­¦ç¿’é–‹å§‹å¤±æ•—")

if __name__ == "__main__":
    main()