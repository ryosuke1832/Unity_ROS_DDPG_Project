#!/usr/bin/env python3
"""
TypeA DDPGå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ï¼ˆä¿®æ­£ç‰ˆãƒ»å³åŠ¹ãƒ‘ãƒƒãƒé©ç”¨æ¸ˆã¿ï¼‰

ä¿®æ­£ç‚¹ï¼š
1. åœæ­¢æ¡ä»¶ã‚’ã€Œåé›†æ¸ˆã¿ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°ã€ã«åŸºã¥ã‹ã›ã‚‹
2. å®Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°ã§self.episode_countã‚’æ›´æ–°
3. ãƒ«ãƒ¼ãƒ—çµ‚äº†æ™‚ã«ãƒ•ãƒ©ã‚°ã‚’è½ã¨ã—ã¦ã‹ã‚‰ä¿å­˜
4. ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ãƒ‡ãƒ¼ãƒ¢ãƒ³ã«ã—ãªã„
5. åœæ­¢æ™‚ã®ä¿å­˜æ¡ä»¶ã‚’ã€Œãƒ‡ãƒ¼ã‚¿ã®æœ‰ç„¡ã€ã§åˆ¤å®š

ã“ã‚Œã«ã‚ˆã‚Šä»¥ä¸‹ã®å•é¡Œã‚’è§£æ±ºï¼š
- 400ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã§åœæ­¢ã—ãªã„å•é¡Œ
- ãƒ­ã‚°ãŒä¿å­˜ã•ã‚Œãªã„å•é¡Œ
- ãƒ‡ãƒ¼ãƒ¢ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ç”±æ¥ã®stdoutãƒ­ãƒƒã‚¯ä¾‹å¤–
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import matplotlib.pyplot as plt
import os
import json
import time
import threading
import queue
import argparse
from datetime import datetime
from collections import deque, namedtuple
from pathlib import Path
from scipy import integrate
import warnings
warnings.filterwarnings('ignore')

# æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from e_tcp_lsl_sync_system import LSLTCPEpisodeCollector, Episode
from c_unity_tcp_interface import EEGTCPInterface

# PyTorchè¨­å®š
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ¯ ãƒ‡ãƒã‚¤ã‚¹: {device}")

# DDPGç”¨ã®çµŒé¨“ãƒãƒƒãƒ•ã‚¡
Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])

class Actor(nn.Module):
    """DDPG Actorãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"""
    def __init__(self, state_dim=4, action_dim=1, hidden_dim=256):
        super().__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)
        
        # HeåˆæœŸåŒ–
        nn.init.kaiming_normal_(self.fc1.weight)
        nn.init.kaiming_normal_(self.fc2.weight)
        nn.init.uniform_(self.fc3.weight, -3e-3, 3e-3)
    
    def forward(self, state):
        x = torch.relu(self.fc1(state))
        x = torch.relu(self.fc2(x))
        x = torch.tanh(self.fc3(x))  # [-1, 1]ã®ç¯„å›²
        return x

class Critic(nn.Module):
    """DDPG Criticãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"""
    def __init__(self, state_dim=4, action_dim=1, hidden_dim=256):
        super().__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim + action_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 1)
        
        nn.init.kaiming_normal_(self.fc1.weight)
        nn.init.kaiming_normal_(self.fc2.weight)
        nn.init.uniform_(self.fc3.weight, -3e-3, 3e-3)
    
    def forward(self, state, action):
        x = torch.relu(self.fc1(state))
        x = torch.cat([x, action], dim=1)
        x = torch.relu(self.fc2(x))
        q_value = self.fc3(x)
        return q_value

class OUNoise:
    """Ornstein-Uhlenbeck ãƒã‚¤ã‚º"""
    def __init__(self, action_dim, mu=0.0, theta=0.15, sigma=0.2):
        self.action_dim = action_dim
        self.mu = mu
        self.theta = theta
        self.sigma = sigma
        self.state = np.ones(self.action_dim) * self.mu
        self.reset()
    
    def reset(self):
        self.state = np.ones(self.action_dim) * self.mu
    
    def sample(self):
        x = self.state
        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))
        self.state = x + dx
        return self.state

class ReplayBuffer:
    """çµŒé¨“å†ç”Ÿãƒãƒƒãƒ•ã‚¡"""
    def __init__(self, capacity=100000):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        experience = Experience(state, action, reward, next_state, done)
        self.buffer.append(experience)
    
    def sample(self, batch_size):
        import random
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = map(np.stack, zip(*batch))
        return state, action, reward, next_state, done
    
    def __len__(self):
        return len(self.buffer)

class AnalysisUtils:
    """åˆ†æç”¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£"""
    
    @staticmethod
    def calculate_auc(y_values):
        """AUCè¨ˆç®—ï¼ˆå°å½¢å‰‡ï¼‰"""
        return float(np.trapz(y_values, dx=1.0))
    
    @staticmethod
    def moving_average(x, window=100):
        """ç§»å‹•å¹³å‡"""
        series = pd.Series(x)
        return series.rolling(window, min_periods=1).mean().values
    
    @staticmethod
    def detect_plateau(y_smooth, window=200, eps=1e-3):
        """plateauæ¤œå‡º"""
        if len(y_smooth) < window:
            return None, None
        
        segment = y_smooth[-window:]
        gradient = np.abs(np.diff(segment)).mean()
        
        if gradient < eps:
            plateau_value = float(np.mean(segment))
            plateau_episode = len(y_smooth) - window
            return plateau_value, plateau_episode
        
        return None, None
    
    @staticmethod
    def find_time_to_threshold(y_values, threshold=0.70):
        """é–¾å€¤åˆ°é”æ™‚é–“"""
        indices = np.where(y_values >= threshold)[0]
        return int(indices[0]) if len(indices) > 0 else None

class TypeADDPGSystem:
    """TypeA DDPGå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå³åŠ¹ãƒ‘ãƒƒãƒé©ç”¨æ¸ˆã¿ï¼‰"""
    
    def __init__(self, experiment_type="A_400", seed=42):
        """
        Args:
            experiment_type: "A_400" or "A_long"
            seed: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰
        """
        # ã‚·ãƒ¼ãƒ‰è¨­å®š
        self.seed = seed
        np.random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(seed)
        
        self.experiment_type = experiment_type
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir = f"DDPG_Python/logs/typea_{experiment_type}_seed{seed}_{self.session_id}"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°è¨­å®š
        if experiment_type == "A_400":
            self.target_episodes = 400
        elif experiment_type == "A_long":
            self.target_episodes = 5000
        else:
            self.target_episodes = 400
        
        # å¯¾ç§°æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆFä¸­å¿ƒ11.5Nã€åŠå¹…3.5Nï¼‰
        self.force_center = 11.5  # N
        self.force_halfwidth = 3.5  # N
        self.force_min = self.force_center - self.force_halfwidth  # 8N
        self.force_max = self.force_center + self.force_halfwidth  # 15N
        
        # çŠ¶æ…‹ç©ºé–“è¨­è¨ˆ [force_norm, contact, broken, prev_action]
        self.state_dim = 4
        self.action_dim = 1
        
        # DDPGãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.actor = Actor(self.state_dim, self.action_dim).to(device)
        self.actor_target = Actor(self.state_dim, self.action_dim).to(device)
        self.critic = Critic(self.state_dim, self.action_dim).to(device)
        self.critic_target = Critic(self.state_dim, self.action_dim).to(device)
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆæœŸåŒ–
        self.actor_target.load_state_dict(self.actor.state_dict())
        self.critic_target.load_state_dict(self.critic.state_dict())
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-3)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)
        
        # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.gamma = 0.99
        self.tau = 0.005
        self.batch_size = 64
        
        # çµŒé¨“ãƒãƒƒãƒ•ã‚¡
        self.replay_buffer = ReplayBuffer(capacity=100000)
        
        # æ¢ç´¢ãƒã‚¤ã‚º
        self.noise = OUNoise(self.action_dim, sigma=0.2)
        self.noise_decay = 0.995
        
        # ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ 
        self.episode_collector = None
        
        # TCPé€šä¿¡ï¼ˆä¿®æ­£: auto_reply=False ã§è‡ªå‹•å¿œç­”ã‚’ç„¡åŠ¹åŒ–ï¼‰
        self.tcp_interface = EEGTCPInterface(host='127.0.0.1', port=12346, auto_reply=False)
        
        # å®Ÿè¡Œåˆ¶å¾¡
        self.is_running = False
        self.learning_thread = None
        
        # Pendingæ–¹å¼ç”¨ã®çŠ¶æ…‹ç®¡ç†
        self.pending_state = None
        self.pending_action = None
        self.episode_count = 0  # è¡¨ç¤ºãƒ»é€²æ—ç”¨ã®ã‚«ã‚¦ãƒ³ã‚¿
        
        # çŠ¶æ…‹ç®¡ç†ï¼ˆä¿®æ­£: ç›´è¿‘ã®ãƒ­ãƒœãƒƒãƒˆçŠ¶æ…‹ã‚’ä¿æŒï¼‰
        self.last_tcp_data = None
        
        # çµ±è¨ˆï¼ˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã”ã¨ï¼‰
        self.episode_data = []  # å„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿
        
        # å ±é…¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.reward_params = {
            'success_reward': 10.0,
            'error_penalty_coeff': 1.0,
            'damage_penalty': 20.0,
            'contact_bonus': 2.0
        }
        
        print(f"ğŸ¤– TypeA DDPGå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†ï¼ˆãƒªã‚¯ã‚¨ã‚¹ãƒˆå‹ä¿®æ­£æ¸ˆã¿ï¼‰")
        print(f"   å®Ÿé¨“ã‚¿ã‚¤ãƒ—: {experiment_type}")
        print(f"   ã‚·ãƒ¼ãƒ‰: {seed}")
        print(f"   ç›®æ¨™ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {self.target_episodes}")
        print(f"   å¯¾ç§°æ­£è¦åŒ–: {self.force_min}-{self.force_max}N â†’ [-1,1]")
        print(f"   å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
    
    def _handle_tcp_state(self, message_data):
        """TCPçŠ¶æ…‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†ï¼ˆãƒ­ãƒœãƒƒãƒˆçŠ¶æ…‹ã‚’ä¿æŒï¼‰"""
        try:
            # JSONã®ãƒ­ãƒœãƒƒãƒˆçŠ¶æ…‹ï¼ˆepisode/grip_forceç­‰ï¼‰ãŒæ¥ãŸã¨ãã«æ›´æ–°
            if isinstance(message_data, dict) and 'episode' in message_data and 'grip_force' in message_data:
                self.last_tcp_data = message_data
                print(f"ğŸ“Š ãƒ­ãƒœãƒƒãƒˆçŠ¶æ…‹æ›´æ–°: ep={message_data.get('episode')}, force={message_data.get('grip_force'):.2f}N")
        except Exception as e:
            print(f"âš ï¸ TCPçŠ¶æ…‹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    
    def normalize_force(self, force):
        """æŠŠæŒåŠ›ã®å¯¾ç§°æ­£è¦åŒ– [8-15N] â†’ [-1,1]"""
        return (force - self.force_center) / self.force_halfwidth
    
    def denormalize_action(self, action):
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®é€†æ­£è¦åŒ– [-1,1] â†’ [8-15N]"""
        return action * self.force_halfwidth + self.force_center
    
    def calculate_explicit_reward(self, tcp_data):
        """æ˜ç¤ºçš„å ±é…¬ã®è¨ˆç®—"""
        actual_grip_force = tcp_data.get('grip_force', 0.0)
        contact = tcp_data.get('contact', False)
        broken = tcp_data.get('broken', False)
        
        reward = 0.0
        
        # æˆåŠŸå ±é…¬ï¼ˆ8-15Nç¯„å›²å†…ï¼‰
        if self.force_min <= actual_grip_force <= self.force_max:
            reward += self.reward_params['success_reward']
        
        # æŠŠæŒåŠ›èª¤å·®ãƒšãƒŠãƒ«ãƒ†ã‚£
        force_error = abs(actual_grip_force - self.force_center)
        reward -= self.reward_params['error_penalty_coeff'] * force_error
        
        # ç ´æãƒšãƒŠãƒ«ãƒ†ã‚£
        if broken:
            reward -= self.reward_params['damage_penalty']
        
        # æ¥è§¦ãƒœãƒ¼ãƒŠã‚¹
        if contact and not broken:
            reward += self.reward_params['contact_bonus']
        
        return reward
    
    def create_state(self, tcp_data, prev_action):
        """çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®ä½œæˆï¼ˆå¯¾ç§°æ­£è¦åŒ–ï¼‰"""
        grip_force = tcp_data.get('grip_force', self.force_center)
        force_norm = self.normalize_force(grip_force)
        
        contact = 1.0 if tcp_data.get('contact', False) else 0.0
        broken = 1.0 if tcp_data.get('broken', False) else 0.0
        
        state = np.array([
            force_norm,
            contact,
            broken,
            prev_action
        ], dtype=np.float32)
        
        return state
    
    def select_action(self, state, add_noise=True):
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³é¸æŠ"""
        state_tensor = torch.FloatTensor(state.reshape(1, -1)).to(device)
        
        self.actor.eval()
        with torch.no_grad():
            action = self.actor(state_tensor).cpu().data.numpy().flatten()
        
        if add_noise and self.is_running:
            noise_sample = self.noise.sample()
            action += noise_sample
        
        action = np.clip(action, -1.0, 1.0)
        return action
    
    def update_networks(self):
        """DDPGãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ›´æ–°"""
        if len(self.replay_buffer) < self.batch_size:
            return
        
        state, action, reward, next_state, done = self.replay_buffer.sample(self.batch_size)
        
        state = torch.FloatTensor(state).to(device)
        action = torch.FloatTensor(action).to(device)
        reward = torch.FloatTensor(reward).to(device)
        next_state = torch.FloatTensor(next_state).to(device)
        done = torch.FloatTensor(done).to(device)
        
        # Criticã®æ›´æ–°
        next_action = self.actor_target(next_state)
        target_q = self.critic_target(next_state, next_action)
        target_q = reward + (self.gamma * target_q * (1 - done))
        
        current_q = self.critic(state, action)
        critic_loss = F.mse_loss(current_q, target_q.detach())
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=1.0)
        self.critic_optimizer.step()
        
        # Actorã®æ›´æ–°
        actor_loss = -self.critic(state, self.actor(state)).mean()
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1.0)
        self.actor_optimizer.step()
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ã‚½ãƒ•ãƒˆã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ
        self.soft_update(self.actor_target, self.actor, self.tau)
        self.soft_update(self.critic_target, self.critic, self.tau)
        
        # ãƒã‚¤ã‚ºæ¸›è¡°
        self.noise.sigma *= self.noise_decay
        self.noise.sigma = max(self.noise.sigma, 0.01)
        
        return actor_loss.item(), critic_loss.item(), current_q.mean().item()
    
    def soft_update(self, target, source, tau):
        """ã‚½ãƒ•ãƒˆã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ"""
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)
    
    def handle_grip_force_request(self, message_data):
        """Unityã‹ã‚‰ã®æŠŠæŒåŠ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†ï¼ˆJSONãƒ»ãƒ†ã‚­ã‚¹ãƒˆä¸¡å¯¾å¿œï¼‰"""
        try:
            # JSONãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å ´åˆ
            is_json_request = (isinstance(message_data, dict) and 
                             message_data.get('type') == 'grip_force_request')
            
            # ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å ´åˆ
            is_text_request = (isinstance(message_data, dict) and 
                             message_data.get('type') == 'text_message' and 
                             message_data.get('content') == 'REQUEST_GRIP_FORCE')
            
            # ã„ãšã‚Œã§ã‚‚ãªã„å ´åˆã¯ç„¡è¦–
            if not (is_json_request or is_text_request):
                return
            
            print(f"ğŸ¯ æŠŠæŒåŠ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ¤œå‡º: {'JSON' if is_json_request else 'TEXT'}")
            
            # çŠ¶æ…‹ä½œæˆç”¨ã®TCPãƒ‡ãƒ¼ã‚¿ã‚’æ±ºå®š
            if is_json_request:
                # JSONãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å ´åˆã¯ãã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                tcp_data = message_data
            else:
                # ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å ´åˆã¯ç›´è¿‘ã®ãƒ­ãƒœãƒƒãƒˆçŠ¶æ…‹ã‚’ä½¿ç”¨
                if self.last_tcp_data is None:
                    print(f"âš ï¸ REQUEST_GRIP_FORCE ã‚’å—ä¿¡ã—ã¾ã—ãŸãŒã€ç›´è¿‘ã®ãƒ­ãƒœãƒƒãƒˆçŠ¶æ…‹ãŒãªã„ãŸã‚ pending ã‚’è¨­å®šã§ãã¾ã›ã‚“")
                    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆçŠ¶æ…‹ã§å‡¦ç†ã‚’ç¶šè¡Œ
                    tcp_data = {
                        'grip_force': self.force_center,
                        'contact': False,
                        'broken': False,
                        'episode': 0
                    }
                else:
                    tcp_data = self.last_tcp_data
                    print(f"ğŸ“Š ç›´è¿‘çŠ¶æ…‹ã‚’ä½¿ç”¨: ep={tcp_data.get('episode')}, force={tcp_data.get('grip_force', 0):.2f}N")
            
            # å‰å›ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å€¤
            prev_action = 0.0 if self.pending_action is None else self.pending_action[0]
            
            # çŠ¶æ…‹ä½œæˆ
            state = self.create_state(tcp_data, prev_action)
            
            # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³é¸æŠ
            action = self.select_action(state, add_noise=True)
            
            # PendingçŠ¶æ…‹ã‚’ä¿å­˜ï¼ˆK=1è¨­è¨ˆï¼‰
            self.pending_state = state
            self.pending_action = action
            
            # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠŠæŒåŠ›ã«å¤‰æ›
            grip_force = self.denormalize_action(action[0])
            grip_force = np.clip(grip_force, 5.0, 25.0)  # å®‰å…¨ã‚¯ãƒ©ãƒ³ãƒ—
            
            print(f"ğŸ¤– TypeAæŠŠæŒåŠ›æ±ºå®š: {grip_force:.2f}N (action: {action[0]:.3f}, noise_Ïƒ: {self.noise.sigma:.3f})")
            
            # TCPå¿œç­”é€ä¿¡
            response = {
                'type': 'grip_force_command',
                'target_force': float(grip_force),
                'timestamp': time.time(),
                'session_id': f"typea_{self.experiment_type}_seed{self.seed}_{int(time.time())}"
            }
            self.tcp_interface.send_message(response)
            
        except Exception as e:
            print(f"âŒ æŠŠæŒåŠ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
    
    def run_learning(self):
        """TypeA DDPGå­¦ç¿’å®Ÿè¡Œ"""
        print(f"ğŸš€ TypeA DDPGå­¦ç¿’é–‹å§‹ ({self.experiment_type}, seed={self.seed})")
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åé›†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ï¼ˆâ˜… ä¿®æ­£: auto_reply=False ã§è‡ªå‹•å¿œç­”ã‚’ç„¡åŠ¹åŒ–ï¼‰
        self.episode_collector = LSLTCPEpisodeCollector(
            lsl_stream_name='MockEEG',
            tcp_host='127.0.0.1',
            tcp_port=12345,
            save_to_csv=True
        )
        
        # â˜… é‡è¦: episode_collectorã®å†…éƒ¨EEGTCPInterfaceã‚‚è‡ªå‹•å¿œç­”ã‚’ç„¡åŠ¹åŒ–ã™ã‚‹å¿…è¦
        # ï¼ˆLSLTCPEpisodeCollectorãŒä¿®æ­£æ¸ˆã¿ã§ã‚ã‚‹ã“ã¨ã‚’å‰æï¼‰
        
        if not self.episode_collector.start_collection():
            print("âŒ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åé›†é–‹å§‹å¤±æ•—")
            return False
        
        # TCPé€šä¿¡é–‹å§‹ã¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
        if not self.tcp_interface.start_server():
            print("âŒ TCPé€šä¿¡é–‹å§‹å¤±æ•—")
            return False
        
        # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
        print("ğŸ”— TCPã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®šä¸­...")
        self.tcp_interface.add_message_callback(self._handle_tcp_state)  # ãƒ­ãƒœãƒƒãƒˆçŠ¶æ…‹ä¿æŒç”¨
        self.tcp_interface.add_message_callback(self.handle_grip_force_request)  # æŠŠæŒåŠ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†ç”¨
        print("âœ… TCPã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®šå®Œäº†")
        print("ğŸ“‹ ãƒãƒ¼ãƒˆè¨­å®š:")
        print("   ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åé›†: 127.0.0.1:12345 (è‡ªå‹•å¿œç­”ç„¡åŠ¹)")
        print("   å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ : 127.0.0.1:12346 (è‡ªå‹•å¿œç­”ç„¡åŠ¹)")
        print("ğŸ’¡ Unityæ¥ç¶šå…ˆ: 12346ãƒãƒ¼ãƒˆã«æ¥ç¶šã—ã¦ãã ã•ã„")
        
        # å­¦ç¿’ãƒ«ãƒ¼ãƒ—é–‹å§‹ï¼ˆâ˜… daemon=False ã«å¤‰æ›´ï¼‰
        self.is_running = True
        self.learning_thread = threading.Thread(target=self._learning_loop, daemon=False)
        self.learning_thread.start()
        
        print(f"âœ… TypeAå­¦ç¿’é–‹å§‹å®Œäº†")
        return True
    
    def _learning_loop(self):
        """å­¦ç¿’ãƒ«ãƒ¼ãƒ—ï¼ˆå³åŠ¹ãƒ‘ãƒƒãƒé©ç”¨æ¸ˆã¿ï¼‰"""
        print(f"ğŸ”„ TypeAå­¦ç¿’ãƒ«ãƒ¼ãƒ—é–‹å§‹ï¼ˆPendingæ–¹å¼ãƒ»å³åŠ¹ãƒ‘ãƒƒãƒé©ç”¨æ¸ˆã¿ï¼‰")
        
        last_episode_count = 0
        
        # â˜… ãƒ‘ãƒƒãƒ1: åœæ­¢æ¡ä»¶ã‚’ã€Œåé›†æ¸ˆã¿ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°ã€ã«åŸºã¥ã‹ã›ã‚‹
        while self.is_running:
            try:
                current_episode_count = len(self.episode_collector.episodes)
                
                # â˜… åé›†æ¸ˆã¿ãŒç›®æ¨™ã«åˆ°é”ã—ãŸã‚‰çµ‚äº†
                if current_episode_count >= self.target_episodes:
                    print(f"ğŸ¯ ç›®æ¨™ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°åˆ°é”: {current_episode_count}/{self.target_episodes}")
                    break
                
                if current_episode_count > last_episode_count:
                    # æ–°ã—ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å‡¦ç†
                    for i in range(last_episode_count, current_episode_count):
                        episode = self.episode_collector.episodes[i]
                        
                        print(f"ğŸ†• ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ {episode.episode_id} å—ä¿¡ (pending: {'ã‚ã‚Š' if self.pending_state is not None else 'ãªã—'})")
                        
                        # PendingçŠ¶æ…‹ãŒã‚ã‚‹å ´åˆã®ã¿çµŒé¨“ã‚’è¿½åŠ ï¼ˆK=1è¨­è¨ˆï¼‰
                        if self.pending_state is not None and self.pending_action is not None:
                            # å ±é…¬è¨ˆç®—
                            reward = self.calculate_explicit_reward(episode.tcp_data)
                            
                            # æ¬¡çŠ¶æ…‹ä½œæˆ
                            next_state = self.create_state(episode.tcp_data, self.pending_action[0])
                            
                            # çµŒé¨“ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ ï¼ˆdone=True: K=1è¨­è¨ˆï¼‰
                            self.replay_buffer.push(
                                self.pending_state,
                                self.pending_action,
                                reward,
                                next_state,
                                True  # K=1ãªã®ã§æ¯å›done=True
                            )
                            
                            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ›´æ–°
                            if len(self.replay_buffer) >= self.batch_size:
                                actor_loss, critic_loss, avg_q = self.update_networks()
                            else:
                                actor_loss, critic_loss, avg_q = 0, 0, 0
                            
                            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿è¨˜éŒ²
                            self._record_episode_data(episode, reward, actor_loss, critic_loss, avg_q)
                            
                            print(f"âœ… å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ: episode={len(self.episode_data)}, reward={reward:.2f}, buffer_size={len(self.replay_buffer)}")
                            
                            # é€²æ—è¡¨ç¤º
                            if len(self.episode_data) % 50 == 0:
                                self._print_learning_progress()
                            
                            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
                            if len(self.episode_data) % 100 == 0:
                                self._save_model()
                        else:
                            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: EPISODE_ENDãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆæ¨å¥¨è¿½åŠ æ©Ÿèƒ½ï¼‰
                            print(f"âš ï¸ PendingçŠ¶æ…‹ãªã—ã§EPISODE_ENDå—ä¿¡: episode={episode.episode_id}")
                            print(f"   ç›´è¿‘çŠ¶æ…‹: {self.last_tcp_data is not None}")
                            print(f"   ã“ã®å ´åˆã¯å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                        
                        # PendingçŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆ
                        self.pending_state = None
                        self.pending_action = None
                    
                    last_episode_count = current_episode_count
                
                # â˜… ãƒ‘ãƒƒãƒ2: è¡¨ç¤ºãƒ»åœæ­¢ç”¨ã®ã‚«ã‚¦ãƒ³ã‚¿ã¯ã€Œåé›†æ¸ˆã¿å®Ÿæ•°ã€ã«åŒæœŸ
                self.episode_count = current_episode_count
                
                time.sleep(0.1)
                
            except Exception as e:
                print(f"âŒ å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
                time.sleep(1.0)
        
        # â˜… ãƒ‘ãƒƒãƒ3: ãƒ«ãƒ¼ãƒ—çµ‚äº†æ™‚ã«ãƒ•ãƒ©ã‚°ã‚’è½ã¨ã—ã¦ã‹ã‚‰ä¿å­˜
        print(f"âœ… TypeAå­¦ç¿’ãƒ«ãƒ¼ãƒ—å®Œäº†: {self.episode_count}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰")
        self.is_running = False
        self._save_final_results()
    
    def _record_episode_data(self, episode, reward, actor_loss, critic_loss, avg_q):
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®è¨˜éŒ²"""
        grip_force = episode.tcp_data.get('grip_force', 0.0)
        contact = episode.tcp_data.get('contact', False)
        broken = episode.tcp_data.get('broken', False)
        
        # æˆåŠŸåˆ¤å®š
        success = self.force_min <= grip_force <= self.force_max
        
        # æŠŠæŒåŠ›èª¤å·®
        force_error = abs(grip_force - self.force_center)
        
        episode_info = {
            'episode': len(self.episode_data) + 1,  # å­¦ç¿’ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç•ªå·
            'reward': reward,
            'grip_force': grip_force,
            'success': success,
            'force_error': force_error,
            'contact': contact,
            'broken': broken,
            'actor_loss': actor_loss,
            'critic_loss': critic_loss,
            'avg_q_value': avg_q,
            'noise_sigma': self.noise.sigma
        }
        
        self.episode_data.append(episode_info)
    
    def _print_learning_progress(self):
        """å­¦ç¿’é€²æ—è¡¨ç¤º"""
        if len(self.episode_data) == 0:
            return
        
        # æœ€æ–°50ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®çµ±è¨ˆ
        recent_data = self.episode_data[-50:]
        recent_rewards = [d['reward'] for d in recent_data]
        recent_successes = [d['success'] for d in recent_data]
        recent_errors = [d['force_error'] for d in recent_data]
        recent_damages = [d['broken'] for d in recent_data]
        
        success_rate = np.mean(recent_successes)
        avg_reward = np.mean(recent_rewards)
        avg_error = np.mean(recent_errors)
        damage_rate = np.mean(recent_damages)
        
        print(f"\nğŸ“Š TypeAé€²æ— (å­¦ç¿’æ¸ˆã¿ {len(self.episode_data)}/{self.target_episodes}):")
        print(f"   å¹³å‡å ±é…¬ï¼ˆæœ€æ–°50ï¼‰: {avg_reward:.2f}")
        print(f"   æˆåŠŸç‡ï¼ˆæœ€æ–°50ï¼‰: {success_rate:.1%}")
        print(f"   å¹³å‡åŠ›èª¤å·®ï¼ˆæœ€æ–°50ï¼‰: {avg_error:.2f}N")
        print(f"   ç ´æç‡ï¼ˆæœ€æ–°50ï¼‰: {damage_rate:.1%}")
        print(f"   ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º: {len(self.replay_buffer)}")
        print(f"   æ¢ç´¢ãƒã‚¤ã‚ºÏƒ: {self.noise.sigma:.3f}")
    
    def _save_model(self):
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        try:
            model_path = os.path.join(self.output_dir, f'typea_model_ep{len(self.episode_data)}.pth')
            torch.save({
                'actor_state_dict': self.actor.state_dict(),
                'critic_state_dict': self.critic.state_dict(),
                'actor_optimizer': self.actor_optimizer.state_dict(),
                'critic_optimizer': self.critic_optimizer.state_dict(),
                'episode_count': len(self.episode_data),
                'experiment_type': self.experiment_type,
                'seed': self.seed
            }, model_path)
        except Exception as e:
            print(f"âš ï¸ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _calculate_advanced_metrics(self):
        """é«˜åº¦ãªæŒ‡æ¨™è¨ˆç®—"""
        if len(self.episode_data) == 0:
            return {}
        
        # DataFrameã«å¤‰æ›
        df = pd.DataFrame(self.episode_data)
        
        # ç§»å‹•å¹³å‡è¨ˆç®—ï¼ˆæˆåŠŸç‡ï¼‰
        success_ma = AnalysisUtils.moving_average(df['success'].values, window=100)
        
        # AUCè¨ˆç®—
        auc_all = AnalysisUtils.calculate_auc(success_ma)
        auc_0_400 = AnalysisUtils.calculate_auc(success_ma[:400]) if len(success_ma) >= 400 else auc_all
        
        # plateauæ¤œå‡º
        plateau_value, plateau_episode = AnalysisUtils.detect_plateau(success_ma, window=200, eps=1e-3)
        
        # time-to-70%
        time_to_70 = AnalysisUtils.find_time_to_threshold(success_ma, threshold=0.70)
        
        # æœ€çµ‚æ€§èƒ½ï¼ˆæœ€æ–°100ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å¹³å‡ï¼‰
        final_success_rate = np.mean(df['success'].iloc[-100:]) if len(df) >= 100 else np.mean(df['success'])
        final_reward = np.mean(df['reward'].iloc[-100:]) if len(df) >= 100 else np.mean(df['reward'])
        final_force_error = np.mean(df['force_error'].iloc[-100:]) if len(df) >= 100 else np.mean(df['force_error'])
        final_damage_rate = np.mean(df['broken'].iloc[-100:]) if len(df) >= 100 else np.mean(df['broken'])
        
        return {
            'auc_all': auc_all,
            'auc_0_400': auc_0_400,
            'plateau_value': plateau_value,
            'plateau_episode': plateau_episode,
            'time_to_70': time_to_70,
            'final_success_rate': final_success_rate,
            'final_reward': final_reward,
            'final_force_error': final_force_error,
            'final_damage_rate': final_damage_rate,
            'success_moving_average': success_ma.tolist()
        }
    
    def _save_final_results(self):
        """æœ€çµ‚çµæœä¿å­˜"""
        print(f"ğŸ’¾ æœ€çµ‚çµæœä¿å­˜ä¸­...")
        
        # â˜… ãƒ‘ãƒƒãƒé©ç”¨: ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã‚‚æœ€ä½é™ã®çµ±è¨ˆã¯ä¿å­˜
        if len(self.episode_data) == 0:
            print(f"âš ï¸ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ãŒã€åŸºæœ¬æƒ…å ±ã‚’ä¿å­˜ã—ã¾ã™")
            # æœ€ä½é™ã®æƒ…å ±ã‚’ä¿å­˜
            basic_stats = {
                'experiment_type': self.experiment_type,
                'seed': self.seed,
                'total_episodes': self.episode_count,
                'target_episodes': self.target_episodes,
                'learning_episodes': 0,
                'message': 'No learning data collected'
            }
            json_path = os.path.join(self.output_dir, 'final_stats.json')
            with open(json_path, 'w') as f:
                json.dump(basic_stats, f, indent=2)
            print(f"ğŸ“„ åŸºæœ¬æƒ…å ±ä¿å­˜: {json_path}")
            return
        
        # DataFrameä½œæˆ
        df = pd.DataFrame(self.episode_data)
        
        # ç§»å‹•å¹³å‡è¿½åŠ 
        df['success_rate_ma100'] = AnalysisUtils.moving_average(df['success'].values, window=100)
        df['reward_ma50'] = AnalysisUtils.moving_average(df['reward'].values, window=50)
        
        # learning_results.csvä¿å­˜
        csv_data = {
            'episode': df['episode'],
            'reward': df['reward'],
            'success_rate': df['success_rate_ma100'],  # ç§»å‹•å¹³å‡
            'force_error': df['force_error'],
            'damage_rate': df['broken'].astype(int)
        }
        results_df = pd.DataFrame(csv_data)
        csv_path = os.path.join(self.output_dir, 'learning_results.csv')
        results_df.to_csv(csv_path, index=False)
        
        # é«˜åº¦ãªæŒ‡æ¨™è¨ˆç®—
        advanced_metrics = self._calculate_advanced_metrics()
        
        # final_stats.jsonä¿å­˜
        final_stats = {
            'experiment_type': self.experiment_type,
            'seed': self.seed,
            'total_episodes': self.episode_count,
            'target_episodes': self.target_episodes,
            'learning_episodes': len(self.episode_data),
            **advanced_metrics,
            'reward_parameters': self.reward_params,
            'force_normalization': {
                'center': self.force_center,
                'halfwidth': self.force_halfwidth,
                'range': [self.force_min, self.force_max]
            }
        }
        
        json_path = os.path.join(self.output_dir, 'final_stats.json')
        with open(json_path, 'w') as f:
            json.dump(final_stats, f, indent=2)
        
        # master_auc.jsonä¿å­˜ï¼ˆé›†è¨ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆç”¨ï¼‰
        master_auc = {
            'auc_0_400': advanced_metrics['auc_0_400'],
            'auc_all': advanced_metrics['auc_all'],
            'time_to_70': advanced_metrics['time_to_70'],
            'plateau_value': advanced_metrics['plateau_value'],
            'plateau_at_episode': advanced_metrics['plateau_episode'],
            'final_success_rate_at_400': advanced_metrics['success_moving_average'][399] if len(advanced_metrics['success_moving_average']) > 399 else None,
            'final_success_rate': advanced_metrics['final_success_rate']
        }
        
        master_auc_path = os.path.join(self.output_dir, 'master_auc.json')
        with open(master_auc_path, 'w') as f:
            json.dump(master_auc, f, indent=2)
        
        # å­¦ç¿’æ›²ç·šãƒ—ãƒ­ãƒƒãƒˆ
        self._plot_learning_curves(df)
        
        print(f"âœ… æœ€çµ‚çµæœä¿å­˜å®Œäº†:")
        print(f"   CSV: {csv_path}")
        print(f"   çµ±è¨ˆ: {json_path}")
        print(f"   Master AUC: {master_auc_path}")
        print(f"   å­¦ç¿’ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {len(self.episode_data)}/{self.episode_count}")
        print(f"   æœ€çµ‚æˆåŠŸç‡: {advanced_metrics['final_success_rate']:.1%}")
        print(f"   AUC(0-400): {advanced_metrics['auc_0_400']:.2f}")
        print(f"   AUC(å…¨åŸŸ): {advanced_metrics['auc_all']:.2f}")
        if advanced_metrics['time_to_70'] is not None:
            print(f"   Time-to-70%: Episode {advanced_metrics['time_to_70']}")
        if advanced_metrics['plateau_value'] is not None:
            print(f"   Plateau: {advanced_metrics['plateau_value']:.3f} (Episode {advanced_metrics['plateau_episode']})")
    
    def _plot_learning_curves(self, df):
        """å­¦ç¿’æ›²ç·šã®ãƒ—ãƒ­ãƒƒãƒˆ"""
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle(f'TypeA DDPG Learning Curves ({self.experiment_type}, seed={self.seed})')
            
            episodes = df['episode'].values
            
            # æˆåŠŸç‡ï¼ˆç§»å‹•å¹³å‡ï¼‰
            axes[0, 0].plot(episodes, df['success_rate_ma100'], 'g-', linewidth=2, label='Success Rate (MA100)')
            axes[0, 0].axhline(y=0.7, color='r', linestyle='--', alpha=0.7, label='70% threshold')
            axes[0, 0].set_title('Success Rate (8-15N)')
            axes[0, 0].set_xlabel('Episode')
            axes[0, 0].set_ylabel('Success Rate')
            axes[0, 0].set_ylim(0, 1)
            axes[0, 0].legend()
            axes[0, 0].grid(True)
            
            # å ±é…¬
            axes[0, 1].plot(episodes, df['reward'], alpha=0.3, label='Raw')
            axes[0, 1].plot(episodes, df['reward_ma50'], 'r-', linewidth=2, label='MA50')
            axes[0, 1].set_title('Episode Rewards')
            axes[0, 1].set_xlabel('Episode')
            axes[0, 1].set_ylabel('Reward')
            axes[0, 1].legend()
            axes[0, 1].grid(True)
            
            # æŠŠæŒåŠ›èª¤å·®
            axes[1, 0].plot(episodes, df['force_error'], alpha=0.3)
            force_error_ma = AnalysisUtils.moving_average(df['force_error'].values, 50)
            axes[1, 0].plot(episodes, force_error_ma, 'orange', linewidth=2)
            axes[1, 0].set_title('Grip Force Error |F - F*|')
            axes[1, 0].set_xlabel('Episode')
            axes[1, 0].set_ylabel('Error (N)')
            axes[1, 0].grid(True)
            
            # Qå€¤ã¨Loss
            if 'avg_q_value' in df.columns and df['avg_q_value'].notna().any():
                q_values = df['avg_q_value'].dropna()
                q_episodes = df.loc[df['avg_q_value'].notna(), 'episode']
                axes[1, 1].plot(q_episodes, q_values, 'b-', alpha=0.7, label='Avg Q-value')
                axes[1, 1].set_ylabel('Q-value', color='b')
                axes[1, 1].tick_params(axis='y', labelcolor='b')
                
                # Actor Lossï¼ˆå³è»¸ï¼‰
                if 'actor_loss' in df.columns and df['actor_loss'].notna().any():
                    ax2 = axes[1, 1].twinx()
                    actor_losses = df['actor_loss'].dropna()
                    loss_episodes = df.loc[df['actor_loss'].notna(), 'episode']
                    ax2.plot(loss_episodes, actor_losses, 'r-', alpha=0.7, label='Actor Loss')
                    ax2.set_ylabel('Actor Loss', color='r')
                    ax2.tick_params(axis='y', labelcolor='r')
            
            axes[1, 1].set_title('Q-values & Actor Loss')
            axes[1, 1].set_xlabel('Episode')
            axes[1, 1].grid(True)
            
            plt.tight_layout()
            
            plot_path = os.path.join(self.output_dir, 'learning_curves.png')
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"ğŸ“ˆ å­¦ç¿’æ›²ç·šä¿å­˜: {plot_path}")
            
        except Exception as e:
            print(f"âš ï¸ ãƒ—ãƒ­ãƒƒãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def stop_learning(self):
        """å­¦ç¿’åœæ­¢ï¼ˆå³åŠ¹ãƒ‘ãƒƒãƒé©ç”¨æ¸ˆã¿ï¼‰"""
        print(f"ğŸ›‘ TypeAå­¦ç¿’åœæ­¢ä¸­...")
        
        self.is_running = False
        
        if self.episode_collector:
            self.episode_collector.stop_collection()
        
        if self.tcp_interface:
            self.tcp_interface.stop_server()
        
        # â˜… ãƒ‘ãƒƒãƒ5: åœæ­¢æ™‚ã®ä¿å­˜æ¡ä»¶ã‚’ã€Œãƒ‡ãƒ¼ã‚¿ã®æœ‰ç„¡ã€ã§åˆ¤å®š
        if len(self.episode_data) > 0:
            self._save_final_results()
        
        # å­¦ç¿’ã‚¹ãƒ¬ãƒƒãƒ‰ã®çµ‚äº†ã‚’å¾…ã¤ï¼ˆdaemon=Falseãªã®ã§ï¼‰
        if self.learning_thread and self.learning_thread.is_alive():
            print(f"â³ å­¦ç¿’ã‚¹ãƒ¬ãƒƒãƒ‰çµ‚äº†å¾…æ©Ÿä¸­...")
            self.learning_thread.join(timeout=10)  # æœ€å¤§10ç§’å¾…æ©Ÿ
            if self.learning_thread.is_alive():
                print(f"âš ï¸ å­¦ç¿’ã‚¹ãƒ¬ãƒƒãƒ‰ãŒ10ç§’ä»¥å†…ã«çµ‚äº†ã—ã¾ã›ã‚“ã§ã—ãŸ")
            else:
                print(f"âœ… å­¦ç¿’ã‚¹ãƒ¬ãƒƒãƒ‰æ­£å¸¸çµ‚äº†")
        
        print(f"âœ… TypeAå­¦ç¿’åœæ­¢å®Œäº†")

# é›†è¨ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
def aggregate_multiple_seeds(base_dir, experiment_type, seeds):
    """è¤‡æ•°ã‚·ãƒ¼ãƒ‰ã®çµæœã‚’é›†è¨ˆ"""
    print(f"ğŸ“Š è¤‡æ•°ã‚·ãƒ¼ãƒ‰çµæœé›†è¨ˆ: {experiment_type}, seeds={seeds}")
    
    all_results = []
    
    for seed in seeds:
        # ã‚·ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ¤œç´¢
        pattern = f"typea_{experiment_type}_seed{seed}_*"
        matching_dirs = list(Path(base_dir).glob(pattern))
        
        if not matching_dirs:
            print(f"âš ï¸ Seed {seed}ã®çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {pattern}")
            continue
        
        # æœ€æ–°ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨
        latest_dir = max(matching_dirs, key=lambda x: x.stat().st_mtime)
        master_auc_path = latest_dir / "master_auc.json"
        
        if master_auc_path.exists():
            with open(master_auc_path, 'r') as f:
                seed_result = json.load(f)
                seed_result['seed'] = seed
                all_results.append(seed_result)
            print(f"   Seed {seed}: èª­ã¿è¾¼ã¿å®Œäº†")
        else:
            print(f"âš ï¸ Seed {seed}: master_auc.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    if len(all_results) == 0:
        print(f"âŒ æœ‰åŠ¹ãªçµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # çµ±è¨ˆè¨ˆç®—
    metrics = ['auc_0_400', 'auc_all', 'final_success_rate', 'time_to_70', 'plateau_value']
    aggregated = {}
    
    for metric in metrics:
        values = [r[metric] for r in all_results if r.get(metric) is not None]
        if values:
            aggregated[f'{metric}_mean'] = np.mean(values)
            aggregated[f'{metric}_std'] = np.std(values)
            aggregated[f'{metric}_ci95'] = 1.96 * np.std(values) / np.sqrt(len(values))
            aggregated[f'{metric}_values'] = values
    
    aggregated['n_seeds'] = len(all_results)
    aggregated['seeds'] = [r['seed'] for r in all_results]
    aggregated['experiment_type'] = experiment_type
    
    # çµæœä¿å­˜
    output_path = Path(base_dir) / f"aggregated_{experiment_type}.json"
    with open(output_path, 'w') as f:
        json.dump(aggregated, f, indent=2)
    
    print(f"âœ… é›†è¨ˆçµæœä¿å­˜: {output_path}")
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print(f"\nğŸ“ˆ {experiment_type} é›†è¨ˆçµæœ (n={len(all_results)}):")
    for metric in ['auc_0_400', 'auc_all', 'final_success_rate']:
        if f'{metric}_mean' in aggregated:
            mean_val = aggregated[f'{metric}_mean']
            ci_val = aggregated[f'{metric}_ci95']
            print(f"   {metric}: {mean_val:.3f} Â± {ci_val:.3f}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description="TypeA DDPGå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå³åŠ¹ãƒ‘ãƒƒãƒé©ç”¨æ¸ˆã¿ï¼‰")
    parser.add_argument("--type", choices=["A_400", "A_long"], default="A_400",
                       help="å®Ÿé¨“ã‚¿ã‚¤ãƒ—")
    parser.add_argument("--seed", type=int, default=42,
                       help="ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰")
    parser.add_argument("--multi-seed", nargs="+", type=int,
                       help="è¤‡æ•°ã‚·ãƒ¼ãƒ‰å®Ÿè¡Œ (ä¾‹: --multi-seed 1 2 3 4 5)")
    parser.add_argument("--aggregate", action="store_true",
                       help="æ—¢å­˜çµæœã®é›†è¨ˆã®ã¿å®Ÿè¡Œ")
    
    args = parser.parse_args()
    
    print(f"ğŸ¤– TypeA DDPGå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå³åŠ¹ãƒ‘ãƒƒãƒé©ç”¨æ¸ˆã¿ï¼‰")
    print(f"=" * 60)
    print(f"ä¿®æ­£å†…å®¹:")
    print(f"  1. åœæ­¢æ¡ä»¶ã‚’ã€Œåé›†æ¸ˆã¿ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°ã€ã«åŸºã¥ã‹ã›ã‚‹")
    print(f"  2. å®Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°ã§episode_countã‚’æ›´æ–°")
    print(f"  3. ãƒ«ãƒ¼ãƒ—çµ‚äº†æ™‚ã«ãƒ•ãƒ©ã‚°ã‚’è½ã¨ã—ã¦ã‹ã‚‰ä¿å­˜")
    print(f"  4. ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ãƒ‡ãƒ¼ãƒ¢ãƒ³ã«ã—ãªã„")
    print(f"  5. åœæ­¢æ™‚ã®ä¿å­˜æ¡ä»¶ã‚’ã€Œãƒ‡ãƒ¼ã‚¿ã®æœ‰ç„¡ã€ã§åˆ¤å®š")
    print(f"=" * 60)
    
    base_output_dir = "DDPG_Python/logs"
    
    if args.aggregate:
        # é›†è¨ˆã®ã¿å®Ÿè¡Œ
        if args.multi_seed:
            aggregate_multiple_seeds(base_output_dir, args.type, args.multi_seed)
        else:
            print(f"âŒ --aggregate ã«ã¯ --multi-seed ãŒå¿…è¦ã§ã™")
        return
    
    if args.multi_seed:
        # è¤‡æ•°ã‚·ãƒ¼ãƒ‰å®Ÿè¡Œ
        print(f"ğŸ”„ è¤‡æ•°ã‚·ãƒ¼ãƒ‰å®Ÿè¡Œ: {args.type}, seeds={args.multi_seed}")
        
        for seed in args.multi_seed:
            print(f"\nğŸŒ± Seed {seed} é–‹å§‹...")
            
            system = TypeADDPGSystem(experiment_type=args.type, seed=seed)
            
            if system.run_learning():
                try:
                    # å­¦ç¿’å®Œäº†ã¾ã§å¾…æ©Ÿ
                    while system.is_running and system.episode_count < system.target_episodes:
                        time.sleep(10)
                        if system.episode_count % 100 == 0 and system.episode_count > 0:
                            print(f"   Seed {seed}: {system.episode_count}/{system.target_episodes}ep")
                    
                    if system.episode_count >= system.target_episodes:
                        print(f"âœ… Seed {seed} å®Œäº†!")
                    
                except KeyboardInterrupt:
                    print(f"â¹ï¸ Seed {seed} ä¸­æ–­")
                finally:
                    system.stop_learning()
            else:
                print(f"âŒ Seed {seed} é–‹å§‹å¤±æ•—")
        
        # è‡ªå‹•é›†è¨ˆ
        print(f"\nğŸ“Š è‡ªå‹•é›†è¨ˆå®Ÿè¡Œ...")
        aggregate_multiple_seeds(base_output_dir, args.type, args.multi_seed)
        
    else:
        # å˜ä¸€ã‚·ãƒ¼ãƒ‰å®Ÿè¡Œ
        system = TypeADDPGSystem(experiment_type=args.type, seed=args.seed)
        
        if system.run_learning():
            try:
                print(f"\nğŸ’¡ TypeAå­¦ç¿’å®Ÿè¡Œä¸­:")
                print(f"   å®Ÿé¨“ã‚¿ã‚¤ãƒ—: {args.type}")
                print(f"   ã‚·ãƒ¼ãƒ‰: {args.seed}")
                print(f"   ç›®æ¨™ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {system.target_episodes}")
                print(f"   Ctrl+C ã§çµ‚äº†")
                
                # é€²æ—ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
                last_progress_time = time.time()
                while system.is_running and system.episode_count < system.target_episodes:
                    time.sleep(5)
                    
                    # 30ç§’ã”ã¨ã«é€²æ—è¡¨ç¤º
                    current_time = time.time()
                    if current_time - last_progress_time >= 30:
                        print(f"ğŸ”„ é€²æ—: {system.episode_count}/{system.target_episodes}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰")
                        last_progress_time = current_time
                
                if system.episode_count >= system.target_episodes:
                    print(f"ğŸ‰ ç›®æ¨™ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°é”æˆï¼")
                
            except KeyboardInterrupt:
                print(f"\nâ¹ï¸ å­¦ç¿’ä¸­æ–­")
            finally:
                system.stop_learning()
        else:
            print(f"âŒ å­¦ç¿’é–‹å§‹å¤±æ•—")

if __name__ == "__main__":
    main()