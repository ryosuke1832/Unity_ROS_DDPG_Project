#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œå…¨æ”¹å–„ç‰ˆã‚°ãƒªãƒƒãƒ‘ãƒ¼åŠ›åˆ†é¡å™¨
å…ƒã®ã‚³ãƒ¼ãƒ‰ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æ–¹å¼ã‚’è¸è¥²ã—ã¤ã¤ã€ä»¥ä¸‹ã‚’æ”¹å–„:
1. EEGNetã‹ã‚‰çµ±è¨ˆçš„ç‰¹å¾´é‡+MLPã«å¤‰æ›´
2. ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾ç­–ï¼ˆSMOTE + é‡ã¿ä»˜ããƒ­ã‚¹ï¼‰
3. æ—©æœŸçµ‚äº†ã¨k-foldäº¤å·®æ¤œè¨¼
4. è©³ç´°ãªè©•ä¾¡æŒ‡æ¨™
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, f1_score
from sklearn.utils.class_weight import compute_class_weight
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline as ImbPipeline
import matplotlib.pyplot as plt
import seaborn as sns
import os
import glob
from datetime import datetime
from collections import Counter
import json
import warnings
from pathlib import Path
warnings.filterwarnings('ignore')

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ¯ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")

class GripForceDataset(Dataset):
    """æŠŠæŒåŠ›åˆ†é¡ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.LongTensor(y)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

class ImprovedGripForceClassifier(nn.Module):
    """çµ±è¨ˆçš„ç‰¹å¾´é‡ãƒ™ãƒ¼ã‚¹ã®æ”¹å–„ç‰ˆåˆ†é¡å™¨"""
    
    def __init__(self, input_size, num_classes=3, dropout_rate=0.4):
        super().__init__()
        
        # å…¥åŠ›æ­£è¦åŒ–
        self.input_bn = nn.BatchNorm1d(input_size)
        
        # ResNeté¢¨æ®‹å·®ãƒ–ãƒ­ãƒƒã‚¯
        hidden_sizes = [512, 256, 128, 64]
        self.blocks = nn.ModuleList()
        
        prev_size = input_size
        for hidden_size in hidden_sizes:
            block = self._make_residual_block(prev_size, hidden_size, dropout_rate)
            self.blocks.append(block)
            prev_size = hidden_size
        
        # åˆ†é¡å±¤
        self.classifier = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(prev_size, num_classes)
        )
        
        # é‡ã¿åˆæœŸåŒ–
        self.apply(self._init_weights)
    
    def _make_residual_block(self, in_features, out_features, dropout_rate):
        """æ®‹å·®ãƒ–ãƒ­ãƒƒã‚¯ä½œæˆ"""
        # ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆæ¥ç¶šç”¨
        if in_features != out_features:
            shortcut = nn.Linear(in_features, out_features)
        else:
            shortcut = nn.Identity()
        
        # ãƒ¡ã‚¤ãƒ³ãƒ‘ã‚¹
        main_path = nn.Sequential(
            nn.Linear(in_features, out_features),
            nn.BatchNorm1d(out_features),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(out_features, out_features),
            nn.BatchNorm1d(out_features),
        )
        
        return nn.ModuleDict({
            'main': main_path,
            'shortcut': shortcut
        })
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)
        elif isinstance(module, nn.BatchNorm1d):
            nn.init.constant_(module.weight, 1)
            nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        x = self.input_bn(x)
        
        for block in self.blocks:
            identity = block['shortcut'](x)
            x = block['main'](x) + identity
            x = torch.relu(x)
        
        return self.classifier(x)

class EarlyStopping:
    """æ—©æœŸçµ‚äº†"""
    def __init__(self, patience=25, min_delta=0.001, restore_best_weights=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_loss = None
        self.counter = 0
        self.best_weights = None
        
    def __call__(self, val_loss, model):
        if self.best_loss is None:
            self.best_loss = val_loss
            self.save_checkpoint(model)
        elif val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            self.save_checkpoint(model)
        else:
            self.counter += 1
            
        return self.counter >= self.patience
    
    def save_checkpoint(self, model):
        if self.restore_best_weights:
            self.best_weights = model.state_dict().copy()
    
    def restore(self, model):
        if self.best_weights:
            model.load_state_dict(self.best_weights)

def load_csv_data_from_episodes(csv_dir):
    """
    å…ƒã®ã‚³ãƒ¼ãƒ‰ã¨åŒæ§˜ã®æ–¹æ³•ã§ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰CSVãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    
    Args:
        csv_dir: CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        
    Returns:
        eeg_data_list: EEGãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ [(300, 32), ...]
        grip_force_labels: æŠŠæŒåŠ›ãƒ©ãƒ™ãƒ«ã®ãƒªã‚¹ãƒˆ [0, 1, 2, ...]
    """
    print(f"ğŸ“‚ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰CSVãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹: {csv_dir}")
    
    eeg_data_list = []
    grip_force_labels = []
    
    # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    info_files = glob.glob(os.path.join(csv_dir, "*_info.csv"))
    
    if not info_files:
        print(f"âŒ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_dir}")
        return eeg_data_list, grip_force_labels
    
    print(f"ğŸ“‹ ç™ºè¦‹ã—ãŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {len(info_files)}ä»¶")
    
    for info_file in sorted(info_files):
        try:
            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æƒ…å ±ã‚’èª­ã¿è¾¼ã¿
            info_df = pd.read_csv(info_file)
            episode_id = info_df['episode_id'].iloc[0]
            grip_force = info_df['grip_force'].iloc[0]
            
            # æŠŠæŒåŠ›ã‹ã‚‰ãƒ©ãƒ™ãƒ«ã‚’æ±ºå®š
            if grip_force < 8.0:
                label = 0  # UnderGrip
                label_name = "UnderGrip"
            elif grip_force > 15.0:
                label = 2  # OverGrip  
                label_name = "OverGrip"
            else:
                label = 1  # Success
                label_name = "Success"
            
            # å¯¾å¿œã™ã‚‹EEGãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            eeg_file = info_file.replace('_info.csv', '_eeg.csv')
            if os.path.exists(eeg_file):
                eeg_df = pd.read_csv(eeg_file)
                
                # EEGãƒ‡ãƒ¼ã‚¿éƒ¨åˆ†ã‚’æŠ½å‡ºï¼ˆãƒãƒ£ãƒ³ãƒãƒ«åˆ—ã®ã¿ï¼‰
                channel_cols = [col for col in eeg_df.columns if col.startswith('ch_')]
                eeg_data = eeg_df[channel_cols].values  # (samples, channels)
                
                # 300ã‚µãƒ³ãƒ—ãƒ«ï¼ˆ1.2ç§’ï¼‰ã«èª¿æ•´
                if eeg_data.shape[0] >= 300:
                    eeg_data = eeg_data[:300, :]  # æœ€åˆã®300ã‚µãƒ³ãƒ—ãƒ«
                    
                    # 32ãƒãƒ£ãƒ³ãƒãƒ«ã«èª¿æ•´
                    if eeg_data.shape[1] >= 32:
                        eeg_data = eeg_data[:, :32]
                    else:
                        # ãƒãƒ£ãƒ³ãƒãƒ«æ•°ãŒè¶³ã‚Šãªã„å ´åˆã¯ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
                        padding = np.zeros((300, 32 - eeg_data.shape[1]))
                        eeg_data = np.hstack([eeg_data, padding])
                    
                    eeg_data_list.append(eeg_data)
                    grip_force_labels.append(label)
                    
                    if len(eeg_data_list) % 100 == 0:
                        print(f"   èª­ã¿è¾¼ã¿é€²æ—: {len(eeg_data_list)}ä»¶")
                
            else:
                print(f"âš ï¸ EEGãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {eeg_file}")
                
        except Exception as e:
            print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {info_file}, {e}")
            continue
    
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(eeg_data_list)}ä»¶")
    
    # ãƒ©ãƒ™ãƒ«åˆ†å¸ƒç¢ºèª
    label_counts = Counter(grip_force_labels)
    label_names = ['UnderGrip', 'Success', 'OverGrip']
    print(f"ğŸ“Š ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ:")
    for i, name in enumerate(label_names):
        count = label_counts.get(i, 0)
        percentage = count / len(grip_force_labels) * 100 if grip_force_labels else 0
        print(f"   {name}: {count}ä»¶ ({percentage:.1f}%)")
    
    return eeg_data_list, grip_force_labels

def extract_eeg_features(eeg_data_list):
    """
    EEGãƒ‡ãƒ¼ã‚¿ã‹ã‚‰çµ±è¨ˆçš„ç‰¹å¾´é‡ã‚’æŠ½å‡º
    
    Args:
        eeg_data_list: EEGãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ [(300, 32), ...]
        
    Returns:
        features_array: ç‰¹å¾´é‡é…åˆ— (n_samples, n_features)
    """
    print("ğŸ”„ EEGãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡æŠ½å‡ºä¸­...")
    
    features_list = []
    
    for i, eeg_data in enumerate(eeg_data_list):
        features = []
        
        # å„ãƒãƒ£ãƒ³ãƒãƒ«ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡º
        for ch in range(eeg_data.shape[1]):  # 32ãƒãƒ£ãƒ³ãƒãƒ«
            ch_data = eeg_data[:, ch]
            
            # æ™‚é–“ãƒ‰ãƒ¡ã‚¤ãƒ³çµ±è¨ˆçš„ç‰¹å¾´é‡
            features.extend([
                np.mean(ch_data),              # å¹³å‡
                np.std(ch_data),               # æ¨™æº–åå·®
                np.var(ch_data),               # åˆ†æ•£
                np.min(ch_data),               # æœ€å°å€¤
                np.max(ch_data),               # æœ€å¤§å€¤
                np.median(ch_data),            # ä¸­å¤®å€¤
                np.percentile(ch_data, 25),    # ç¬¬1å››åˆ†ä½æ•°
                np.percentile(ch_data, 75),    # ç¬¬3å››åˆ†ä½æ•°
                np.ptp(ch_data),               # ãƒ¬ãƒ³ã‚¸ï¼ˆæœ€å¤§-æœ€å°ï¼‰
                len(ch_data[ch_data > 0]) / len(ch_data)  # æ­£ã®å€¤ã®å‰²åˆ
            ])
            
            # å‘¨æ³¢æ•°ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹å¾´é‡
            try:
                fft = np.fft.fft(ch_data)
                freqs = np.fft.fftfreq(len(ch_data), 1/250)  # 250Hz
                power_spectrum = np.abs(fft)**2
                
                # å„å‘¨æ³¢æ•°å¸¯åŸŸã®ãƒ‘ãƒ¯ãƒ¼
                # ãƒ‡ãƒ«ã‚¿æ³¢ (0.5-4Hz)
                delta_mask = (freqs >= 0.5) & (freqs <= 4)
                delta_power = np.mean(power_spectrum[delta_mask]) if np.any(delta_mask) else 0
                
                # ã‚·ãƒ¼ã‚¿æ³¢ (4-8Hz)
                theta_mask = (freqs >= 4) & (freqs <= 8)
                theta_power = np.mean(power_spectrum[theta_mask]) if np.any(theta_mask) else 0
                
                # ã‚¢ãƒ«ãƒ•ã‚¡æ³¢ (8-12Hz)
                alpha_mask = (freqs >= 8) & (freqs <= 12)
                alpha_power = np.mean(power_spectrum[alpha_mask]) if np.any(alpha_mask) else 0
                
                # ãƒ™ãƒ¼ã‚¿æ³¢ (12-30Hz)
                beta_mask = (freqs >= 12) & (freqs <= 30)
                beta_power = np.mean(power_spectrum[beta_mask]) if np.any(beta_mask) else 0
                
                # ã‚¬ãƒ³ãƒæ³¢ (30-100Hz)
                gamma_mask = (freqs >= 30) & (freqs <= 100)
                gamma_power = np.mean(power_spectrum[gamma_mask]) if np.any(gamma_mask) else 0
                
                features.extend([delta_power, theta_power, alpha_power, beta_power, gamma_power])
                
            except:
                # FFTã‚¨ãƒ©ãƒ¼æ™‚ã¯0ã§åŸ‹ã‚ã‚‹
                features.extend([0, 0, 0, 0, 0])
        
        # ãƒãƒ£ãƒ³ãƒãƒ«é–“ã®ç›¸é–¢ç‰¹å¾´é‡ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        try:
            corr_matrix = np.corrcoef(eeg_data.T)
            # ä¸Šä¸‰è§’è¡Œåˆ—ã®è¦ç´ ã‚’ç‰¹å¾´é‡ã¨ã—ã¦ä½¿ç”¨
            upper_tri_indices = np.triu_indices(32, k=1)
            corr_features = corr_matrix[upper_tri_indices]
            
            # ç›¸é–¢ã®çµ±è¨ˆé‡
            features.extend([
                np.mean(corr_features),
                np.std(corr_features),
                np.max(corr_features),
                np.min(corr_features)
            ])
        except:
            features.extend([0, 0, 0, 0])
        
        features_list.append(features)
        
        if i % 100 == 0 and i > 0:
            print(f"   ç‰¹å¾´é‡æŠ½å‡ºé€²æ—: {i}/{len(eeg_data_list)}")
    
    features_array = np.array(features_list)
    print(f"âœ… ç‰¹å¾´é‡æŠ½å‡ºå®Œäº†: {features_array.shape[1]}æ¬¡å…ƒ")
    
    return features_array

def prepare_data_with_balancing(data_source, balance_method='combined'):
    """
    ãƒ‡ãƒ¼ã‚¿æº–å‚™ã¨ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾ç­–
    
    Args:
        data_source: CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¾ãŸã¯ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
        balance_method: 'smote', 'undersample', 'combined', 'none'
        
    Returns:
        X_balanced, y_balanced, scaler, le, class_names
    """
    print("ğŸ“‚ ãƒ‡ãƒ¼ã‚¿æº–å‚™ä¸­...")
    
    if os.path.isdir(data_source):
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰èª­ã¿è¾¼ã¿
        print("ğŸ—‚ï¸ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰èª­ã¿è¾¼ã¿...")
        eeg_data_list, grip_force_labels = load_csv_data_from_episodes(data_source)
        
        if len(eeg_data_list) == 0:
            print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return None, None, None, None, None
        
        # EEGãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡æŠ½å‡º
        X = extract_eeg_features(eeg_data_list)
        y = np.array(grip_force_labels)
        
        # ã‚¯ãƒ©ã‚¹åè¨­å®š
        class_names = np.array(['UnderGrip', 'Success', 'OverGrip'])
        le = LabelEncoder()
        le.classes_ = class_names
        
    else:
        # å˜ä¸€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
        print("ğŸ“„ å˜ä¸€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿...")
        df = pd.read_csv(data_source)
        
        # ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        le = LabelEncoder()
        y = le.fit_transform(df['result'].values)
        class_names = le.classes_
        
        # ç‰¹å¾´é‡æº–å‚™
        feature_cols = [col for col in df.columns if col != 'result']
        X = df[feature_cols].values
    
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†:")
    print(f"   ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X)}")
    print(f"   ç‰¹å¾´é‡æ•°: {X.shape[1]}")
    print(f"   ã‚¯ãƒ©ã‚¹: {class_names}")
    
    # ã‚¯ãƒ©ã‚¹åˆ†å¸ƒç¢ºèª
    unique, counts = np.unique(y, return_counts=True)
    for i, (class_idx, count) in enumerate(zip(unique, counts)):
        print(f"   {class_names[class_idx]}: {count}ä»¶ ({count/len(y)*100:.1f}%)")
    
    # ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾ç­–
    if balance_method == 'smote':
        print("ğŸ”„ SMOTEé©ç”¨ä¸­...")
        k_neighbors = min(3, min(counts) - 1) if min(counts) > 1 else 1
        smote = SMOTE(random_state=42, k_neighbors=k_neighbors)
        X_balanced, y_balanced = smote.fit_resample(X_scaled, y)
        
    elif balance_method == 'undersample':
        print("ğŸ”„ ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é©ç”¨ä¸­...")
        undersampler = RandomUnderSampler(random_state=42)
        X_balanced, y_balanced = undersampler.fit_resample(X_scaled, y)
        
    elif balance_method == 'combined':
        print("ğŸ”„ SMOTE + ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é©ç”¨ä¸­...")
        k_neighbors = min(3, min(counts) - 1) if min(counts) > 1 else 1
        pipeline = ImbPipeline([
            ('smote', SMOTE(random_state=42, k_neighbors=k_neighbors)),
            ('undersample', RandomUnderSampler(random_state=42))
        ])
        X_balanced, y_balanced = pipeline.fit_resample(X_scaled, y)
        
    else:  # 'none'
        X_balanced, y_balanced = X_scaled, y
    
    # ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°å¾Œã®åˆ†å¸ƒç¢ºèª
    if balance_method != 'none':
        print(f"ğŸ“Š ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°å¾Œ:")
        unique, counts = np.unique(y_balanced, return_counts=True)
        for class_idx, count in zip(unique, counts):
            print(f"   {class_names[class_idx]}: {count}ä»¶ ({count/len(y_balanced)*100:.1f}%)")
    
    return X_balanced, y_balanced, scaler, le, class_names

def train_model_with_kfold(X, y, class_names, n_splits=5, epochs=200):
    """k-foldäº¤å·®æ¤œè¨¼ã§ã®å­¦ç¿’"""
    print(f"ğŸ”„ {n_splits}-foldäº¤å·®æ¤œè¨¼é–‹å§‹")
    
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    fold_results = []
    
    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
        print(f"\nğŸ“‹ Fold {fold + 1}/{n_splits}")
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]
        
        # ã‚¯ãƒ©ã‚¹é‡ã¿è¨ˆç®—
        class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
        class_weights = torch.FloatTensor(class_weights).to(device)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        train_dataset = GripForceDataset(X_train, y_train)
        val_dataset = GripForceDataset(X_val, y_val)
        
        batch_size = min(32, len(train_dataset))
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        model = ImprovedGripForceClassifier(
            input_size=X.shape[1], 
            num_classes=len(class_names),
            dropout_rate=0.4
        ).to(device)
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã¨ãƒ­ã‚¹é–¢æ•°
        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
        criterion = nn.CrossEntropyLoss(weight=class_weights)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=15, min_lr=1e-6
        )
        
        # æ—©æœŸçµ‚äº†
        early_stopping = EarlyStopping(patience=30, min_delta=0.001)
        
        # å­¦ç¿’å±¥æ­´
        history = {'train_loss': [], 'val_loss': [], 'val_f1': []}
        
        # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
        best_val_f1 = 0
        for epoch in range(epochs):
            # è¨“ç·´
            model.train()
            train_loss = 0
            for batch_X, batch_y in train_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                
                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                
                train_loss += loss.item()
            
            # æ¤œè¨¼
            model.eval()
            val_loss = 0
            val_preds = []
            val_true = []
            
            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                    outputs = model(batch_X)
                    loss = criterion(outputs, batch_y)
                    val_loss += loss.item()
                    
                    _, predicted = outputs.max(1)
                    val_preds.extend(predicted.cpu().numpy())
                    val_true.extend(batch_y.cpu().numpy())
            
            train_loss /= len(train_loader)
            val_loss /= len(val_loader)
            val_f1 = f1_score(val_true, val_preds, average='weighted')
            
            history['train_loss'].append(train_loss)
            history['val_loss'].append(val_loss)
            history['val_f1'].append(val_f1)
            
            scheduler.step(val_loss)
            
            # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«æ›´æ–°
            if val_f1 > best_val_f1:
                best_val_f1 = val_f1
            
            # é€²æ—è¡¨ç¤º
            if epoch % 25 == 0 or epoch == epochs - 1:
                lr = optimizer.param_groups[0]['lr']
                print(f"  Epoch {epoch:3d}: Train Loss={train_loss:.4f}, "
                      f"Val Loss={val_loss:.4f}, Val F1={val_f1:.3f}, LR={lr:.2e}")
            
            # æ—©æœŸçµ‚äº†ãƒã‚§ãƒƒã‚¯
            if early_stopping(val_loss, model):
                print(f"  ğŸ›‘ æ—©æœŸçµ‚äº† (Epoch {epoch})")
                break
        
        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«å¾©å…ƒ
        early_stopping.restore(model)
        
        # æœ€çµ‚è©•ä¾¡
        model.eval()
        final_preds = []
        final_true = []
        
        with torch.no_grad():
            for batch_X, batch_y in val_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                outputs = model(batch_X)
                _, predicted = outputs.max(1)
                final_preds.extend(predicted.cpu().numpy())
                final_true.extend(batch_y.cpu().numpy())
        
        fold_f1 = f1_score(final_true, final_preds, average='weighted')
        fold_acc = np.mean(np.array(final_preds) == np.array(final_true))
        
        fold_results.append({
            'fold': fold + 1,
            'accuracy': fold_acc,
            'f1_score': fold_f1,
            'model': model.state_dict().copy(),
            'history': history
        })
        
        print(f"  âœ… Fold {fold + 1} å®Œäº†: Acc={fold_acc:.3f}, F1={fold_f1:.3f}")
    
    return fold_results

def evaluate_model(model, X_test, y_test, class_names):
    """è©³ç´°ãªãƒ¢ãƒ‡ãƒ«è©•ä¾¡"""
    print("ğŸ” æœ€çµ‚ãƒ†ã‚¹ãƒˆè©•ä¾¡ä¸­...")
    
    test_dataset = GripForceDataset(X_test, y_test)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    model.eval()
    test_preds = []
    test_true = []
    
    with torch.no_grad():
        for batch_X, batch_y in test_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            outputs = model(batch_X)
            _, predicted = outputs.max(1)
            
            test_preds.extend(predicted.cpu().numpy())
            test_true.extend(batch_y.cpu().numpy())
    
    test_acc = np.mean(np.array(test_preds) == np.array(test_true))
    test_f1 = f1_score(test_true, test_preds, average='weighted')
    
    print(f"ğŸ“Š æœ€çµ‚ãƒ†ã‚¹ãƒˆçµæœ:")
    print(f"   ç²¾åº¦: {test_acc:.3f}")
    print(f"   F1ã‚¹ã‚³ã‚¢: {test_f1:.3f}")
    
    # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ
    print(f"\nğŸ“‹ è©³ç´°åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
    print(classification_report(test_true, test_preds, target_names=class_names))
    
    # æ··åŒè¡Œåˆ—
    cm = confusion_matrix(test_true, test_preds)
    
    try:
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=class_names, yticklabels=class_names)
        plt.title('æ”¹å–„ç‰ˆæŠŠæŒåŠ›åˆ†é¡ - æ··åŒè¡Œåˆ—')
        plt.ylabel('å®Ÿéš›ã®ã‚¯ãƒ©ã‚¹')
        plt.xlabel('äºˆæ¸¬ã‚¯ãƒ©ã‚¹')
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        BASE_DIR = Path(__file__).resolve().parent
        cm_path = BASE_DIR / "models" / f"confusion_matrix_improved_{timestamp}.png"
        os.makedirs(cm_path.parent, exist_ok=True)
        plt.savefig(cm_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"æ··åŒè¡Œåˆ—ä¿å­˜: {cm_path}")
    except Exception as e:
        print(f"æ··åŒè¡Œåˆ—ä¿å­˜ã‚¨ãƒ©ãƒ¼ï¼ˆç„¡è¦–å¯èƒ½ï¼‰: {e}")
    
    return {
        'accuracy': test_acc,
        'f1_score': test_f1,
        'classification_report': classification_report(test_true, test_preds, target_names=class_names, output_dict=True),
        'confusion_matrix': cm.tolist()
    }

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ å®Œå…¨æ”¹å–„ç‰ˆã‚°ãƒªãƒƒãƒ‘ãƒ¼åŠ›åˆ†é¡å™¨")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹é¸æŠ
    print("\nãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’é¸æŠã—ã¦ãã ã•ã„:")
    print("1. ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆå…ƒã®ã‚³ãƒ¼ãƒ‰ã¨åŒã˜æ–¹å¼ï¼‰")
    print("2. å˜ä¸€CSVãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆepisode_data_raw.csvï¼‰")
    
    choice = input("é¸æŠ (1-2): ").strip()
    
    if choice == "1":
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰èª­ã¿è¾¼ã¿
        csv_dir = input("ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ï¼ˆç©ºç™½ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¤œç´¢ï¼‰: ").strip()
        
        if not csv_dir:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹æ¤œç´¢
            log_dirs = glob.glob("logs/episodes_20250908_*")
            if not log_dirs:
                log_dirs = glob.glob("DDPG_Python/logs/episodes_20250908_*")
            
            if log_dirs:
                csv_dir = max(log_dirs)  # æœ€æ–°ã®ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
                print(f"ğŸ” ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½¿ç”¨: {csv_dir}")
            else:
                print(f"âŒ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return
        
        if not os.path.exists(csv_dir):
            print(f"âŒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {csv_dir}")
            return
            
        data_source = csv_dir
        
    else:
        # å˜ä¸€CSVãƒ•ã‚¡ã‚¤ãƒ«
        csv_file = 'episode_data_raw.csv'
        if not os.path.exists(csv_file):
            print(f"âŒ CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_file}")
            return
        data_source = csv_file
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾ç­–ä»˜ãï¼‰
    print("\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿æº–å‚™ä¸­...")
    result = prepare_data_with_balancing(
        data_source, 
        balance_method='combined'  # 'smote', 'undersample', 'combined', 'none'
    )
    
    if result[0] is None:
        print("âŒ ãƒ‡ãƒ¼ã‚¿æº–å‚™ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
        
    X, y, scaler, le, class_names = result
    
    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=0.15, random_state=42, stratify=y
    )
    
    print(f"ğŸ“Š æœ€çµ‚ãƒ‡ãƒ¼ã‚¿åˆ†å‰²:")
    print(f"   å­¦ç¿’+æ¤œè¨¼: {len(X_temp)}ä»¶")
    print(f"   ãƒ†ã‚¹ãƒˆ: {len(X_test)}ä»¶")
    
    # k-foldäº¤å·®æ¤œè¨¼ã§å­¦ç¿’
    fold_results = train_model_with_kfold(X_temp, y_temp, class_names, n_splits=5, epochs=200)
    
    # æœ€é«˜æ€§èƒ½ã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
    best_fold = max(fold_results, key=lambda x: x['f1_score'])
    print(f"\nğŸ† æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«: Fold {best_fold['fold']} (F1={best_fold['f1_score']:.3f})")
    
    # æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã§æœ€çµ‚è©•ä¾¡
    best_model = ImprovedGripForceClassifier(
        input_size=X.shape[1], 
        num_classes=len(class_names)
    ).to(device)
    best_model.load_state_dict(best_fold['model'])
    
    test_results = evaluate_model(best_model, X_test, y_test, class_names)
    
    # çµæœä¿å­˜
    os.makedirs('models', exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    BASE_DIR = Path(__file__).resolve().parent  
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    model_path = BASE_DIR / "models" / f"improved_grip_force_classifier_{timestamp}.pth"
    torch.save({
        'model_state_dict': best_model.state_dict(),
        'scaler': scaler,
        'label_encoder': le,
        'class_names': class_names,
        'input_size': X.shape[1],
        'test_results': test_results,
        'fold_results': fold_results
    }, model_path)
    
    # çµæœä¿å­˜
    results = {
        'timestamp': timestamp,
        'data_source': data_source,
        'total_samples': len(X),
        'features_count': X.shape[1],
        'test_accuracy': test_results['accuracy'],
        'test_f1_score': test_results['f1_score'],
        'cross_validation_scores': [r['f1_score'] for r in fold_results],
        'mean_cv_f1': np.mean([r['f1_score'] for r in fold_results]),
        'std_cv_f1': np.std([r['f1_score'] for r in fold_results]),
        'classification_report': test_results['classification_report'],
        'confusion_matrix': test_results['confusion_matrix'],
        'class_names': class_names.tolist()
    }

    result_path = BASE_DIR / "models" / f"improved_results_{timestamp}.json"
    with open(result_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… å­¦ç¿’å®Œäº†!")
    print(f"   ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_path}")
    print(f"   çµæœä¿å­˜: {result_path}")
    print(f"   ãƒ†ã‚¹ãƒˆç²¾åº¦: {test_results['accuracy']:.3f}")
    print(f"   ãƒ†ã‚¹ãƒˆF1: {test_results['f1_score']:.3f}")
    print(f"   CVå¹³å‡F1: {np.mean([r['f1_score'] for r in fold_results]):.3f} Â± {np.std([r['f1_score'] for r in fold_results]):.3f}")
    
    # æ”¹å–„çŠ¶æ³ã‚’ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print(f"\nğŸ“ˆ æ”¹å–„ã‚µãƒãƒªãƒ¼:")
    print(f"   ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿æ•°: {len(X):,}ä»¶ï¼ˆå…ƒã®1000ä»¶ã™ã¹ã¦ä½¿ç”¨ï¼‰")
    print(f"   ç‰¹å¾´é‡æ•°: {X.shape[1]}æ¬¡å…ƒï¼ˆçµ±è¨ˆçš„ç‰¹å¾´é‡ï¼‰")
    print(f"   ãƒ¢ãƒ‡ãƒ«: ResNeté¢¨MLPï¼ˆEEGNetã‹ã‚‰å¤‰æ›´ï¼‰")
    print(f"   ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾ç­–: SMOTE + ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°")
    print(f"   äº¤å·®æ¤œè¨¼: 5-fold")
    print(f"   æ—©æœŸçµ‚äº†: 30ã‚¨ãƒãƒƒã‚¯ patience")
    
    if test_results['accuracy'] > 0.7:
        print(f"ğŸ‰ å„ªç§€ãªæ€§èƒ½ã§ã™ï¼ ãƒ†ã‚¹ãƒˆç²¾åº¦ {test_results['accuracy']:.1%}")
    elif test_results['accuracy'] > 0.6:
        print(f"ğŸ‘ è‰¯å¥½ãªæ€§èƒ½ã§ã™ã€‚ãƒ†ã‚¹ãƒˆç²¾åº¦ {test_results['accuracy']:.1%}")
    else:
        print(f"ğŸ“Š æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚ãƒ†ã‚¹ãƒˆç²¾åº¦ {test_results['accuracy']:.1%}")
        print(f"   ã•ã‚‰ãªã‚‹ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚„ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")

def test_model_loading():
    """ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ")
    
    # æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    model_files = glob.glob("models/improved_grip_force_classifier_*.pth")
    if not model_files:
        print("âŒ ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return False
    
    latest_model = max(model_files)
    print(f"ãƒ†ã‚¹ãƒˆå¯¾è±¡: {latest_model}")
    
    try:
        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        checkpoint = torch.load(latest_model, map_location=device)
        
        input_size = checkpoint['input_size']
        class_names = checkpoint['class_names']
        
        model = ImprovedGripForceClassifier(
            input_size=input_size,
            num_classes=len(class_names)
        ).to(device)
        
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        
        # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ
        dummy_input = torch.randn(1, input_size).to(device)
        
        with torch.no_grad():
            output = model(dummy_input)
            probabilities = torch.softmax(output, dim=1)
            predicted_class = torch.argmax(probabilities, dim=1).item()
            confidence = probabilities[0, predicted_class].item()
        
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆæˆåŠŸ!")
        print(f"   å…¥åŠ›ã‚µã‚¤ã‚º: {input_size}")
        print(f"   ã‚¯ãƒ©ã‚¹æ•°: {len(class_names)}")
        print(f"   äºˆæ¸¬ã‚¯ãƒ©ã‚¹: {class_names[predicted_class]}")
        print(f"   ä¿¡é ¼åº¦: {confidence:.3f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        return False

if __name__ == "__main__":
    import sys
    
    print("ğŸ§  å®Œå…¨æ”¹å–„ç‰ˆã‚°ãƒªãƒƒãƒ‘ãƒ¼åŠ›åˆ†é¡å™¨")
    print("å…ƒã®ã‚³ãƒ¼ãƒ‰ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æ–¹å¼ã‚’è¸è¥²ã—ã€çµ±è¨ˆçš„ç‰¹å¾´é‡+MLPã§å¤§å¹…æ”¹å–„")
    print("UnderGrip(<8N), Success(8-15N), OverGrip(>15N)")
    print("")
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§ã®å®Ÿè¡Œ
    if len(sys.argv) > 1:
        if sys.argv[1] == "test_model":
            test_model_loading()
        else:
            main()
    else:
        main()