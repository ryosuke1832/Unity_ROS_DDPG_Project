#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DDPG ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 

æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’çµ±åˆ:
1. e_tcp_lsl_sync_system.py - LSL/TCPãƒ‡ãƒ¼ã‚¿å—ä¿¡
2. g_grip_force_realtime_classifier.py - EEGåˆ†é¡
3. DDPGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã‚‹æŠŠæŒåŠ›æœ€é©åŒ–å­¦ç¿’
4. Unityå´ã¸ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é€ä¿¡

ãƒ•ãƒ­ãƒ¼:
Unity â†’ TCP/LSLãƒ‡ãƒ¼ã‚¿ â†’ EEGåˆ†é¡ â†’ DDPGå­¦ç¿’ â†’ æŠŠæŒåŠ›ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ â†’ Unity
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import random
import time
import threading
import queue
import json
import os
from datetime import datetime
from collections import deque, namedtuple
from typing import Dict, List, Optional, Tuple, Any
import matplotlib.pyplot as plt

# æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from e_tcp_lsl_sync_system import LSLTCPEpisodeCollector, Episode
from g_grip_force_realtime_classifier import RealtimeGripForceClassifier
from c_unity_tcp_interface import EEGTCPInterface

# PyTorchè¨­å®š
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ”§ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")

# çµŒé¨“ãƒãƒƒãƒ•ã‚¡ç”¨ã®namedtuple
Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])

class DDPGActor(nn.Module):
    """DDPG ã‚¢ã‚¯ã‚¿ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆæŠŠæŒåŠ›ã‚’å‡ºåŠ›ï¼‰"""
    
    def __init__(self, state_dim=7, action_dim=1, hidden_dim=256, max_action=1.0):
        super(DDPGActor, self).__init__()
        
        self.max_action = max_action
        
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)
        
        # Layer Normalization (BatchNormã®ä»£æ›¿ã€å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«å¯¾å¿œ)
        self.ln1 = nn.LayerNorm(hidden_dim)
        self.ln2 = nn.LayerNorm(hidden_dim)
        
        # Dropout for regularization
        self.dropout = nn.Dropout(0.1)
        
        # é‡ã¿åˆæœŸåŒ–
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)
    
    def forward(self, state):
        x = F.relu(self.ln1(self.fc1(state)))
        x = self.dropout(x)
        x = F.relu(self.ln2(self.fc2(x)))
        x = self.dropout(x)
        x = torch.tanh(self.fc3(x))  # [-1, 1]ã®ç¯„å›²ã«æ­£è¦åŒ–
        return x * self.max_action

class DDPGCritic(nn.Module):
    """DDPG ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆQå€¤ã‚’å‡ºåŠ›ï¼‰"""
    
    def __init__(self, state_dim=7, action_dim=1, hidden_dim=256):
        super(DDPGCritic, self).__init__()
        
        # State pathway
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.ln1 = nn.LayerNorm(hidden_dim)  # LayerNormä½¿ç”¨
        
        # Combined pathway (state + action)
        self.fc2 = nn.Linear(hidden_dim + action_dim, hidden_dim)
        self.ln2 = nn.LayerNorm(hidden_dim)  # LayerNormä½¿ç”¨
        self.fc3 = nn.Linear(hidden_dim, 1)
        
        # Dropout for regularization
        self.dropout = nn.Dropout(0.1)
        
        # é‡ã¿åˆæœŸåŒ–
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)
    
    def forward(self, state, action):
        x = F.relu(self.ln1(self.fc1(state)))
        x = self.dropout(x)
        x = torch.cat([x, action], dim=1)
        x = F.relu(self.ln2(self.fc2(x)))
        x = self.dropout(x)
        q_value = self.fc3(x)
        return q_value

class OUNoise:
    """Ornstein-Uhlenbeck ãƒã‚¤ã‚ºï¼ˆæ¢ç´¢ç”¨ï¼‰"""
    
    def __init__(self, action_dim, mu=0.0, theta=0.15, sigma=0.2):
        self.action_dim = action_dim
        self.mu = mu
        self.theta = theta
        self.sigma = sigma
        self.state = np.ones(self.action_dim) * self.mu
        self.reset()
    
    def reset(self):
        self.state = np.ones(self.action_dim) * self.mu
    
    def sample(self):
        x = self.state
        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))
        self.state = x + dx
        return self.state

class ReplayBuffer:
    """çµŒé¨“å†ç”Ÿãƒãƒƒãƒ•ã‚¡"""
    
    def __init__(self, capacity=50000):
        self.buffer = deque(maxlen=capacity)
        self.capacity = capacity
    
    def push(self, state, action, reward, next_state, done):
        """çµŒé¨“ã‚’è¿½åŠ """
        experience = Experience(state, action, reward, next_state, done)
        self.buffer.append(experience)
    
    def sample(self, batch_size):
        """ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        if len(self.buffer) < batch_size:
            return None
        
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = map(np.stack, zip(*batch))
        return state, action, reward, next_state, done
    
    def __len__(self):
        return len(self.buffer)

class DDPGAgent:
    """DDPGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self, state_dim=7, action_dim=1, lr_actor=1e-4, lr_critic=1e-3, gamma=0.99, tau=0.005):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.tau = tau
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆæœŸåŒ–
        self.actor = DDPGActor(state_dim, action_dim).to(device)
        self.actor_target = DDPGActor(state_dim, action_dim).to(device)
        self.critic = DDPGCritic(state_dim, action_dim).to(device)
        self.critic_target = DDPGCritic(state_dim, action_dim).to(device)
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®åˆæœŸåŒ–
        self.hard_update(self.actor_target, self.actor)
        self.hard_update(self.critic_target, self.critic)
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)
        
        # çµŒé¨“å†ç”Ÿãƒãƒƒãƒ•ã‚¡
        self.memory = ReplayBuffer()
        
        # ãƒã‚¤ã‚º
        self.noise = OUNoise(action_dim)
        
        # å­¦ç¿’çµ±è¨ˆ
        self.actor_losses = []
        self.critic_losses = []
    
    def hard_update(self, target, source):
        """ãƒãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆï¼ˆå®Œå…¨ã‚³ãƒ”ãƒ¼ï¼‰"""
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(param.data)
    
    def soft_update(self, target, source, tau):
        """ã‚½ãƒ•ãƒˆã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆï¼ˆå¾ã€…ã«æ›´æ–°ï¼‰"""
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)
    
    def select_action(self, state, add_noise=True, noise_scale=0.1):
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³é¸æŠ"""
        state = torch.FloatTensor(state.reshape(1, -1)).to(device)
        
        # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®šï¼ˆBatchNormå¯¾å¿œï¼‰
        self.actor.eval()
        
        with torch.no_grad():
            action = self.actor(state).cpu().data.numpy().flatten()
        
        # å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ã«æˆ»ã™
        self.actor.train()
        
        if add_noise:
            noise = self.noise.sample() * noise_scale
            action = np.clip(action + noise, -1.0, 1.0)
        
        return action
    
    def update(self, batch_size=64):
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ›´æ–°"""
        sample = self.memory.sample(batch_size)
        if sample is None:
            return
        
        # å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
        self.actor.train()
        self.critic.train()
        
        state, action, reward, next_state, done = sample
        
        state = torch.FloatTensor(state).to(device)
        action = torch.FloatTensor(action).to(device)
        reward = torch.FloatTensor(reward).to(device)
        next_state = torch.FloatTensor(next_state).to(device)
        done = torch.FloatTensor(done).to(device)
        
        # Criticã®æ›´æ–°
        with torch.no_grad():
            next_action = self.actor_target(next_state)
            target_q = self.critic_target(next_state, next_action)
            target_q = reward + (self.gamma * target_q * (1 - done))
        
        current_q = self.critic(state, action)
        critic_loss = F.mse_loss(current_q, target_q)
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=1.0)
        self.critic_optimizer.step()
        
        # Actorã®æ›´æ–°
        actor_loss = -self.critic(state, self.actor(state)).mean()
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1.0)
        self.actor_optimizer.step()
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ›´æ–°
        self.soft_update(self.actor_target, self.actor, self.tau)
        self.soft_update(self.critic_target, self.critic, self.tau)
        
        # çµ±è¨ˆè¨˜éŒ²
        self.actor_losses.append(actor_loss.item())
        self.critic_losses.append(critic_loss.item())

class GripForceEnvironment:
    """æŠŠæŒåŠ›ç’°å¢ƒï¼ˆçŠ¶æ…‹ç®¡ç†ãƒ»å ±é…¬è¨ˆç®—ï¼‰"""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        """ç’°å¢ƒãƒªã‚»ãƒƒãƒˆ"""
        self.episode_count = 0
        self.success_count = 0
        self.total_reward = 0.0
        self.previous_action = 0.0
    
    def create_state(self, classification_result, tcp_data, previous_action=0.0):
        """
        çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ä½œæˆ
        
        Returns:
            state: [class_onehot(3), grip_force(1), contact(1), broken(1), prev_action(1)] = 7æ¬¡å…ƒ
        """
        # åˆ†é¡çµæœã‚’ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        class_onehot = [0, 0, 0]
        if classification_result is not None:
            if isinstance(classification_result, dict):
                class_idx = classification_result.get('predicted_class_idx', 0)
            else:
                class_idx = classification_result
            
            if 0 <= class_idx <= 2:
                class_onehot[class_idx] = 1
        
        # TCP ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡æŠ½å‡º
        grip_force = tcp_data.get('grip_force', 10.0) / 30.0  # æ­£è¦åŒ– [0-30N] -> [0-1]
        contact = 1.0 if tcp_data.get('contact', False) else 0.0
        broken = 1.0 if tcp_data.get('broken', False) else 0.0
        
        # çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ä½œæˆ
        state = np.array(class_onehot + [grip_force, contact, broken, previous_action], dtype=np.float32)
        return state
    
    def calculate_reward(self, classification_result, tcp_data, action_value):
        """
        å ±é…¬è¨ˆç®—
        
        Args:
            classification_result: EEGåˆ†é¡çµæœ
            tcp_data: TCPãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ­ãƒœãƒƒãƒˆçŠ¶æ…‹ï¼‰
            action_value: DDPGã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®å€¤
            
        Returns:
            reward: è¨ˆç®—ã•ã‚ŒãŸå ±é…¬å€¤
        """
        reward = 0.0
        
        # åˆ†é¡çµæœã«åŸºã¥ãåŸºæœ¬å ±é…¬
        if isinstance(classification_result, dict):
            predicted_class = classification_result.get('predicted_class', 'Success')
            confidence = classification_result.get('confidence', 0.5)
        else:
            class_names = ['UnderGrip', 'Success', 'OverGrip']
            predicted_class = class_names[classification_result] if 0 <= classification_result <= 2 else 'Success'
            confidence = 0.8
        
        # ã‚¯ãƒ©ã‚¹åˆ¥å ±é…¬
        if predicted_class == 'Success':
            reward += 10.0 * confidence  # æˆåŠŸã«å¯¾ã™ã‚‹é«˜ã„å ±é…¬
            self.success_count += 1
        elif predicted_class == 'UnderGrip':
            reward -= 3.0 * confidence   # è»½ã„æŠŠæŒä¸è¶³ãƒšãƒŠãƒ«ãƒ†ã‚£
        elif predicted_class == 'OverGrip':
            reward -= 8.0 * confidence   # é‡ã„éåº¦æŠŠæŒãƒšãƒŠãƒ«ãƒ†ã‚£
        
        # æ¥è§¦æˆåŠŸå ±é…¬
        if tcp_data.get('contact', False):
            reward += 5.0
            # æ¥è§¦åŠ›ã«åŸºã¥ãè¿½åŠ å ±é…¬
            contact_force = tcp_data.get('contact_force', 0)
            if 0 < contact_force < 20:  # é©åº¦ãªæ¥è§¦åŠ›
                reward += 2.0
        else:
            reward -= 2.0  # æ¥è§¦å¤±æ•—ãƒšãƒŠãƒ«ãƒ†ã‚£
        
        # ç ´æãƒšãƒŠãƒ«ãƒ†ã‚£
        if tcp_data.get('broken', False):
            reward -= 20.0  # é‡ã„ãƒšãƒŠãƒ«ãƒ†ã‚£
        
        # æŠŠæŒåŠ›ã®é©åˆ‡æ€§ã‚’è©•ä¾¡
        actual_grip_force = tcp_data.get('grip_force', 10.0)
        
        # ç›®æ¨™ç¯„å›²ï¼ˆ8-15Nï¼‰ã¸ã®è¿‘ã•ã‚’å ±é…¬åŒ–
        target_min, target_max = 8.0, 15.0
        if target_min <= actual_grip_force <= target_max:
            reward += 3.0  # é©åˆ‡ç¯„å›²å ±é…¬
        else:
            # ç¯„å›²å¤–ã®è·é›¢ã«å¿œã˜ãŸãƒšãƒŠãƒ«ãƒ†ã‚£
            if actual_grip_force < target_min:
                distance = target_min - actual_grip_force
            else:
                distance = actual_grip_force - target_max
            reward -= distance * 0.5
        
        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚¹ãƒ ãƒ¼ã‚ºã•å ±é…¬ï¼ˆæ€¥æ¿€ãªå¤‰åŒ–ã‚’æŠ‘åˆ¶ï¼‰
        action_change = abs(action_value - self.previous_action)
        if action_change > 0.5:
            reward -= action_change * 0.5
        
        # çµ±è¨ˆæ›´æ–°
        self.total_reward += reward
        self.previous_action = action_value
        
        return reward

class DDPGRealtimeFeedbackSystem:
    """DDPG ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ çµ±åˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, 
                 model_path='models/improved_grip_force_classifier_*.pth',
                 lsl_stream_name='MockEEG',
                 tcp_host='127.0.0.1',
                 tcp_port=12345,
                 feedback_port=12346):
        
        self.model_path = model_path
        self.lsl_stream_name = lsl_stream_name
        self.tcp_host = tcp_host
        self.tcp_port = tcp_port
        self.feedback_port = feedback_port
        
        # ã‚µãƒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        self.init_data_collector()
        self.init_classifier()
        self.init_feedback_interface()
        
        # DDPGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
        self.agent = DDPGAgent(state_dim=7, action_dim=1, lr_actor=1e-4, lr_critic=1e-3)
        
        # ç’°å¢ƒ
        self.environment = GripForceEnvironment()
        
        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ç®¡ç†
        self.is_running = False
        self.learning_thread = None
        
        # å­¦ç¿’çµ±è¨ˆ
        self.stats = {
            'total_episodes': 0,
            'successful_episodes': 0,
            'total_rewards': [],
            'episode_rewards': [],
            'classification_accuracy': [],
            'grip_force_history': [],
            'start_time': None,
            'learning_updates': 0
        }
        
        # çŠ¶æ…‹ç®¡ç†
        self.previous_state = None
        self.previous_action = None
        self.current_episode_reward = 0.0
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.model_save_dir = f"models/ddpg_realtime_{self.session_id}"
        os.makedirs(self.model_save_dir, exist_ok=True)
        
        print(f"ğŸš€ DDPG ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        print(f"   ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {self.session_id}")
        print(f"   ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ: {self.model_save_dir}")
    
    def init_data_collector(self):
        """ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        self.data_collector = LSLTCPEpisodeCollector(
            lsl_stream_name=self.lsl_stream_name,
            tcp_host=self.tcp_host,
            tcp_port=self.tcp_port,
            save_to_csv=False  # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’ã§ã¯ç„¡åŠ¹
        )
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def init_classifier(self):
        """åˆ†é¡æ©ŸåˆæœŸåŒ–"""
        self.classifier = RealtimeGripForceClassifier(
            model_path=self.model_path,
            lsl_stream_name=self.lsl_stream_name,
            tcp_host=self.tcp_host,
            tcp_port=self.tcp_port
        )
        
        if not self.classifier.load_model():
            print(f"âš ï¸ åˆ†é¡æ©Ÿèª­ã¿è¾¼ã¿å¤±æ•— - ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰ã§ç¶šè¡Œ")
            self.classifier = None
        else:
            print(f"âœ… åˆ†é¡æ©ŸåˆæœŸåŒ–å®Œäº†")
    
    def init_feedback_interface(self):
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é€šä¿¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹åˆæœŸåŒ–"""
        self.feedback_interface = EEGTCPInterface(
            host=self.tcp_host,
            port=self.feedback_port
        )
        
        # æŠŠæŒåŠ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
        self.feedback_interface.add_message_callback(self.handle_grip_force_request)
        
        print(f"âœ… ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é€šä¿¡åˆæœŸåŒ–å®Œäº† (Port: {self.feedback_port})")
    
    def handle_grip_force_request(self, message_data):
        """Unityå´ã‹ã‚‰ã®æŠŠæŒåŠ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†"""
        try:
            if message_data.get('type') == 'grip_force_request':
                # ç¾åœ¨ã®çŠ¶æ…‹ã‹ã‚‰æœ€é©ãªæŠŠæŒåŠ›ã‚’è¨ˆç®—
                if self.previous_state is not None:
                    # DDPGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‹ã‚‰ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å–å¾—ï¼ˆãƒã‚¤ã‚ºãªã—ã€æ¨è«–ç”¨ï¼‰
                    action = self.agent.select_action(self.previous_state, add_noise=False, noise_scale=0.0)
                    
                    # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠŠæŒåŠ›ã«å¤‰æ› [-1,1] -> [5,25]N
                    grip_force = self.action_to_grip_force(action[0])
                    
                    print(f"ğŸ¯ DDPGæŠŠæŒåŠ›ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯: {grip_force:.2f}N (action: {action[0]:.3f})")
                    
                else:
                    # çŠ¶æ…‹ãŒãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                    grip_force = 12.0
                    print(f"âš ï¸ åˆæœŸçŠ¶æ…‹ - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæŠŠæŒåŠ›: {grip_force}N")
                
                # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é€ä¿¡
                response = {
                    'type': 'grip_force_command',
                    'target_force': float(grip_force),
                    'timestamp': time.time(),
                    'session_id': f"ddpg_rt_{self.session_id}",
                    'learning_episode': self.stats['total_episodes']
                }
                self.feedback_interface.send_message(response)
                
                # çµ±è¨ˆæ›´æ–°
                self.stats['grip_force_history'].append(grip_force)
                
        except Exception as e:
            print(f"âŒ æŠŠæŒåŠ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
    
    def action_to_grip_force(self, action_value):
        """DDPGã‚¢ã‚¯ã‚·ãƒ§ãƒ³å€¤ã‚’æŠŠæŒåŠ›ã«å¤‰æ›"""
        # [-1, 1] -> [5, 25]N ã®ç¯„å›²ã§ãƒãƒƒãƒ”ãƒ³ã‚°
        min_force, max_force = 5.0, 25.0
        grip_force = (action_value + 1.0) / 2.0 * (max_force - min_force) + min_force
        return np.clip(grip_force, min_force, max_force)
    
    def grip_force_to_action(self, grip_force):
        """æŠŠæŒåŠ›ã‚’DDPGã‚¢ã‚¯ã‚·ãƒ§ãƒ³å€¤ã«å¤‰æ›"""
        # [5, 25]N -> [-1, 1] ã®ç¯„å›²ã§ãƒãƒƒãƒ”ãƒ³ã‚°
        min_force, max_force = 5.0, 25.0
        action_value = 2.0 * (grip_force - min_force) / (max_force - min_force) - 1.0
        return np.clip(action_value, -1.0, 1.0)
    
    def classify_episode_data(self, episode):
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’EEGåˆ†é¡"""
        try:
            if self.classifier:
                return self.classifier.classify_episode(episode)
            else:
                # ãƒ‡ãƒ¢ç”¨ã®ãƒ©ãƒ³ãƒ€ãƒ åˆ†é¡
                classes = ['UnderGrip', 'Success', 'OverGrip']
                random_class = np.random.choice(classes, p=[0.3, 0.5, 0.2])
                class_idx = classes.index(random_class)
                return {
                    'predicted_class': random_class,
                    'predicted_class_idx': class_idx,
                    'confidence': np.random.uniform(0.6, 0.9)
                }
        except Exception as e:
            print(f"âš ï¸ EEGåˆ†é¡ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'predicted_class': 'Success',
                'predicted_class_idx': 1,
                'confidence': 0.5
            }
    
    def start_learning(self):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’é–‹å§‹"""
        print(f"ğŸ”´ DDPGãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’é–‹å§‹")
        
        self.is_running = True
        self.stats['start_time'] = time.time()
        self.environment.reset()
        
        # ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹
        if not self.data_collector.start_collection():
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹å¤±æ•—")
            return False
        
        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é€šä¿¡é–‹å§‹
        if not self.feedback_interface.start_server():
            print(f"âŒ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é€šä¿¡é–‹å§‹å¤±æ•—")
            return False
        
        # å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹
        self.learning_thread = threading.Thread(target=self._learning_loop, daemon=True)
        self.learning_thread.start()
        
        print(f"âœ… DDPGãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’é–‹å§‹å®Œäº†")
        print(f"ğŸ’¡ Unityå´ã§ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:")
        print(f"   1. ãƒ­ãƒœãƒƒãƒˆçŠ¶æ…‹ãƒ‡ãƒ¼ã‚¿é€ä¿¡ (TCP Port {self.tcp_port})")
        print(f"   2. EPISODE_END ãƒˆãƒªã‚¬ãƒ¼é€ä¿¡")
        print(f"   3. æŠŠæŒåŠ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ (TCP Port {self.feedback_port})")
        return True
    
    def _learning_loop(self):
        """å­¦ç¿’ãƒ«ãƒ¼ãƒ—ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å®Ÿè¡Œï¼‰"""
        print(f"ğŸ”„ å­¦ç¿’ãƒ«ãƒ¼ãƒ—é–‹å§‹")
        
        while self.is_running:
            try:
                # æ–°ã—ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
                if len(self.data_collector.episodes) > self.stats['total_episodes']:
                    # æœ€æ–°ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å–å¾—
                    latest_episode = self.data_collector.episodes[self.stats['total_episodes']]
                    
                    print(f"ğŸ†• æ–°ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å—ä¿¡: Episode {latest_episode.episode_id}")
                    
                    # EEGåˆ†é¡å®Ÿè¡Œ
                    classification_result = self.classify_episode_data(latest_episode)
                    
                    print(f"ğŸ§  EEGåˆ†é¡çµæœ: {classification_result['predicted_class']} "
                          f"(ä¿¡é ¼åº¦: {classification_result['confidence']:.3f})")
                    
                    # ç¾åœ¨ã®çŠ¶æ…‹ä½œæˆ
                    current_state = self.environment.create_state(
                        classification_result, 
                        latest_episode.tcp_data, 
                        self.previous_action[0] if self.previous_action is not None else 0.0
                    )
                    
                    # å‰ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®çµŒé¨“ã‚’å‡¦ç†
                    if (self.previous_state is not None and 
                        self.previous_action is not None):
                        
                        # å ±é…¬è¨ˆç®—
                        reward = self.environment.calculate_reward(
                            classification_result, 
                            latest_episode.tcp_data, 
                            self.previous_action[0]
                        )
                        
                        self.current_episode_reward += reward
                        
                        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†åˆ¤å®š
                        done = latest_episode.tcp_data.get('broken', False)
                        
                        # çµŒé¨“ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
                        self.agent.memory.push(
                            self.previous_state, 
                            self.previous_action, 
                            reward, 
                            current_state, 
                            done
                        )
                        
                        print(f"ğŸ“ˆ å ±é…¬: {reward:.2f}, ç´¯ç©å ±é…¬: {self.current_episode_reward:.2f}")
                        
                        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ›´æ–°ï¼ˆååˆ†ãªçµŒé¨“ãŒã‚ã‚‹å ´åˆï¼‰
                        if len(self.agent.memory) >= 64:
                            self.agent.update(batch_size=64)
                            self.stats['learning_updates'] += 1
                            
                            if self.stats['learning_updates'] % 10 == 0:
                                print(f"ğŸ“ å­¦ç¿’æ›´æ–°: {self.stats['learning_updates']}å›")
                        
                        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†å‡¦ç†
                        if done or self.stats['total_episodes'] % 10 == 0:
                            self.stats['episode_rewards'].append(self.current_episode_reward)
                            if classification_result['predicted_class'] == 'Success':
                                self.stats['successful_episodes'] += 1
                            
                            print(f"ğŸ“Š ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰{self.stats['total_episodes']}å®Œäº†: "
                                  f"ç´¯ç©å ±é…¬={self.current_episode_reward:.2f}")
                            
                            self.current_episode_reward = 0.0
                            self.agent.noise.reset()  # ãƒã‚¤ã‚ºãƒªã‚»ãƒƒãƒˆ
                    
                    # æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³é¸æŠï¼ˆæ¢ç´¢ç”¨ãƒã‚¤ã‚ºä»˜ãï¼‰
                    current_action = self.agent.select_action(current_state, add_noise=True, noise_scale=0.2)
                    
                    # çŠ¶æ…‹æ›´æ–°
                    self.previous_state = current_state
                    self.previous_action = current_action
                    self.stats['total_episodes'] += 1
                    
                    # å®šæœŸçš„ãªãƒ¢ãƒ‡ãƒ«ä¿å­˜
                    if self.stats['total_episodes'] % 50 == 0:
                        self.save_model()
                        self.plot_learning_curves()
                
                time.sleep(0.1)  # 100mså¾…æ©Ÿ
                
            except Exception as e:
                print(f"âŒ å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
                import traceback
                traceback.print_exc()
                time.sleep(1.0)
        
        print(f"ğŸ”„ å­¦ç¿’ãƒ«ãƒ¼ãƒ—çµ‚äº†")
    
    def save_model(self, filepath=None):
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        if filepath is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = os.path.join(self.model_save_dir, f"ddpg_realtime_{timestamp}.pth")
        
        try:
            torch.save({
                'actor_state_dict': self.agent.actor.state_dict(),
                'critic_state_dict': self.agent.critic.state_dict(),
                'actor_optimizer': self.agent.actor_optimizer.state_dict(),
                'critic_optimizer': self.agent.critic_optimizer.state_dict(),
                'stats': self.stats,
                'episodes': self.stats['total_episodes'],
                'session_id': self.session_id
            }, filepath)
            
            print(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {filepath}")
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def plot_learning_curves(self):
        """å­¦ç¿’ã‚«ãƒ¼ãƒ–ã‚’ãƒ—ãƒ­ãƒƒãƒˆ"""
        try:
            if len(self.stats['episode_rewards']) < 2:
                return
            
            fig, axes = plt.subplots(2, 2, figsize=(12, 8))
            fig.suptitle(f'DDPG ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’é€²æ— - Episode {self.stats["total_episodes"]}')
            
            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å ±é…¬
            axes[0, 0].plot(self.stats['episode_rewards'])
            axes[0, 0].set_title('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å ±é…¬')
            axes[0, 0].set_xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰')
            axes[0, 0].set_ylabel('ç´¯ç©å ±é…¬')
            axes[0, 0].grid(True)
            
            # ç§»å‹•å¹³å‡å ±é…¬
            if len(self.stats['episode_rewards']) > 10:
                window = 10
                moving_avg = np.convolve(self.stats['episode_rewards'], 
                                       np.ones(window)/window, mode='valid')
                axes[0, 1].plot(moving_avg)
                axes[0, 1].set_title(f'ç§»å‹•å¹³å‡å ±é…¬ (çª“å¹…={window})')
                axes[0, 1].set_xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰')
                axes[0, 1].set_ylabel('å¹³å‡å ±é…¬')
                axes[0, 1].grid(True)
            
            # å­¦ç¿’ãƒ­ã‚¹
            if self.agent.actor_losses:
                axes[1, 0].plot(self.agent.actor_losses, label='Actor Loss', alpha=0.7)
                axes[1, 0].plot(self.agent.critic_losses, label='Critic Loss', alpha=0.7)
                axes[1, 0].set_title('å­¦ç¿’ãƒ­ã‚¹')
                axes[1, 0].set_xlabel('æ›´æ–°å›æ•°')
                axes[1, 0].set_ylabel('ãƒ­ã‚¹')
                axes[1, 0].legend()
                axes[1, 0].grid(True)
            
            # æˆåŠŸç‡
            success_rate = self.stats['successful_episodes'] / max(self.stats['total_episodes'], 1) * 100
            axes[1, 1].bar(['æˆåŠŸç‡'], [success_rate])
            axes[1, 1].set_title(f'æˆåŠŸç‡: {success_rate:.1f}%')
            axes[1, 1].set_ylabel('æˆåŠŸç‡ (%)')
            axes[1, 1].grid(True)
            
            plt.tight_layout()
            
            # ä¿å­˜
            plot_path = os.path.join(self.model_save_dir, f"learning_progress_{self.stats['total_episodes']:04d}.png")
            plt.savefig(plot_path, dpi=150, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            print(f"âš ï¸ ãƒ—ãƒ­ãƒƒãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def stop_learning(self):
        """å­¦ç¿’åœæ­¢"""
        print(f"â¹ï¸ DDPGãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’åœæ­¢ä¸­...")
        
        self.is_running = False
        
        # å„ã‚µãƒ–ã‚·ã‚¹ãƒ†ãƒ åœæ­¢
        if self.data_collector:
            self.data_collector.stop_collection()
        
        if self.feedback_interface:
            self.feedback_interface.stop_server()
        
        # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        self.save_model()
        self.plot_learning_curves()
        
        # æœ€çµ‚çµ±è¨ˆè¡¨ç¤º
        self.print_final_statistics()
        
        print(f"âœ… DDPGãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’åœæ­¢å®Œäº†")
    
    def print_final_statistics(self):
        """æœ€çµ‚çµ±è¨ˆè¡¨ç¤º"""
        print(f"\nğŸ“Š DDPGãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’çµ±è¨ˆ:")
        print(f"   ç·ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {self.stats['total_episodes']}")
        print(f"   æˆåŠŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {self.stats['successful_episodes']}")
        
        if self.stats['total_episodes'] > 0:
            success_rate = self.stats['successful_episodes'] / self.stats['total_episodes'] * 100
            print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
        
        if self.stats['episode_rewards']:
            avg_reward = np.mean(self.stats['episode_rewards'][-50:])  # æœ€æ–°50ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®å¹³å‡
            print(f"   å¹³å‡å ±é…¬ï¼ˆæœ€æ–°50ï¼‰: {avg_reward:.2f}")
        
        if self.stats['grip_force_history']:
            avg_grip_force = np.mean(self.stats['grip_force_history'][-100:])
            print(f"   å¹³å‡æŠŠæŒåŠ›ï¼ˆæœ€æ–°100ï¼‰: {avg_grip_force:.2f}N")
        
        print(f"   å­¦ç¿’æ›´æ–°å›æ•°: {self.stats['learning_updates']}")
        print(f"   çµŒé¨“ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º: {len(self.agent.memory)}")
        
        if self.stats['start_time']:
            uptime = time.time() - self.stats['start_time']
            print(f"   ç¨¼åƒæ™‚é–“: {uptime:.1f}ç§’")
        
        print(f"   ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ: {self.model_save_dir}")
    
    def run_demo(self):
        """ãƒ‡ãƒ¢å®Ÿè¡Œ"""
        print(f"ğŸš€ DDPGãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ  ãƒ‡ãƒ¢å®Ÿè¡Œ")
        
        if self.start_learning():
            try:
                print(f"\nğŸ’¡ ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒä¸­...")
                print(f"   ğŸ“¡ LSLãƒ‡ãƒ¼ã‚¿å—ä¿¡: {self.lsl_stream_name}")
                print(f"   ğŸ“¡ TCPå—ä¿¡ãƒãƒ¼ãƒˆ: {self.tcp_port}")
                print(f"   ğŸ“¡ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é€ä¿¡ãƒãƒ¼ãƒˆ: {self.feedback_port}")
                print(f"   ğŸ“ DDPGãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’å®Ÿè¡Œä¸­")
                print(f"   Ctrl+C ã§çµ‚äº†")
                
                while self.is_running:
                    time.sleep(5.0)
                    
                    # å®šæœŸçš„ãªé€²æ—è¡¨ç¤º
                    print(f"ğŸ“ˆ é€²æ—: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰{self.stats['total_episodes']}, "
                          f"å­¦ç¿’æ›´æ–°{self.stats['learning_updates']}å›, "
                          f"æˆåŠŸ{self.stats['successful_episodes']}ä»¶")
                    
            except KeyboardInterrupt:
                print(f"\nâ¹ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼åœæ­¢")
            finally:
                self.stop_learning()
        else:
            print(f"âŒ ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹å¤±æ•—")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print(f"ğŸ§  DDPG ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ ")
    print(f"=" * 70)
    print(f"æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«çµ±åˆ:")
    print(f"  ğŸ“¡ e_tcp_lsl_sync_system.py - LSL/TCPãƒ‡ãƒ¼ã‚¿å—ä¿¡")
    print(f"  ğŸ§  g_grip_force_realtime_classifier.py - EEGåˆ†é¡")
    print(f"  ğŸ¤– DDPGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ - æŠŠæŒåŠ›æœ€é©åŒ–å­¦ç¿’")
    print(f"  ğŸ“¤ Unity ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é€ä¿¡")
    print(f"=" * 70)
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ï¼ˆè¨­å®šå¯èƒ½ï¼‰
    system = DDPGRealtimeFeedbackSystem(
        model_path='models/improved_grip_force_classifier_*.pth',
        lsl_stream_name='MockEEG',  # å¿…è¦ã«å¿œã˜ã¦å¤‰æ›´
        tcp_host='127.0.0.1',
        tcp_port=12345,          # ãƒ‡ãƒ¼ã‚¿å—ä¿¡ãƒãƒ¼ãƒˆ
        feedback_port=12346      # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é€ä¿¡ãƒãƒ¼ãƒˆ
    )
    
    # ãƒ‡ãƒ¢å®Ÿè¡Œ
    system.run_demo()

if __name__ == "__main__":
    main()