#!/usr/bin/env python3
"""
EEGË™çÁü•Á´∂ÂêàÂàÜÈ°ûÂô®Â≠¶Áøí„Ç∑„Çπ„ÉÜ„É†
TCP„Åã„Çâ„ÅÆÊòéÁ§∫ÁöÑ„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ„ÇíÊïôÂ∏´„Éá„Éº„Çø„Å®„Åó„Å¶„ÄÅ
LSL„ÅÆEEG„Éá„Éº„Çø„Åã„ÇâË™çÁü•Á´∂Âêà„ÇíÂàÜÈ°û„Åô„ÇãÂà§Âà•Âô®„Çí‰ΩúÊàê„ÉªÂ≠¶Áøí

Â≠¶Áøí„Éï„É≠„Éº:
1. Unity TCP ‚Üí ÊòéÁ§∫ÁöÑ„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ (success/under_grip/over_grip)
2. LSL EEG ‚Üí Êé•Ëß¶ÊôÇÂâçÂæå1.2Áßí„ÅÆ„Ç®„Éù„ÉÉ„ÇØ„Éá„Éº„Çø 
3. („Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ, EEG„Ç®„Éù„ÉÉ„ÇØ) „Éö„Ç¢„ÇíËìÑÁ©ç
4. DeepConvNet CNN„ÅßÂàÜÈ°ûÂô®„ÇíÂ≠¶Áøí
5. Â≠¶ÁøíÊ∏à„ÅøÂàÜÈ°ûÂô®„ÅßÊñ∞„Åó„ÅÑEEG„Éá„Éº„Çø„ÇíËá™ÂãïÂàÜÈ°û
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from scipy import signal
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import json
import csv
import os
import time
from collections import deque, Counter
import pickle
from datetime import datetime

# Êó¢Â≠ò„Ç∑„Çπ„ÉÜ„É†„Åã„ÇâÊã°Âºµ
from systems.episode_contact_sync_system import EpisodeContactSynchronizer
from lsl_data_send.eeg_neuroadaptation_preprocessor import NeuroadaptationEEGPreprocessor

class EEGClassifierDataset(Dataset):
    """
    EEGÂàÜÈ°ûÂô®Áî®„Éá„Éº„Çø„Çª„ÉÉ„Éà
    (EEG„Ç®„Éù„ÉÉ„ÇØ, ÊòéÁ§∫ÁöÑ„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ) „ÅÆ„Éö„Ç¢„ÇíÁÆ°ÁêÜ
    """
    
    def __init__(self, eeg_epochs, feedback_labels, transform=None):
        """
        Args:
            eeg_epochs: List of (300, 32) EEG„Ç®„Éù„ÉÉ„ÇØ
            feedback_labels: List of feedback labels (0: success, 1: under_grip, 2: over_grip)
            transform: „Éá„Éº„ÇøÊã°Âºµ„ÉªÂâçÂá¶ÁêÜÈñ¢Êï∞
        """
        self.eeg_epochs = eeg_epochs
        self.feedback_labels = feedback_labels
        self.transform = transform
        
        # „É©„Éô„É´Áµ±Ë®à
        self.label_counts = Counter(feedback_labels)
        print(f"üìä „Éá„Éº„Çø„Çª„ÉÉ„ÉàÁµ±Ë®à: {dict(self.label_counts)}")
    
    def __len__(self):
        return len(self.eeg_epochs)
    
    def __getitem__(self, idx):
        epoch = self.eeg_epochs[idx]  # (300, 32)
        label = self.feedback_labels[idx]
        
        if self.transform:
            epoch = self.transform(epoch)
            
        # CNN„ÅÆÂÖ•ÂäõÂΩ¢Áä∂„Å´Â§âÊèõ: (1, channels, samples)
        epoch_tensor = torch.FloatTensor(epoch.T).unsqueeze(0)  # (1, 32, 300)
        label_tensor = torch.LongTensor([label])
        
        return epoch_tensor, label_tensor.squeeze()

class DeepConvNetClassifier(nn.Module):
    """
    DeepConvNetÂàÜÈ°ûÂô®ÔºàË´ñÊñáÊ∫ñÊã†Ôºâ
    EEG„Ç®„Éù„ÉÉ„ÇØ ‚Üí Ë™çÁü•Á´∂Âêà3„ÇØ„É©„ÇπÂàÜÈ°û
    """
    
    def __init__(self, n_channels=32, n_samples=300, n_classes=3, dropout=0.5):
        super(DeepConvNetClassifier, self).__init__()
        
        self.n_channels = n_channels
        self.n_samples = n_samples
        
        # Block 1: Temporal Convolution
        self.conv1 = nn.Conv2d(1, 25, (1, 10), padding=(0, 4))
        
        # Block 2: Spatial Convolution
        self.conv2 = nn.Conv2d(25, 25, (n_channels, 1), padding=0)
        self.batch_norm1 = nn.BatchNorm2d(25)
        self.pool1 = nn.MaxPool2d((1, 3))
        
        # Block 3: Separable Convolution
        self.conv3 = nn.Conv2d(25, 50, (1, 10), padding=(0, 4))
        self.batch_norm2 = nn.BatchNorm2d(50)
        self.pool2 = nn.MaxPool2d((1, 3))
        
        # Block 4: Separable Convolution
        self.conv4 = nn.Conv2d(50, 100, (1, 10), padding=(0, 4))
        self.batch_norm3 = nn.BatchNorm2d(100)
        self.pool3 = nn.MaxPool2d((1, 3))
        
        # Block 5: Separable Convolution
        self.conv5 = nn.Conv2d(100, 200, (1, 10), padding=(0, 4))
        self.batch_norm4 = nn.BatchNorm2d(200)
        self.pool4 = nn.MaxPool2d((1, 3))
        
        # ÈÅ©ÂøúÁöÑ„Éó„Éº„É™„É≥„Ç∞ÔºàÂèØÂ§âÈï∑ÂÖ•ÂäõÂØæÂøúÔºâ
        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 8))
        
        # Classification Head
        self.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(200 * 8, 256),
            nn.ELU(),
            nn.Dropout(dropout * 0.6),
            nn.Linear(256, 128),
            nn.ELU(),
            nn.Dropout(dropout * 0.3),
            nn.Linear(128, n_classes)
        )
        
        print(f"üß† DeepConvNetÂàùÊúüÂåñ: {n_channels}ch, {n_samples}samples ‚Üí {n_classes}classes")
        
    def forward(self, x):
        # x: (batch_size, 1, channels, samples)
        
        # Block 1
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.batch_norm1(x)
        x = F.elu(x)
        x = self.pool1(x)
        
        # Block 2
        x = self.conv3(x)
        x = self.batch_norm2(x)
        x = F.elu(x)
        x = self.pool2(x)
        
        # Block 3
        x = self.conv4(x)
        x = self.batch_norm3(x)
        x = F.elu(x)
        x = self.pool3(x)
        
        # Block 4
        x = self.conv5(x)
        x = self.batch_norm4(x)
        x = F.elu(x)
        x = self.pool4(x)
        
        # Global Average Pooling
        x = self.adaptive_pool(x)
        x = x.view(x.size(0), -1)
        
        # Classification
        x = self.classifier(x)
        return x

class EEGClassifierTrainer:
    """
    EEGÂàÜÈ°ûÂô®„ÅÆÂ≠¶Áøí„ÉªË©ï‰æ°„Éª‰øùÂ≠ò„ÇíÁÆ°ÁêÜ
    """
    
    def __init__(self, model, device='auto'):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device == 'auto' else device
        self.model = model.to(self.device)
        
        # Â≠¶ÁøíË®≠ÂÆö
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3, weight_decay=1e-4)
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=10, factor=0.5)
        
        # Â≠¶ÁøíÂ±•Ê≠¥
        self.train_history = {'loss': [], 'accuracy': []}
        self.val_history = {'loss': [], 'accuracy': []}
        self.best_val_accuracy = 0.0
        
        print(f"üéì Â≠¶ÁøíÁí∞Â¢É: {self.device}")
    
    def train_epoch(self, train_loader):
        """1„Ç®„Éù„ÉÉ„ÇØ„ÅÆÂ≠¶Áøí"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(self.device), target.to(self.device)
            
            self.optimizer.zero_grad()
            output = self.model(data)
            loss = self.criterion(output, target)
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)
            
            if batch_idx % 10 == 0:
                print(f'   Batch {batch_idx:3d}: Loss={loss.item():.4f}, Acc={100.*correct/total:.1f}%')
        
        avg_loss = total_loss / len(train_loader)
        accuracy = 100. * correct / total
        return avg_loss, accuracy
    
    def validate(self, val_loader):
        """Ê§úË®º"""
        self.model.eval()
        val_loss = 0
        correct = 0
        total = 0
        all_preds = []
        all_targets = []
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                val_loss += self.criterion(output, target).item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)
                
                all_preds.extend(pred.cpu().numpy().flatten())
                all_targets.extend(target.cpu().numpy())
        
        avg_loss = val_loss / len(val_loader)
        accuracy = 100. * correct / total
        return avg_loss, accuracy, all_preds, all_targets
    
    def train_full(self, train_loader, val_loader, epochs=100, early_stopping=15):
        """ÂÆåÂÖ®Â≠¶Áøí„É´„Éº„Éó"""
        print(f"üéì Â≠¶ÁøíÈñãÂßã: {epochs}„Ç®„Éù„ÉÉ„ÇØ, Early Stopping={early_stopping}")
        
        best_epoch = 0
        epochs_without_improvement = 0
        
        for epoch in range(epochs):
            print(f"\n--- Epoch {epoch+1}/{epochs} ---")
            
            # Â≠¶Áøí
            train_loss, train_acc = self.train_epoch(train_loader)
            
            # Ê§úË®º
            val_loss, val_acc, val_preds, val_targets = self.validate(val_loader)
            
            # Â≠¶ÁøíÁéáË™øÊï¥
            self.scheduler.step(val_loss)
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # Â±•Ê≠¥‰øùÂ≠ò
            self.train_history['loss'].append(train_loss)
            self.train_history['accuracy'].append(train_acc)
            self.val_history['loss'].append(val_loss)
            self.val_history['accuracy'].append(val_acc)
            
            print(f"Â≠¶Áøí   : Loss={train_loss:.4f}, Acc={train_acc:.1f}%")
            print(f"Ê§úË®º   : Loss={val_loss:.4f}, Acc={val_acc:.1f}%, LR={current_lr:.6f}")
            
            # „Éô„Çπ„Éà„É¢„Éá„É´Êõ¥Êñ∞
            if val_acc > self.best_val_accuracy:
                self.best_val_accuracy = val_acc
                best_epoch = epoch + 1
                epochs_without_improvement = 0
                torch.save(self.model.state_dict(), 'models/best_eeg_classifier.pth')
                print(f"üéØ „Éô„Çπ„Éà„É¢„Éá„É´Êõ¥Êñ∞! Val Acc: {val_acc:.1f}%")
            else:
                epochs_without_improvement += 1
            
            # Early Stopping
            if epochs_without_improvement >= early_stopping:
                print(f"‚è∞ Early Stopping at epoch {epoch+1}")
                print(f"   „Éô„Çπ„Éà: Epoch {best_epoch}, Val Acc: {self.best_val_accuracy:.1f}%")
                break
        
        return self.best_val_accuracy
    
    def evaluate_final(self, test_loader, class_names=['Success', 'Under-grip', 'Over-grip']):
        """ÊúÄÁµÇË©ï‰æ°ÔºàÊ∑∑ÂêåË°åÂàó„ÄÅÂàÜÈ°û„É¨„Éù„Éº„ÉàÔºâ"""
        print("\nüîç ÊúÄÁµÇË©ï‰æ°ÂÆüË°å...")
        
        # „Éô„Çπ„Éà„É¢„Éá„É´Ë™≠„ÅøËæº„Åø
        self.model.load_state_dict(torch.load('models/best_eeg_classifier.pth'))
        
        # „ÉÜ„Çπ„Éà
        test_loss, test_acc, test_preds, test_targets = self.validate(test_loader)
        
        print(f"üìä „ÉÜ„Çπ„ÉàÁµêÊûú: Loss={test_loss:.4f}, Accuracy={test_acc:.1f}%")
        
        # üîç „ÇØ„É©„ÇπÂàÜÂ∏É„ÉÅ„Çß„ÉÉ„ÇØ
        unique_targets = np.unique(test_targets)
        unique_preds = np.unique(test_preds)
        
        print(f"üîç „Éá„Éº„ÇøÂàÜÂ∏ÉÁ¢∫Ë™ç:")
        print(f"   Áúü„ÅÆ„ÇØ„É©„Çπ: {unique_targets}")
        print(f"   ‰∫àÊ∏¨„ÇØ„É©„Çπ: {unique_preds}")
        print(f"   „ÇØ„É©„ÇπÊï∞: Áúü={len(unique_targets)}, ‰∫àÊ∏¨={len(unique_preds)}")
        
        # Âçò‰∏Ä„ÇØ„É©„ÇπÂïèÈ°å„ÅÆÊ§úÂá∫
        if len(unique_targets) <= 1:
            print(f"‚ö†Ô∏è Ë≠¶Âëä: Âçò‰∏Ä„ÇØ„É©„Çπ„ÅÆ„Åø„ÅÆ„Éá„Éº„Çø„Åß„Åô")
            print(f"   Â≠¶Áøí„Å´„ÅØÊúÄ‰Ωé2„ÇØ„É©„Çπ‰ª•‰∏ä„ÅÆ„Éá„Éº„Çø„ÅåÂøÖË¶Å„Åß„Åô")
            print(f"   ÁèæÂú®„ÅÆ„ÇØ„É©„Çπ: {unique_targets}")
            
            # Âçò‰∏Ä„ÇØ„É©„ÇπÁî®„ÅÆÁ∞°Âçò„Å™Ë©ï‰æ°
            if len(unique_targets) == 1:
                single_class = unique_targets[0]
                class_name = class_names[single_class] if single_class < len(class_names) else f'Class_{single_class}'
                print(f"\nüìà Âçò‰∏Ä„ÇØ„É©„ÇπÂàÜÊûê:")
                print(f"   „ÇØ„É©„Çπ: {class_name}")
                print(f"   „Çµ„É≥„Éó„É´Êï∞: {len(test_targets)}")
                print(f"   Ê≠£Ëß£Áéá: {test_acc:.1f}%")
                
                return test_acc
        
        # ÈÄöÂ∏∏„ÅÆË©≥Á¥∞ÂàÜÊûêÔºàË§áÊï∞„ÇØ„É©„ÇπÊôÇÔºâ
        try:
            # Âà©Áî®ÂèØËÉΩ„Å™„ÇØ„É©„ÇπÂêç„ÅÆ„Åø‰ΩøÁî®
            available_class_names = [class_names[i] for i in unique_targets if i < len(class_names)]
            
            print("\nüìà ÂàÜÈ°û„É¨„Éù„Éº„Éà:")
            report = classification_report(test_targets, test_preds, 
                                         target_names=available_class_names,
                                         labels=unique_targets)
            print(report)
            
            # Ê∑∑ÂêåË°åÂàó
            cm = confusion_matrix(test_targets, test_preds, labels=unique_targets)
            
            plt.figure(figsize=(8, 6))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                       xticklabels=available_class_names, 
                       yticklabels=available_class_names)
            plt.title(f'Ê∑∑ÂêåË°åÂàó (Test Accuracy: {test_acc:.1f}%)')
            plt.ylabel('Áúü„ÅÆÂàÜÈ°û')
            plt.xlabel('‰∫àÊ∏¨ÂàÜÈ°û')
            plt.tight_layout()
            
            os.makedirs('plots', exist_ok=True)
            plt.savefig(f'plots/confusion_matrix_{datetime.now().strftime("%Y%m%d_%H%M%S")}.png')
            print(f"üíæ Ê∑∑ÂêåË°åÂàó‰øùÂ≠ò: plots/confusion_matrix_*.png")
            # plt.show()  # „Ç≥„É°„É≥„Éà„Ç¢„Ç¶„ÉàÔºàËá™ÂãïÂÆüË°åÊôÇ„ÅÆ„Ç®„É©„ÉºÂõûÈÅøÔºâ
            
        except Exception as e:
            print(f"‚ö†Ô∏è Ë©≥Á¥∞ÂàÜÊûê„Ç®„É©„Éº: {e}")
            print(f"   Âü∫Êú¨Ë©ï‰æ°„ÅÆ„ÅøÂÆüË°å")
        
        # Â≠¶ÁøíÊõ≤Á∑ö
        try:
            self._plot_training_curves()
        except:
            print(f"‚ö†Ô∏è Â≠¶ÁøíÊõ≤Á∑öÊèèÁîª„Çπ„Ç≠„ÉÉ„Éó")
        
        return test_acc
    
    def _plot_training_curves(self):
        """Â≠¶ÁøíÊõ≤Á∑ö„ÅÆÊèèÁîª"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
        
        # Loss
        ax1.plot(self.train_history['loss'], label='Train')
        ax1.plot(self.val_history['loss'], label='Validation')
        ax1.set_title('Loss Curves')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True)
        
        # Accuracy
        ax2.plot(self.train_history['accuracy'], label='Train')
        ax2.plot(self.val_history['accuracy'], label='Validation')
        ax2.set_title('Accuracy Curves')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy (%)')
        ax2.legend()
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig(f'plots/training_curves_{datetime.now().strftime("%Y%m%d_%H%M%S")}.png')
        plt.show()

class EEGClassifierDataCollector(EpisodeContactSynchronizer):
    """
    EEGÂàÜÈ°ûÂô®Áî®„Éá„Éº„ÇøÂèéÈõÜ„Ç∑„Çπ„ÉÜ„É†
    TCP„Åã„Çâ„ÅÆÊòéÁ§∫ÁöÑ„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ + LSL„ÅÆEEG„Ç®„Éù„ÉÉ„ÇØ „Éö„Ç¢„ÇíÂèéÈõÜ
    """
    
    def __init__(self, *args, **kwargs):
        # „Éá„Éº„ÇøÂèéÈõÜË®≠ÂÆö
        self.collect_training_data = kwargs.pop('collect_training_data', True)
        self.min_samples_per_class = kwargs.pop('min_samples_per_class', 100)

        super().__init__(*args, **kwargs)

        # EEGÂâçÂá¶ÁêÜÂô®„ÇíÂàùÊúüÂåñ
        self.eeg_preprocessor = NeuroadaptationEEGPreprocessor(
            sampling_rate=self.sampling_rate,
            n_channels=self.n_channels,
            epoch_duration=self.epoch_duration
        )

        # Â≠¶Áøí„Éá„Éº„ÇøËìÑÁ©ç
        self.training_data = {
            'eeg_epochs': [],           # List of (300, 32) arrays
            'feedback_labels': [],      # List of integers (0, 1, 2)
            'timestamps': [],           # List of timestamps
            'episode_info': []          # List of episode metadata
        }
        
        # „Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØÂàÜÈ°û„Éû„ÉÉ„Éî„É≥„Ç∞
        self.feedback_mapping = {
            'success': 0,
            'normal': 0,
            'good': 0,
            'correct': 0,
            'under_grip': 1,
            'weak': 1,
            'insufficient': 1,
            'light': 1,
            'over_grip': 2,
            'strong': 2,
            'excessive': 2,
            'crush': 2,
            'deform': 2
        }
        
        # ÂèéÈõÜÁµ±Ë®à
        self.collection_stats = Counter()
        
        print(f"üìö EEGÂàÜÈ°ûÂô®„Éá„Éº„ÇøÂèéÈõÜ„Ç∑„Çπ„ÉÜ„É†ÂàùÊúüÂåñ:")
        print(f"   ÁõÆÊ®ô: ÂêÑ„ÇØ„É©„Çπ{self.min_samples_per_class}„Çµ„É≥„Éó„É´‰ª•‰∏ä")
        print(f"   „Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ„Éû„ÉÉ„Éî„É≥„Ç∞: {self.feedback_mapping}")

    def _parse_explicit_feedback(self, tcp_data):
        """
        TCP„Éá„Éº„Çø„Åã„ÇâÊòéÁ§∫ÁöÑ„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØËß£ÊûêÔºàgrip_forceÂà§ÂÆöÁâàÔºâ

        „Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ„É´„Éº„É´:
        - 8 <= grip_force <= 15 ‚Üí success (ÊàêÂäü)
        - grip_force > 15      ‚Üí over_grip (Âº∑„Åô„Åé)
        - grip_force < 8       ‚Üí under_grip (Âº±„Åô„Åé)

        Args:
            tcp_data: Unity TCP„Éá„Éº„Çø

        Returns:
            feedback_info: Ëß£Êûê„Åï„Çå„Åü„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØÊÉÖÂ†±
        """
        feedback_info = None

        try:
            grip_force = tcp_data.get('grip_force', None)

            print(f"üîç „Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØÁä∂ÊÖãËß£Êûê:")
            print(f"   grip_force: {grip_force}")

            if grip_force is not None:
                if isinstance(grip_force, str):
                    try:
                        grip_force = float(grip_force)
                    except ValueError:
                        grip_force = None
                elif isinstance(grip_force, (int, float)):
                    grip_force = float(grip_force)

            if grip_force is not None:
                # üéØ „Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØÂà§ÂÆö„É´„Éº„É´
                if 8 <= grip_force <= 15:
                    feedback_class = 'success'
                    feedback_label = 0
                    confidence = 0.9
                    reasoning = "ÈÅ©Âàá„Å™ÊääÊåÅÂäõÔºà8-15NÔºâ"
                elif grip_force > 15:
                    feedback_class = 'over_grip'
                    feedback_label = 2
                    confidence = 0.95
                    reasoning = "ÊääÊåÅÂäõÈÅéÂâ∞Ôºà>15NÔºâ"
                else:
                    feedback_class = 'under_grip'
                    feedback_label = 1
                    confidence = 0.9
                    reasoning = "ÊääÊåÅÂäõ‰∏çË∂≥Ôºà<8NÔºâ"

                feedback_info = {
                    'class': feedback_class,
                    'label': feedback_label,
                    'confidence': confidence,
                    'explicit': True,
                    'raw_feedback': f"grip_force={grip_force}",
                    'source_field': 'grip_force',
                    'generation_rule': reasoning
                }

                print(f"‚úÖ „Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØÂà§ÂÆöÊàêÂäü:")
                print(f"   ÂàÜÈ°û: {feedback_class} (label={feedback_label})")
                print(f"   ‰ø°È†ºÂ∫¶: {confidence:.2f}")
                print(f"   Ê†πÊã†: {reasoning}")
            else:
                print(f"‚ùå ÂøÖË¶Å„Å™„Éï„Ç£„Éº„É´„Éâ„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì:")
                print(f"   grip_force „Éï„Ç£„Éº„É´„Éâ: {grip_force}")
                print(f"   Âà©Áî®ÂèØËÉΩ„Éï„Ç£„Éº„É´„Éâ: {list(tcp_data.keys())}")

        except Exception as e:
            print(f"‚ùå „Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØËß£Êûê„Ç®„É©„Éº: {e}")
            import traceback

            print(f"   Ë©≥Á¥∞: {traceback.format_exc()}")

        return feedback_info

    def _create_synchronized_event(self, tcp_event: dict, lsl_event: dict = None):
        """
        „Éá„Éº„ÇøÂèéÈõÜÁâà„ÅÆÂêåÊúü„Ç§„Éô„É≥„Éà‰ΩúÊàê
        ÊòéÁ§∫ÁöÑ„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ + EEG„Ç®„Éù„ÉÉ„ÇØ „Éö„Ç¢„ÇíÂèéÈõÜ
        """
        try:
            tcp_timestamp = tcp_event['system_time']
            episode_info = tcp_event['episode_info']
            episode_num = episode_info['episode']
            
            # 1. EEG„Ç®„Éù„ÉÉ„ÇØÊäΩÂá∫
            epoch_data, epoch_timestamps, sync_latency = self._extract_epoch_around_time(
                tcp_timestamp, episode_num
            )
            
            if epoch_data is None:
                print(f"‚ö†Ô∏è Episode {episode_num}: EEG„Ç®„Éù„ÉÉ„ÇØ„Éá„Éº„Çø‰∏çË∂≥")
                return None
            
            # 2. „Ç®„Éù„ÉÉ„ÇØÂâçÂá¶ÁêÜ
            preprocess_result = self.eeg_preprocessor.preprocess_epoch(epoch_data)
            epoch_data = preprocess_result['processed_epoch']


            # Âü∫Êú¨ÂìÅË≥™Ë©ï‰æ°„Å®È´òÂ∫¶„Å™ÂìÅË≥™ÊåáÊ®ô„ÇíÁµ±Âêà
            basic_quality = self._assess_epoch_quality(epoch_data)
            advanced_quality = preprocess_result.get('quality_metrics', {})
            epoch_quality = {**basic_quality, **advanced_quality}


            # 3. ÊòéÁ§∫ÁöÑ„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØËß£Êûê
            feedback_info = self._parse_explicit_feedback(tcp_event['data'])

            if feedback_info is None:
                print(f"‚ö†Ô∏è Episode {episode_num}: ÊòéÁ§∫ÁöÑ„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØÊú™Ê§úÂá∫")
                # „É©„É≥„ÉÄ„É†„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ„ÅØÁîüÊàê„Åô„Çã„ÅåÂ≠¶Áøí„Éá„Éº„Çø„Å´„ÅØ‰Ωø„Çè„Å™„ÅÑ
                feedback_value = self._generate_random_feedback(episode_info)
            else:
                # ÊòéÁ§∫ÁöÑ„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ„ÇíÂ≠¶Áøí„Éá„Éº„Çø„Å®„Åó„Å¶ËìÑÁ©ç
                if self.collect_training_data:
                    self.training_data['eeg_epochs'].append(epoch_data.copy())
                    self.training_data['feedback_labels'].append(feedback_info['label'])
                    self.training_data['timestamps'].append(tcp_timestamp)
                    self.training_data['episode_info'].append(episode_info.copy())

                    # Áµ±Ë®àÊõ¥Êñ∞
                    self.collection_stats[feedback_info['class']] += 1

                    print(f"üíæ Â≠¶Áøí„Éá„Éº„ÇøËìÑÁ©ç: Episode {episode_num}")
                    print(f"   „Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ: {feedback_info['class']} (‰ø°È†ºÂ∫¶: {feedback_info['confidence']:.2f})")
                    print(f"   ÂèéÈõÜÁµ±Ë®à: {dict(self.collection_stats)}")

                # „Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØÂÄ§„ÅØÂÆüÈöõ„ÅÆÂàÜÈ°ûÁµêÊûú„Éô„Éº„Çπ„ÅßÁîüÊàê
                reward_mapping = {0: 25.0, 1: 15.0, 2: 5.0}  # success, under_grip, over_grip
                feedback_value = self._generate_random_feedback(episode_info)

            # 4. ÂêåÊúü„Ç§„Éô„É≥„Éà‰ΩúÊàê
            collection_sync_event = {
                'episode_number': episode_info['episode'],
                'contact_timestamp': tcp_timestamp,
                'epoch_data': epoch_data,
                'epoch_timestamps': epoch_timestamps,
                'episode_info': episode_info,
                'tcp_data': tcp_event['data'],
                
                # „Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØÊÉÖÂ†±
                'feedback_info': feedback_info,
                'feedback_value': feedback_value,
                'has_explicit_feedback': feedback_info is not None,

                # ÂìÅË≥™Ë©ï‰æ°
                'sync_latency': sync_latency,
                'epoch_quality': epoch_quality
            }
            
            return collection_sync_event
            
        except Exception as e:
            print(f"‚ùå „Éá„Éº„ÇøÂèéÈõÜÂêåÊúü„Ç§„Éô„É≥„Éà‰ΩúÊàê„Ç®„É©„Éº: {e}")
            return None

    def save_training_data(self, filename=None):
        """Â≠¶Áøí„Éá„Éº„Çø„Çí„Éï„Ç°„Ç§„É´„Å´‰øùÂ≠ò"""
        if filename is None:
            filename = f'training_data/eeg_training_data_{self.session_id}.pkl'
        
        try:
            os.makedirs('training_data', exist_ok=True)
            
            # „Éá„Éº„ÇøÊï¥ÂêàÊÄß„ÉÅ„Çß„ÉÉ„ÇØ
            n_epochs = len(self.training_data['eeg_epochs'])
            n_labels = len(self.training_data['feedback_labels'])
            
            if n_epochs != n_labels:
                print(f"‚ö†Ô∏è „Éá„Éº„Çø‰∏çÊï¥Âêà: „Ç®„Éù„ÉÉ„ÇØÊï∞={n_epochs}, „É©„Éô„É´Êï∞={n_labels}")
                return False
            
            # ‰øùÂ≠ò
            with open(filename, 'wb') as f:
                pickle.dump(self.training_data, f)
            
            print(f"üíæ Â≠¶Áøí„Éá„Éº„Çø‰øùÂ≠òÂÆå‰∫Ü: {filename}")
            print(f"   Á∑è„Çµ„É≥„Éó„É´Êï∞: {n_epochs}")
            print(f"   „ÇØ„É©„ÇπÂà•ÂàÜÂ∏É: {dict(self.collection_stats)}")
            
            # CSV„Çµ„Éû„É™„Éº„ÇÇ‰øùÂ≠ò
            summary_file = filename.replace('.pkl', '_summary.csv')
            self._save_data_summary(summary_file)
            
            return True
            
        except Exception as e:
            print(f"‚ùå Â≠¶Áøí„Éá„Éº„Çø‰øùÂ≠ò„Ç®„É©„Éº: {e}")
            return False

    def _save_data_summary(self, filename):
        """„Éá„Éº„ÇøÂèéÈõÜ„Çµ„Éû„É™„Éº„ÇíCSV‰øùÂ≠ò"""
        try:
            with open(filename, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(['episode_number', 'timestamp', 'feedback_class', 'feedback_label', 'confidence', 'sync_latency_ms', 'epoch_quality'])
                
                for i, (timestamp, episode_info, feedback_label) in enumerate(zip(
                    self.training_data['timestamps'],
                    self.training_data['episode_info'], 
                    self.training_data['feedback_labels']
                )):
                    # „É©„Éô„É´„Åã„Çâ„ÇØ„É©„ÇπÂêç„ÇíÈÄÜÂºï„Åç
                    class_name = {v: k for k, v in self.feedback_mapping.items() if v == feedback_label}
                    class_name = list(class_name.keys())[0] if class_name else 'unknown'
                    
                    writer.writerow([
                        episode_info['episode'],
                        timestamp,
                        class_name,
                        feedback_label,
                        1.0,  # confidence („Éá„Éï„Ç©„É´„Éà)
                        0.0,  # sync_latency_ms (Ë®àÁÆóÁúÅÁï•)
                        'good'  # epoch_quality (Á∞°Áï•Âåñ)
                    ])
                    
        except Exception as e:
            print(f"‚ùå „Çµ„Éû„É™„ÉºCSV‰øùÂ≠ò„Ç®„É©„Éº: {e}")

    def check_data_sufficiency(self):
        """„Éá„Éº„ÇøÂèéÈõÜÂçÅÂàÜÊÄß„ÉÅ„Çß„ÉÉ„ÇØÔºàÊîπËâØÁâàÔºâ"""
        sufficient = True
        print(f"\nüìä „Éá„Éº„ÇøÂèéÈõÜÁä∂Ê≥Å„ÉÅ„Çß„ÉÉ„ÇØ:")
        
        # ÂÆüÈöõ„Å´ÂèéÈõÜ„Åï„Çå„Åü„É©„Éô„É´„ÅÆÂàÜÂ∏É
        actual_labels = Counter(self.training_data['feedback_labels'])
        total_samples = sum(actual_labels.values())
        
        print(f"   Á∑è„Çµ„É≥„Éó„É´Êï∞: {total_samples}")
        
        # ÂêÑ„ÇØ„É©„Çπ„ÅÆÁä∂Ê≥Å
        class_names = {0: 'success', 1: 'under_grip', 2: 'over_grip'}
        for label, class_name in class_names.items():
            count = actual_labels.get(label, 0)
            percentage = (count / total_samples * 100) if total_samples > 0 else 0
            needed = max(0, self.min_samples_per_class - count)
            status = "OK" if count >= self.min_samples_per_class else "NG"
            
            print(f"   {class_name:12s}: {count:3d}/{self.min_samples_per_class} ({percentage:4.1f}%) {status}")
            if needed > 0:
                print(f"                    ‚Üí „ÅÇ„Å®{needed}„Çµ„É≥„Éó„É´ÂøÖË¶Å")
                sufficient = False
        
        # Âçò‰∏Ä„ÇØ„É©„ÇπÂïèÈ°å„ÅÆË≠¶Âëä
        n_classes_present = len([c for c in actual_labels.values() if c > 0])
        if n_classes_present <= 1:
            print(f"ÈáçË¶Å: ÁèæÂú®{n_classes_present}„ÇØ„É©„Çπ„ÅÆ„ÅøÊ§úÂá∫")
            print(f"   ÂàÜÈ°ûÂô®Â≠¶Áøí„Å´„ÅØÊúÄ‰Ωé2„ÇØ„É©„Çπ‰ª•‰∏äÂøÖË¶Å„Åß„Åô")
            print(f"   ÁèæÂú®„ÅÆËá™Âãï„É©„Éô„É´ÁîüÊàê„É´„Éº„É´„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ")
            sufficient = False
        elif n_classes_present == 2:
            print(f"Ê≥®ÊÑè: ÁèæÂú®{n_classes_present}„ÇØ„É©„Çπ„ÅÆ„ÅøÊ§úÂá∫")
            print(f"   „Çà„ÇäËâØ„ÅÑÊÄßËÉΩ„Å´„ÅØ3„ÇØ„É©„ÇπÂÖ®„Å¶„ÅåÊé®Â•®„Åï„Çå„Åæ„Åô")
        
        if sufficient:
            print(f"„Éá„Éº„ÇøÂèéÈõÜÂÆå‰∫ÜÔºÅÂàÜÈ°ûÂô®Â≠¶Áøí„ÇíÈñãÂßã„Åß„Åç„Åæ„Åô„ÄÇ")
        else:
            if n_classes_present <= 1:
                print(f"„Éá„Éº„Çø‰∏çË∂≥: „Çà„ÇäÂ§öÊßò„Å™„Ç∑„Éä„É™„Ç™„Åß„Éá„Éº„ÇøÂèéÈõÜ„ÅåÂøÖË¶Å")
            else:
                print(f"„Éá„Éº„ÇøÂèéÈõÜÁ∂ôÁ∂ö‰∏≠...")
        
        return sufficient and n_classes_present >= 2
        

    def run_data_collection_session(self, duration_seconds=18000, auto_train=True):
        """
        „Éá„Éº„ÇøÂèéÈõÜ„Çª„ÉÉ„Ç∑„Éß„É≥ÂÆüË°å
        
        Args:
            duration_seconds: ÊúÄÂ§ßÂèéÈõÜÊôÇÈñìÔºà„Éá„Éï„Ç©„É´„Éà30ÂàÜÔºâ
            auto_train: „Éá„Éº„ÇøÂçÅÂàÜÊôÇ„Å´Ëá™ÂãïÂ≠¶ÁøíÈñãÂßã
        """
        if not self.start_synchronization_system():
            return None
            
        print(f"üìö EEGÂàÜÈ°ûÂô®„Éá„Éº„ÇøÂèéÈõÜ„Çª„ÉÉ„Ç∑„Éß„É≥ÈñãÂßã")
        print(f"‚è±Ô∏è ÊúÄÂ§ßÂèéÈõÜÊôÇÈñì: {duration_seconds}Áßí ({duration_seconds//60}ÂàÜ)")
        print(f"üéØ ÂêÑ„ÇØ„É©„ÇπÁõÆÊ®ô: {self.min_samples_per_class}„Çµ„É≥„Éó„É´")
        
        start_time = time.time()
        last_check_time = start_time
        
        try:
            while self.is_running:
                elapsed = time.time() - start_time
                
                # 30Áßí„Åî„Å®„Å´ÈÄ≤ÊçóÁ¢∫Ë™ç
                if elapsed - (last_check_time - start_time) >= 30:
                    sufficient = self.check_data_sufficiency()
                    
                    if sufficient and auto_train:
                        print(f"üéì „Éá„Éº„ÇøÂçÅÂàÜÔºÅËá™ÂãïÂ≠¶ÁøíÈñãÂßã...")
                        break
                    
                    last_check_time = time.time()
                
                # ÁµÇ‰∫ÜÊù°‰ª∂„ÉÅ„Çß„ÉÉ„ÇØ
                if elapsed >= duration_seconds:
                    print(f"‚è∞ Âà∂ÈôêÊôÇÈñìÂà∞ÈÅîÔºà{duration_seconds}ÁßíÔºâ")
                    break
                
                time.sleep(1.0)
                
        except KeyboardInterrupt:
            print(f"\n‚ö° „É¶„Éº„Ç∂„Éº‰∏≠Êñ≠")
        finally:
            print(f"üîö „Éá„Éº„ÇøÂèéÈõÜÁµÇ‰∫Ü...")
            self.stop_synchronization_system()
            
            # „Éá„Éº„Çø‰øùÂ≠ò
            saved = self.save_training_data()
            
            if saved and auto_train:
                # Ëá™ÂãïÂ≠¶ÁøíÂÆüË°å
                return self.train_classifier_from_collected_data()
            
            return saved

    def train_classifier_from_collected_data(self):
        """ÂèéÈõÜ„Åó„Åü„Éá„Éº„Çø„Åã„ÇâÂàÜÈ°ûÂô®„ÇíÂ≠¶Áøí"""
        print(f"\nüéì EEGÂàÜÈ°ûÂô®Â≠¶ÁøíÈñãÂßã...")
        
        if len(self.training_data['eeg_epochs']) == 0:
            print(f"‚ùå Â≠¶Áøí„Éá„Éº„Çø„Åå„ÅÇ„Çä„Åæ„Åõ„Çì")
            return None
        
        # „Éá„Éº„Çø„Çª„ÉÉ„Éà‰ΩúÊàê
        dataset = EEGClassifierDataset(
            self.training_data['eeg_epochs'],
            self.training_data['feedback_labels']
        )
        
        # „Éá„Éº„ÇøÂàÜÂâ≤
        total_size = len(dataset)
        train_size = int(0.7 * total_size)
        val_size = int(0.15 * total_size) 
        test_size = total_size - train_size - val_size
        
        train_dataset, val_dataset, test_dataset = random_split(
            dataset, [train_size, val_size, test_size]
        )
        
        print(f"üìä „Éá„Éº„ÇøÂàÜÂâ≤: Train={train_size}, Val={val_size}, Test={test_size}")
        
        # DataLoader‰ΩúÊàê
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)
        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
        
        # „É¢„Éá„É´‰ΩúÊàê
        model = DeepConvNetClassifier(n_channels=32, n_samples=300, n_classes=3)
        trainer = EEGClassifierTrainer(model)
        
        # Â≠¶ÁøíÂÆüË°å
        print(f"üöÄ Â≠¶ÁøíÈñãÂßã...")
        best_accuracy = trainer.train_full(
            train_loader, val_loader, 
            epochs=100, early_stopping=15
        )
        
        # ÊúÄÁµÇË©ï‰æ°
        print(f"\nüîç ÊúÄÁµÇ„ÉÜ„Çπ„ÉàË©ï‰æ°...")
        test_accuracy = trainer.evaluate_final(test_loader)
        
        print(f"\nüéâ Â≠¶ÁøíÂÆå‰∫Ü!")
        print(f"   „Éô„Çπ„ÉàÊ§úË®ºÁ≤æÂ∫¶: {best_accuracy:.1f}%")
        print(f"   ÊúÄÁµÇ„ÉÜ„Çπ„ÉàÁ≤æÂ∫¶: {test_accuracy:.1f}%")
        
        # Â≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´ÊÉÖÂ†±„ÇíËøî„Åô
        return {
            'model_path': 'models/best_eeg_classifier.pth',
            'val_accuracy': best_accuracy,
            'test_accuracy': test_accuracy,
            'total_samples': total_size,
            'class_distribution': dict(self.collection_stats)
        }

class EEGDataAugmentation:
    """
    EEG„Éá„Éº„ÇøÊã°Âºµ„ÇØ„É©„Çπ
    Â∞ë„Å™„ÅÑ„Éá„Éº„Çø„Åã„ÇâÂ≠¶ÁøíÂäπÊûú„ÇíÊúÄÂ§ßÂåñ
    """
    
    @staticmethod
    def add_noise(epoch, noise_level=0.05):
        """„Ç¨„Ç¶„Ç∑„Ç¢„É≥„Éé„Ç§„Ç∫ËøΩÂä†"""
        noise = np.random.normal(0, noise_level, epoch.shape)
        return epoch + noise
    
    @staticmethod
    def time_shift(epoch, max_shift_samples=10):
        """ÊôÇÈñìËª∏„Ç∑„Éï„Éà"""
        shift = np.random.randint(-max_shift_samples, max_shift_samples + 1)
        if shift > 0:
            shifted = np.zeros_like(epoch)
            shifted[shift:, :] = epoch[:-shift, :]
        elif shift < 0:
            shifted = np.zeros_like(epoch)
            shifted[:shift, :] = epoch[-shift:, :]
        else:
            shifted = epoch
        return shifted
    
    @staticmethod
    def channel_dropout(epoch, dropout_prob=0.1):
        """„ÉÅ„É£„É≥„Éç„É´„Éâ„É≠„ÉÉ„Éó„Ç¢„Ç¶„Éà"""
        mask = np.random.random(epoch.shape[1]) > dropout_prob
        augmented = epoch.copy()
        augmented[:, ~mask] = 0
        return augmented
    
    @staticmethod
    def amplitude_scaling(epoch, scale_range=(0.8, 1.2)):
        """ÊåØÂπÖ„Çπ„Ç±„Éº„É™„É≥„Ç∞"""
        scale = np.random.uniform(*scale_range)
        return epoch * scale

def preprocess_eeg_epoch_for_training(epoch_data, sampling_rate=250):
    """
    Â≠¶ÁøíÁî®EEG„Ç®„Éù„ÉÉ„ÇØÂâçÂá¶ÁêÜÔºàË´ñÊñáÊ∫ñÊã†Ôºâ
    
    Args:
        epoch_data: (300, 32) EEG„Ç®„Éù„ÉÉ„ÇØ
        sampling_rate: „Çµ„É≥„Éó„É™„É≥„Ç∞Âë®Ê≥¢Êï∞
        
    Returns:
        processed_epoch: ÂâçÂá¶ÁêÜÊ∏à„Åø„Ç®„Éù„ÉÉ„ÇØ
    """
    # „Éê„É≥„Éâ„Éë„Çπ„Éï„Ç£„É´„Çø (2-50Hz)
    sos = signal.butter(5, [2, 50], btype='band', fs=sampling_rate, output='sos')
    filtered_epoch = np.zeros_like(epoch_data)
    
    for ch in range(epoch_data.shape[1]):
        try:
            filtered_epoch[:, ch] = signal.sosfilt(sos, epoch_data[:, ch])
        except:
            filtered_epoch[:, ch] = epoch_data[:, ch]  # „Éï„Ç£„É´„Çø„É™„É≥„Ç∞Â§±ÊïóÊôÇ„ÅØÂÖÉ„Éá„Éº„Çø
    
    # Artifact Subspace Reconstruction (ASR) „ÅÆÁ∞°ÊòìÁâà
    # Ê•µÁ´Ø„Å™Â§ñ„ÇåÂÄ§„ÉÅ„É£„É≥„Éç„É´„ÅÆÈô§Âéª
    for ch in range(filtered_epoch.shape[1]):
        ch_data = filtered_epoch[:, ch]
        if np.std(ch_data) > 0:
            z_scores = np.abs((ch_data - np.mean(ch_data)) / np.std(ch_data))
            if np.max(z_scores) > 5:  # 5œÉ„ÇíË∂Ö„Åà„ÇãÂ§ñ„ÇåÂÄ§
                print(f"‚ö†Ô∏è Channel {ch}: Â§ñ„ÇåÂÄ§Ê§úÂá∫„ÄÅ„Çº„É≠Âåñ")
                filtered_epoch[:, ch] = 0
    
    # Ê≠£Ë¶èÂåñ („ÉÅ„É£„É≥„Éç„É´„Åî„Å®Z-score)
    for ch in range(filtered_epoch.shape[1]):
        ch_data = filtered_epoch[:, ch]
        if np.std(ch_data) > 1e-10:  # „Çº„É≠Èô§ÁÆóÂõûÈÅø
            filtered_epoch[:, ch] = (ch_data - np.mean(ch_data)) / np.std(ch_data)
    
    return filtered_epoch

# üéì Â≠¶ÁøíÂÆüË°å„Çπ„ÇØ„É™„Éó„Éà
def run_complete_training_pipeline():
    """
    ÂÆåÂÖ®„Å™Â≠¶Áøí„Éë„Ç§„Éó„É©„Ç§„É≥„ÅÆÂÆüË°å
    1. „Éá„Éº„ÇøÂèéÈõÜ ‚Üí 2. Â≠¶Áøí ‚Üí 3. Ë©ï‰æ° ‚Üí 4. Âº∑ÂåñÂ≠¶ÁøíÁµ±Âêà
    """
    print("üéì EEGË™çÁü•Á´∂ÂêàÂàÜÈ°ûÂô® ÂÆåÂÖ®Â≠¶Áøí„Éë„Ç§„Éó„É©„Ç§„É≥")
    print("=" * 70)
    
    # Step 1: „Éá„Éº„ÇøÂèéÈõÜ
    print("\nüìö Step 1: Â≠¶Áøí„Éá„Éº„ÇøÂèéÈõÜ")
    data_collector = EEGClassifierDataCollector(
        tcp_host='127.0.0.1',
        tcp_port=12345,
        lsl_stream_name='MockEEG',
        contact_buffer_duration=1.5,
        min_samples_per_class=50,  # ÂêÑ„ÇØ„É©„Çπ50„Çµ„É≥„Éó„É´
        collect_training_data=True
    )
    
    # „Éá„Éº„ÇøÂèéÈõÜ„Çª„ÉÉ„Ç∑„Éß„É≥
    training_result = data_collector.run_data_collection_session(
        duration_seconds=1800,  # 30ÂàÜÈñì
        auto_train=True
    )
    
    if training_result is None:
        print("‚ùå „Éá„Éº„ÇøÂèéÈõÜÂ§±Êïó")
        return
    
    # Step 2: Â≠¶ÁøíÁµêÊûúÁ¢∫Ë™ç
    print(f"\n‚úÖ Â≠¶ÁøíÂÆå‰∫Ü!")
    print(f"   „É¢„Éá„É´„Éë„Çπ: {training_result['model_path']}")
    print(f"   „ÉÜ„Çπ„ÉàÁ≤æÂ∫¶: {training_result['test_accuracy']:.1f}%")
    print(f"   Â≠¶Áøí„Çµ„É≥„Éó„É´Êï∞: {training_result['total_samples']}")
    
    # Step 3: Âº∑ÂåñÂ≠¶ÁøíÁµ±Âêà„Ç∑„Çπ„ÉÜ„É†„ÅßÂÆüÁî®Âåñ
    if training_result['test_accuracy'] > 70:  # 70%‰ª•‰∏ä„ÅßÂÆüÁî®ÂèØËÉΩ
        print(f"\nü§ñ Step 3: Âº∑ÂåñÂ≠¶Áøí„Ç∑„Çπ„ÉÜ„É†Áµ±Âêà")
        
        eeg_rl_system = EEGReinforcementLearningSystem(
            tcp_host='127.0.0.1',
            tcp_port=12345,
            lsl_stream_name='MockEEG',
            contact_buffer_duration=1.5,
            eeg_model_path=training_result['model_path'],
            enable_eeg_classification=True
        )
        
        print(f"üöÄ EEGÂº∑ÂåñÂ≠¶Áøí„Çª„ÉÉ„Ç∑„Éß„É≥ÈñãÂßã...")
        eeg_rl_system.run_eeg_reinforcement_learning_session(
            duration_seconds=1200,  # 20ÂàÜÈñì
            target_episodes=100
        )
    else:
        print(f"‚ö†Ô∏è ÂàÜÈ°ûÁ≤æÂ∫¶„Åå‰Ωé„Åô„Åé„Åæ„Åô ({training_result['test_accuracy']:.1f}%)")
        print(f"   „Çà„ÇäÂ§ö„Åè„ÅÆ„Éá„Éº„ÇøÂèéÈõÜ„ÅåÂøÖË¶Å„Åß„Åô")

if __name__ == "__main__":
    print("üß† EEGË™çÁü•Á´∂ÂêàÂàÜÈ°ûÂô®Â≠¶Áøí„Ç∑„Çπ„ÉÜ„É†")
    print("TCP„Åã„Çâ„ÅÆÊòéÁ§∫ÁöÑ„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ„ÅßLSL EEG„Éá„Éº„Çø„ÇíÂ≠¶Áøí")
    
    # ÈÅ∏Êäû„É°„Éã„É•„Éº
    print("\nÈÅ∏Êäû„Åó„Å¶„Åè„Å†„Åï„ÅÑ:")
    print("1. „Éá„Éº„ÇøÂèéÈõÜ„ÅÆ„ÅøÂÆüË°å")
    print("2. Êó¢Â≠ò„Éá„Éº„Çø„Åã„ÇâÂ≠¶Áøí„ÅÆ„ÅøÂÆüË°å") 
    
    choice = input("ÈÅ∏Êäû (1/2): ").strip()
    
    if choice == "1":
        # „Éá„Éº„ÇøÂèéÈõÜ„ÅÆ„Åø
        collector = EEGClassifierDataCollector(
            tcp_host='127.0.0.1',
            tcp_port=12345,
            lsl_stream_name='MockEEG',
            contact_buffer_duration=1.5,
            min_samples_per_class=100
        )
        collector.run_data_collection_session(duration_seconds=1800, auto_train=False)
        
    elif choice == "2":
        # Êó¢Â≠ò„Éá„Éº„Çø„Åã„ÇâÂ≠¶Áøí
        data_file = input("„Éá„Éº„Çø„Éï„Ç°„Ç§„É´„Éë„Çπ (.pkl): ").strip()
        
        try:
            with open(data_file, 'rb') as f:
                training_data = pickle.load(f)
            
            dataset = EEGClassifierDataset(
                training_data['eeg_epochs'],
                training_data['feedback_labels']
            )
            
            # „Éá„Éº„ÇøÂàÜÂâ≤„ÉªÂ≠¶ÁøíÂÆüË°å
            total_size = len(dataset)
            train_size = int(0.7 * total_size)
            val_size = int(0.15 * total_size)
            test_size = total_size - train_size - val_size
            
            train_dataset, val_dataset, test_dataset = random_split(
                dataset, [train_size, val_size, test_size]
            )
            
            train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
            test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
            
            model = DeepConvNetClassifier()
            trainer = EEGClassifierTrainer(model)
            
            trainer.train_full(train_loader, val_loader, epochs=100)
            trainer.evaluate_final(test_loader)
            
        except Exception as e:
            print(f"‚ùå Â≠¶Áøí„Ç®„É©„Éº: {e}")
    
    elif choice == "3":
        # ÂÆåÂÖ®„Éë„Ç§„Éó„É©„Ç§„É≥
        run_complete_training_pipeline()
    
    else:
        print("‚ùå ÁÑ°Âäπ„Å™ÈÅ∏Êäû„Åß„Åô")

"""
üß† ‰ΩøÁî®ÊñπÊ≥ï:

1. **„Éá„Éº„ÇøÂèéÈõÜ„Éï„Çß„Éº„Ç∫**:
   - UnityÂÅ¥„ÅßÊòéÁ§∫ÁöÑ„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØÈÄÅ‰ø°
   - TCP: {"feedback": "success"} / {"feedback": "over_grip"} Á≠â
   - Ëá™Âãï„ÅßEEG„Ç®„Éù„ÉÉ„ÇØ„Å®„Éö„Ç¢„Å´„Åó„Å¶ËìÑÁ©ç

2. **Â≠¶Áøí„Éï„Çß„Éº„Ç∫**: 
   - ÂèéÈõÜ„Éá„Éº„Çø„ÅßDeepConvNet CNNÂ≠¶Áøí
   - 70%‰ª•‰∏ä„ÅÆÁ≤æÂ∫¶ÈÅîÊàê„ÇíÁõÆÊ®ô

3. **ÂÆüÁî®„Éï„Çß„Éº„Ç∫**:
   - Â≠¶ÁøíÊ∏à„ÅøÂàÜÈ°ûÂô®„ÅßÊñ∞„Åó„ÅÑEEG„ÇíËá™ÂãïÂàÜÈ°û
   - CC-DDPGÂº∑ÂåñÂ≠¶Áøí„Åß„É≠„Éú„ÉÉ„ÉàÂà∂Âæ°ÊúÄÈÅ©Âåñ

üìä **ÊúüÂæÖ„Åï„Çå„ÇãÁ≤æÂ∫¶**: Ë´ñÊñá„Åß„ÅØ80-90%„ÅÆÂàÜÈ°ûÁ≤æÂ∫¶„ÇíÈÅîÊàê
üéØ **ÊúÄÂ∞è„Éá„Éº„ÇøË¶Å‰ª∂**: ÂêÑ„ÇØ„É©„Çπ50-100„Çµ„É≥„Éó„É´ÔºàÂêàË®à150-300„Ç®„Éî„ÇΩ„Éº„ÉâÔºâ
‚ö° **„É™„Ç¢„É´„Çø„Ç§„É†ÊÄßËÉΩ**: Êé•Ëß¶Ê§úÂá∫„Åã„Çâ50ms‰ª•ÂÜÖ„ÅßÂàÜÈ°û„ÉªÂà∂Âæ°
"""