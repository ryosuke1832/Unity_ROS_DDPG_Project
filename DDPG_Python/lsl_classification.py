#!/usr/bin/env python3
"""
EEGèªçŸ¥ç«¶åˆåˆ†é¡å™¨å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 
TCPã‹ã‚‰ã®æ˜ç¤ºçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã€
LSLã®EEGãƒ‡ãƒ¼ã‚¿ã‹ã‚‰èªçŸ¥ç«¶åˆã‚’åˆ†é¡ã™ã‚‹åˆ¤åˆ¥å™¨ã‚’ä½œæˆãƒ»å­¦ç¿’

å­¦ç¿’ãƒ•ãƒ­ãƒ¼:
1. Unity TCP â†’ æ˜ç¤ºçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ (success/under_grip/over_grip)
2. LSL EEG â†’ æ¥è§¦æ™‚å‰å¾Œ1.2ç§’ã®ã‚¨ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ 
3. (ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯, EEGã‚¨ãƒãƒƒã‚¯) ãƒšã‚¢ã‚’è“„ç©
4. DeepConvNet CNNã§åˆ†é¡å™¨ã‚’å­¦ç¿’
5. å­¦ç¿’æ¸ˆã¿åˆ†é¡å™¨ã§æ–°ã—ã„EEGãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•åˆ†é¡
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from scipy import signal
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import json
import csv
import os
import time
from collections import deque, Counter
import pickle
from datetime import datetime

# æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰æ‹¡å¼µ
from systems.episode_contact_sync_system import EpisodeContactSynchronizer
from lsl_data_send.eeg_neuroadaptation_preprocessor import NeuroadaptationEEGPreprocessor

class EEGClassifierDataset(Dataset):
    """
    EEGåˆ†é¡å™¨ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    (EEGã‚¨ãƒãƒƒã‚¯, æ˜ç¤ºçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯) ã®ãƒšã‚¢ã‚’ç®¡ç†
    """
    
    def __init__(self, eeg_epochs, feedback_labels, transform=None):
        """
        Args:
            eeg_epochs: List of (300, 32) EEGã‚¨ãƒãƒƒã‚¯
            feedback_labels: List of feedback labels (0: success, 1: under_grip, 2: over_grip)
            transform: ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µãƒ»å‰å‡¦ç†é–¢æ•°
        """
        self.eeg_epochs = eeg_epochs
        self.feedback_labels = feedback_labels
        self.transform = transform
        
        # ãƒ©ãƒ™ãƒ«çµ±è¨ˆ
        self.label_counts = Counter(feedback_labels)
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆ: {dict(self.label_counts)}")
    
    def __len__(self):
        return len(self.eeg_epochs)
    
    def __getitem__(self, idx):
        epoch = self.eeg_epochs[idx]  # (300, 32)
        label = self.feedback_labels[idx]
        
        if self.transform:
            epoch = self.transform(epoch)
            
        # CNNã®å…¥åŠ›å½¢çŠ¶ã«å¤‰æ›: (1, channels, samples)
        epoch_tensor = torch.FloatTensor(epoch.T).unsqueeze(0)  # (1, 32, 300)
        label_tensor = torch.LongTensor([label])
        
        return epoch_tensor, label_tensor.squeeze()

class DeepConvNetClassifier(nn.Module):
    """
    DeepConvNetåˆ†é¡å™¨ï¼ˆè«–æ–‡æº–æ‹ ï¼‰
    EEGã‚¨ãƒãƒƒã‚¯ â†’ èªçŸ¥ç«¶åˆ3ã‚¯ãƒ©ã‚¹åˆ†é¡
    """
    
    def __init__(self, n_channels=32, n_samples=300, n_classes=3, dropout=0.5):
        super(DeepConvNetClassifier, self).__init__()
        
        self.n_channels = n_channels
        self.n_samples = n_samples
        
        # Block 1: Temporal Convolution
        self.conv1 = nn.Conv2d(1, 25, (1, 10), padding=(0, 4))
        
        # Block 2: Spatial Convolution
        self.conv2 = nn.Conv2d(25, 25, (n_channels, 1), padding=0)
        self.batch_norm1 = nn.BatchNorm2d(25)
        self.pool1 = nn.MaxPool2d((1, 3))
        
        # Block 3: Separable Convolution
        self.conv3 = nn.Conv2d(25, 50, (1, 10), padding=(0, 4))
        self.batch_norm2 = nn.BatchNorm2d(50)
        self.pool2 = nn.MaxPool2d((1, 3))
        
        # Block 4: Separable Convolution
        self.conv4 = nn.Conv2d(50, 100, (1, 10), padding=(0, 4))
        self.batch_norm3 = nn.BatchNorm2d(100)
        self.pool3 = nn.MaxPool2d((1, 3))
        
        # Block 5: Separable Convolution
        self.conv5 = nn.Conv2d(100, 200, (1, 10), padding=(0, 4))
        self.batch_norm4 = nn.BatchNorm2d(200)
        self.pool4 = nn.MaxPool2d((1, 3))
        
        # é©å¿œçš„ãƒ—ãƒ¼ãƒªãƒ³ã‚°ï¼ˆå¯å¤‰é•·å…¥åŠ›å¯¾å¿œï¼‰
        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 8))
        
        # Classification Head
        self.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(200 * 8, 256),
            nn.ELU(),
            nn.Dropout(dropout * 0.6),
            nn.Linear(256, 128),
            nn.ELU(),
            nn.Dropout(dropout * 0.3),
            nn.Linear(128, n_classes)
        )
        
        print(f"ğŸ§  DeepConvNetåˆæœŸåŒ–: {n_channels}ch, {n_samples}samples â†’ {n_classes}classes")
        
    def forward(self, x):
        # x: (batch_size, 1, channels, samples)
        
        # Block 1
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.batch_norm1(x)
        x = F.elu(x)
        x = self.pool1(x)
        
        # Block 2
        x = self.conv3(x)
        x = self.batch_norm2(x)
        x = F.elu(x)
        x = self.pool2(x)
        
        # Block 3
        x = self.conv4(x)
        x = self.batch_norm3(x)
        x = F.elu(x)
        x = self.pool3(x)
        
        # Block 4
        x = self.conv5(x)
        x = self.batch_norm4(x)
        x = F.elu(x)
        x = self.pool4(x)
        
        # Global Average Pooling
        x = self.adaptive_pool(x)
        x = x.view(x.size(0), -1)
        
        # Classification
        x = self.classifier(x)
        return x

class EEGClassifierTrainer:
    """
    EEGåˆ†é¡å™¨ã®å­¦ç¿’ãƒ»è©•ä¾¡ãƒ»ä¿å­˜ã‚’ç®¡ç†
    """
    
    def __init__(self, model, device='auto'):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device == 'auto' else device
        self.model = model.to(self.device)
        
        # å­¦ç¿’è¨­å®š
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3, weight_decay=1e-4)
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=10, factor=0.5)
        
        # å­¦ç¿’å±¥æ­´
        self.train_history = {'loss': [], 'accuracy': []}
        self.val_history = {'loss': [], 'accuracy': []}
        self.best_val_accuracy = 0.0
        
        print(f"ğŸ“ å­¦ç¿’ç’°å¢ƒ: {self.device}")
    
    def train_epoch(self, train_loader):
        """1ã‚¨ãƒãƒƒã‚¯ã®å­¦ç¿’"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(self.device), target.to(self.device)
            
            self.optimizer.zero_grad()
            output = self.model(data)
            loss = self.criterion(output, target)
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)
            
            if batch_idx % 10 == 0:
                print(f'   Batch {batch_idx:3d}: Loss={loss.item():.4f}, Acc={100.*correct/total:.1f}%')
        
        avg_loss = total_loss / len(train_loader)
        accuracy = 100. * correct / total
        return avg_loss, accuracy
    
    def validate(self, val_loader):
        """æ¤œè¨¼"""
        self.model.eval()
        val_loss = 0
        correct = 0
        total = 0
        all_preds = []
        all_targets = []
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                val_loss += self.criterion(output, target).item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)
                
                all_preds.extend(pred.cpu().numpy().flatten())
                all_targets.extend(target.cpu().numpy())
        
        avg_loss = val_loss / len(val_loader)
        accuracy = 100. * correct / total
        return avg_loss, accuracy, all_preds, all_targets
    
    def train_full(self, train_loader, val_loader, epochs=100, early_stopping=15):
        """å®Œå…¨å­¦ç¿’ãƒ«ãƒ¼ãƒ—"""
        print(f"ğŸ“ å­¦ç¿’é–‹å§‹: {epochs}ã‚¨ãƒãƒƒã‚¯, Early Stopping={early_stopping}")
        
        best_epoch = 0
        epochs_without_improvement = 0
        
        for epoch in range(epochs):
            print(f"\n--- Epoch {epoch+1}/{epochs} ---")
            
            # å­¦ç¿’
            train_loss, train_acc = self.train_epoch(train_loader)
            
            # æ¤œè¨¼
            val_loss, val_acc, val_preds, val_targets = self.validate(val_loader)
            
            # å­¦ç¿’ç‡èª¿æ•´
            self.scheduler.step(val_loss)
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # å±¥æ­´ä¿å­˜
            self.train_history['loss'].append(train_loss)
            self.train_history['accuracy'].append(train_acc)
            self.val_history['loss'].append(val_loss)
            self.val_history['accuracy'].append(val_acc)
            
            print(f"å­¦ç¿’   : Loss={train_loss:.4f}, Acc={train_acc:.1f}%")
            print(f"æ¤œè¨¼   : Loss={val_loss:.4f}, Acc={val_acc:.1f}%, LR={current_lr:.6f}")
            
            # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«æ›´æ–°
            if val_acc > self.best_val_accuracy:
                self.best_val_accuracy = val_acc
                best_epoch = epoch + 1
                epochs_without_improvement = 0
                torch.save(self.model.state_dict(), 'models/best_eeg_classifier.pth')
                print(f"ğŸ¯ ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«æ›´æ–°! Val Acc: {val_acc:.1f}%")
            else:
                epochs_without_improvement += 1
            
            # Early Stopping
            if epochs_without_improvement >= early_stopping:
                print(f"â° Early Stopping at epoch {epoch+1}")
                print(f"   ãƒ™ã‚¹ãƒˆ: Epoch {best_epoch}, Val Acc: {self.best_val_accuracy:.1f}%")
                break
        
        return self.best_val_accuracy
    
    def evaluate_final(self, test_loader, class_names=['Success', 'Under-grip', 'Over-grip']):
        """æœ€çµ‚è©•ä¾¡ï¼ˆæ··åŒè¡Œåˆ—ã€åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆï¼‰"""
        print("\nğŸ” æœ€çµ‚è©•ä¾¡å®Ÿè¡Œ...")
        
        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        self.model.load_state_dict(torch.load('models/best_eeg_classifier.pth'))
        
        # ãƒ†ã‚¹ãƒˆ
        test_loss, test_acc, test_preds, test_targets = self.validate(test_loader)
        
        print(f"ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœ: Loss={test_loss:.4f}, Accuracy={test_acc:.1f}%")
        
        # ğŸ” ã‚¯ãƒ©ã‚¹åˆ†å¸ƒãƒã‚§ãƒƒã‚¯
        unique_targets = np.unique(test_targets)
        unique_preds = np.unique(test_preds)
        
        print(f"ğŸ” ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒç¢ºèª:")
        print(f"   çœŸã®ã‚¯ãƒ©ã‚¹: {unique_targets}")
        print(f"   äºˆæ¸¬ã‚¯ãƒ©ã‚¹: {unique_preds}")
        print(f"   ã‚¯ãƒ©ã‚¹æ•°: çœŸ={len(unique_targets)}, äºˆæ¸¬={len(unique_preds)}")
        
        # å˜ä¸€ã‚¯ãƒ©ã‚¹å•é¡Œã®æ¤œå‡º
        if len(unique_targets) <= 1:
            print(f"âš ï¸ è­¦å‘Š: å˜ä¸€ã‚¯ãƒ©ã‚¹ã®ã¿ã®ãƒ‡ãƒ¼ã‚¿ã§ã™")
            print(f"   å­¦ç¿’ã«ã¯æœ€ä½2ã‚¯ãƒ©ã‚¹ä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™")
            print(f"   ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹: {unique_targets}")
            
            # å˜ä¸€ã‚¯ãƒ©ã‚¹ç”¨ã®ç°¡å˜ãªè©•ä¾¡
            if len(unique_targets) == 1:
                single_class = unique_targets[0]
                class_name = class_names[single_class] if single_class < len(class_names) else f'Class_{single_class}'
                print(f"\nğŸ“ˆ å˜ä¸€ã‚¯ãƒ©ã‚¹åˆ†æ:")
                print(f"   ã‚¯ãƒ©ã‚¹: {class_name}")
                print(f"   ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(test_targets)}")
                print(f"   æ­£è§£ç‡: {test_acc:.1f}%")
                
                return test_acc
        
        # é€šå¸¸ã®è©³ç´°åˆ†æï¼ˆè¤‡æ•°ã‚¯ãƒ©ã‚¹æ™‚ï¼‰
        try:
            # åˆ©ç”¨å¯èƒ½ãªã‚¯ãƒ©ã‚¹åã®ã¿ä½¿ç”¨
            available_class_names = [class_names[i] for i in unique_targets if i < len(class_names)]
            
            print("\nğŸ“ˆ åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
            report = classification_report(test_targets, test_preds, 
                                         target_names=available_class_names,
                                         labels=unique_targets)
            print(report)
            
            # æ··åŒè¡Œåˆ—
            cm = confusion_matrix(test_targets, test_preds, labels=unique_targets)
            
            plt.figure(figsize=(8, 6))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                       xticklabels=available_class_names, 
                       yticklabels=available_class_names)
            plt.title(f'æ··åŒè¡Œåˆ— (Test Accuracy: {test_acc:.1f}%)')
            plt.ylabel('çœŸã®åˆ†é¡')
            plt.xlabel('äºˆæ¸¬åˆ†é¡')
            plt.tight_layout()
            
            os.makedirs('plots', exist_ok=True)
            plt.savefig(f'plots/confusion_matrix_{datetime.now().strftime("%Y%m%d_%H%M%S")}.png')
            print(f"ğŸ’¾ æ··åŒè¡Œåˆ—ä¿å­˜: plots/confusion_matrix_*.png")
            # plt.show()  # ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼ˆè‡ªå‹•å®Ÿè¡Œæ™‚ã®ã‚¨ãƒ©ãƒ¼å›é¿ï¼‰
            
        except Exception as e:
            print(f"âš ï¸ è©³ç´°åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            print(f"   åŸºæœ¬è©•ä¾¡ã®ã¿å®Ÿè¡Œ")
        
        # å­¦ç¿’æ›²ç·š
        try:
            self._plot_training_curves()
        except:
            print(f"âš ï¸ å­¦ç¿’æ›²ç·šæç”»ã‚¹ã‚­ãƒƒãƒ—")
        
        return test_acc
    
    def _plot_training_curves(self):
        """å­¦ç¿’æ›²ç·šã®æç”»"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
        
        # Loss
        ax1.plot(self.train_history['loss'], label='Train')
        ax1.plot(self.val_history['loss'], label='Validation')
        ax1.set_title('Loss Curves')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True)
        
        # Accuracy
        ax2.plot(self.train_history['accuracy'], label='Train')
        ax2.plot(self.val_history['accuracy'], label='Validation')
        ax2.set_title('Accuracy Curves')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy (%)')
        ax2.legend()
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig(f'plots/training_curves_{datetime.now().strftime("%Y%m%d_%H%M%S")}.png')
        plt.show()

class EEGClassifierDataCollector(EpisodeContactSynchronizer):
    """
    EEGåˆ†é¡å™¨ç”¨ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ 
    TCPã‹ã‚‰ã®æ˜ç¤ºçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ + LSLã®EEGã‚¨ãƒãƒƒã‚¯ ãƒšã‚¢ã‚’åé›†
    """
    
    def __init__(self, *args, **kwargs):
        # ãƒ‡ãƒ¼ã‚¿åé›†è¨­å®š
        self.collect_training_data = kwargs.pop('collect_training_data', True)
        self.min_samples_per_class = kwargs.pop('min_samples_per_class', 100)

        super().__init__(*args, **kwargs)

        # EEGå‰å‡¦ç†å™¨ã‚’åˆæœŸåŒ–
        self.eeg_preprocessor = NeuroadaptationEEGPreprocessor(
            sampling_rate=self.sampling_rate,
            n_channels=self.n_channels,
            epoch_duration=self.epoch_duration
        )

        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿è“„ç©
        self.training_data = {
            'eeg_epochs': [],           # List of (300, 32) arrays
            'feedback_labels': [],      # List of integers (0, 1, 2)
            'timestamps': [],           # List of timestamps
            'episode_info': []          # List of episode metadata
        }
        
        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†é¡ãƒãƒƒãƒ”ãƒ³ã‚°
        self.feedback_mapping = {
            'success': 0,
            'normal': 0,
            'good': 0,
            'correct': 0,
            'under_grip': 1,
            'weak': 1,
            'insufficient': 1,
            'light': 1,
            'over_grip': 2,
            'strong': 2,
            'excessive': 2,
            'crush': 2,
            'deform': 2
        }
        
        # åé›†çµ±è¨ˆ
        self.collection_stats = Counter()
        
        print(f"ğŸ“š EEGåˆ†é¡å™¨ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–:")
        print(f"   ç›®æ¨™: å„ã‚¯ãƒ©ã‚¹{self.min_samples_per_class}ã‚µãƒ³ãƒ—ãƒ«ä»¥ä¸Š")
        print(f"   ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒãƒƒãƒ”ãƒ³ã‚°: {self.feedback_mapping}")

    def _parse_explicit_feedback(self, tcp_data):
        """
        TCPãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ˜ç¤ºçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯è§£æï¼ˆgrip_forceåˆ¤å®šç‰ˆï¼‰

        ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ«:
        - 8 <= grip_force <= 15 â†’ success (æˆåŠŸ)
        - grip_force > 15      â†’ over_grip (å¼·ã™ã)
        - grip_force < 8       â†’ under_grip (å¼±ã™ã)

        Args:
            tcp_data: Unity TCPãƒ‡ãƒ¼ã‚¿

        Returns:
            feedback_info: è§£æã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æƒ…å ±
        """
        feedback_info = None

        try:
            grip_force = tcp_data.get('grip_force', None)

            print(f"ğŸ” ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯çŠ¶æ…‹è§£æ:")
            print(f"   grip_force: {grip_force}")

            if grip_force is not None:
                if isinstance(grip_force, str):
                    try:
                        grip_force = float(grip_force)
                    except ValueError:
                        grip_force = None
                elif isinstance(grip_force, (int, float)):
                    grip_force = float(grip_force)

            if grip_force is not None:
                # ğŸ¯ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ¤å®šãƒ«ãƒ¼ãƒ«
                if 8 <= grip_force <= 15:
                    feedback_class = 'success'
                    feedback_label = 0
                    confidence = 0.9
                    reasoning = "é©åˆ‡ãªæŠŠæŒåŠ›ï¼ˆ8-15Nï¼‰"
                elif grip_force > 15:
                    feedback_class = 'over_grip'
                    feedback_label = 2
                    confidence = 0.95
                    reasoning = "æŠŠæŒåŠ›éå‰°ï¼ˆ>15Nï¼‰"
                else:
                    feedback_class = 'under_grip'
                    feedback_label = 1
                    confidence = 0.9
                    reasoning = "æŠŠæŒåŠ›ä¸è¶³ï¼ˆ<8Nï¼‰"

                feedback_info = {
                    'class': feedback_class,
                    'label': feedback_label,
                    'confidence': confidence,
                    'explicit': True,
                    'raw_feedback': f"grip_force={grip_force}",
                    'source_field': 'grip_force',
                    'generation_rule': reasoning
                }

                print(f"âœ… ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ¤å®šæˆåŠŸ:")
                print(f"   åˆ†é¡: {feedback_class} (label={feedback_label})")
                print(f"   ä¿¡é ¼åº¦: {confidence:.2f}")
                print(f"   æ ¹æ‹ : {reasoning}")
            else:
                print(f"âŒ å¿…è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“:")
                print(f"   grip_force ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰: {grip_force}")
                print(f"   åˆ©ç”¨å¯èƒ½ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰: {list(tcp_data.keys())}")

        except Exception as e:
            print(f"âŒ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯è§£æã‚¨ãƒ©ãƒ¼: {e}")
            import traceback

            print(f"   è©³ç´°: {traceback.format_exc()}")

        return feedback_info

    def _create_synchronized_event(self, tcp_event: dict, lsl_event: dict = None):
        """
        ãƒ‡ãƒ¼ã‚¿åé›†ç‰ˆã®åŒæœŸã‚¤ãƒ™ãƒ³ãƒˆä½œæˆ
        æ˜ç¤ºçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ + EEGã‚¨ãƒãƒƒã‚¯ ãƒšã‚¢ã‚’åé›†
        """
        try:
            tcp_timestamp = tcp_event['system_time']
            episode_info = tcp_event['episode_info']
            episode_num = episode_info['episode']
            
            # 1. EEGã‚¨ãƒãƒƒã‚¯æŠ½å‡º
            epoch_data, epoch_timestamps, sync_latency = self._extract_epoch_around_time(
                tcp_timestamp, episode_num
            )
            
            if epoch_data is None:
                print(f"âš ï¸ Episode {episode_num}: EEGã‚¨ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ä¸è¶³")
                return None
            
            # 2. ã‚¨ãƒãƒƒã‚¯å‰å‡¦ç†
            preprocess_result = self.eeg_preprocessor.preprocess_epoch(epoch_data)
            epoch_data = preprocess_result['processed_epoch']


            # åŸºæœ¬å“è³ªè©•ä¾¡ã¨é«˜åº¦ãªå“è³ªæŒ‡æ¨™ã‚’çµ±åˆ
            basic_quality = self._assess_epoch_quality(epoch_data)
            advanced_quality = preprocess_result.get('quality_metrics', {})
            epoch_quality = {**basic_quality, **advanced_quality}


            # 3. æ˜ç¤ºçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯è§£æ
            feedback_info = self._parse_explicit_feedback(tcp_event['data'])

            if feedback_info is None:
                print(f"âš ï¸ Episode {episode_num}: æ˜ç¤ºçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æœªæ¤œå‡º")
                # ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¯ç”Ÿæˆã™ã‚‹ãŒå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«ã¯ä½¿ã‚ãªã„
                feedback_value = self._generate_random_feedback(episode_info)
            else:
                # æ˜ç¤ºçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦è“„ç©
                if self.collect_training_data:
                    self.training_data['eeg_epochs'].append(epoch_data.copy())
                    self.training_data['feedback_labels'].append(feedback_info['label'])
                    self.training_data['timestamps'].append(tcp_timestamp)
                    self.training_data['episode_info'].append(episode_info.copy())

                    # çµ±è¨ˆæ›´æ–°
                    self.collection_stats[feedback_info['class']] += 1

                    print(f"ğŸ’¾ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿è“„ç©: Episode {episode_num}")
                    print(f"   ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯: {feedback_info['class']} (ä¿¡é ¼åº¦: {feedback_info['confidence']:.2f})")
                    print(f"   åé›†çµ±è¨ˆ: {dict(self.collection_stats)}")

                # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å€¤ã¯å®Ÿéš›ã®åˆ†é¡çµæœãƒ™ãƒ¼ã‚¹ã§ç”Ÿæˆ
                reward_mapping = {0: 25.0, 1: 15.0, 2: 5.0}  # success, under_grip, over_grip
                feedback_value = self._generate_random_feedback(episode_info)

            # 4. åŒæœŸã‚¤ãƒ™ãƒ³ãƒˆä½œæˆ
            collection_sync_event = {
                'episode_number': episode_info['episode'],
                'contact_timestamp': tcp_timestamp,
                'epoch_data': epoch_data,
                'epoch_timestamps': epoch_timestamps,
                'episode_info': episode_info,
                'tcp_data': tcp_event['data'],
                
                # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æƒ…å ±
                'feedback_info': feedback_info,
                'feedback_value': feedback_value,
                'has_explicit_feedback': feedback_info is not None,

                # å“è³ªè©•ä¾¡
                'sync_latency': sync_latency,
                'epoch_quality': epoch_quality
            }
            
            return collection_sync_event
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿åé›†åŒæœŸã‚¤ãƒ™ãƒ³ãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def save_training_data(self, filename=None):
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if filename is None:
            filename = f'training_data/eeg_training_data_{self.session_id}.pkl'
        
        try:
            os.makedirs('training_data', exist_ok=True)
            
            # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            n_epochs = len(self.training_data['eeg_epochs'])
            n_labels = len(self.training_data['feedback_labels'])
            
            if n_epochs != n_labels:
                print(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ä¸æ•´åˆ: ã‚¨ãƒãƒƒã‚¯æ•°={n_epochs}, ãƒ©ãƒ™ãƒ«æ•°={n_labels}")
                return False
            
            # ä¿å­˜
            with open(filename, 'wb') as f:
                pickle.dump(self.training_data, f)
            
            print(f"ğŸ’¾ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {filename}")
            print(f"   ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {n_epochs}")
            print(f"   ã‚¯ãƒ©ã‚¹åˆ¥åˆ†å¸ƒ: {dict(self.collection_stats)}")
            
            # CSVã‚µãƒãƒªãƒ¼ã‚‚ä¿å­˜
            summary_file = filename.replace('.pkl', '_summary.csv')
            self._save_data_summary(summary_file)
            
            return True
            
        except Exception as e:
            print(f"âŒ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def _save_data_summary(self, filename):
        """ãƒ‡ãƒ¼ã‚¿åé›†ã‚µãƒãƒªãƒ¼ã‚’CSVä¿å­˜"""
        try:
            with open(filename, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(['episode_number', 'timestamp', 'feedback_class', 'feedback_label', 'confidence', 'sync_latency_ms', 'epoch_quality'])
                
                for i, (timestamp, episode_info, feedback_label) in enumerate(zip(
                    self.training_data['timestamps'],
                    self.training_data['episode_info'], 
                    self.training_data['feedback_labels']
                )):
                    # ãƒ©ãƒ™ãƒ«ã‹ã‚‰ã‚¯ãƒ©ã‚¹åã‚’é€†å¼•ã
                    class_name = {v: k for k, v in self.feedback_mapping.items() if v == feedback_label}
                    class_name = list(class_name.keys())[0] if class_name else 'unknown'
                    
                    writer.writerow([
                        episode_info['episode'],
                        timestamp,
                        class_name,
                        feedback_label,
                        1.0,  # confidence (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)
                        0.0,  # sync_latency_ms (è¨ˆç®—çœç•¥)
                        'good'  # epoch_quality (ç°¡ç•¥åŒ–)
                    ])
                    
        except Exception as e:
            print(f"âŒ ã‚µãƒãƒªãƒ¼CSVä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def check_data_sufficiency(self):
        """ãƒ‡ãƒ¼ã‚¿åé›†ååˆ†æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        sufficient = True
        print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿åé›†çŠ¶æ³ãƒã‚§ãƒƒã‚¯:")
        
        # å®Ÿéš›ã«åé›†ã•ã‚ŒãŸãƒ©ãƒ™ãƒ«ã®åˆ†å¸ƒ
        actual_labels = Counter(self.training_data['feedback_labels'])
        total_samples = sum(actual_labels.values())
        
        print(f"   ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {total_samples}")
        
        # å„ã‚¯ãƒ©ã‚¹ã®çŠ¶æ³
        class_names = {0: 'success', 1: 'under_grip', 2: 'over_grip'}
        for label, class_name in class_names.items():
            count = actual_labels.get(label, 0)
            percentage = (count / total_samples * 100) if total_samples > 0 else 0
            needed = max(0, self.min_samples_per_class - count)
            status = "OK" if count >= self.min_samples_per_class else "NG"
            
            print(f"   {class_name:12s}: {count:3d}/{self.min_samples_per_class} ({percentage:4.1f}%) {status}")
            if needed > 0:
                print(f"                    â†’ ã‚ã¨{needed}ã‚µãƒ³ãƒ—ãƒ«å¿…è¦")
                sufficient = False
        
        # å˜ä¸€ã‚¯ãƒ©ã‚¹å•é¡Œã®è­¦å‘Š
        n_classes_present = len([c for c in actual_labels.values() if c > 0])
        if n_classes_present <= 1:
            print(f"é‡è¦: ç¾åœ¨{n_classes_present}ã‚¯ãƒ©ã‚¹ã®ã¿æ¤œå‡º")
            print(f"   åˆ†é¡å™¨å­¦ç¿’ã«ã¯æœ€ä½2ã‚¯ãƒ©ã‚¹ä»¥ä¸Šå¿…è¦ã§ã™")
            print(f"   ç¾åœ¨ã®è‡ªå‹•ãƒ©ãƒ™ãƒ«ç”Ÿæˆãƒ«ãƒ¼ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            sufficient = False
        elif n_classes_present == 2:
            print(f"æ³¨æ„: ç¾åœ¨{n_classes_present}ã‚¯ãƒ©ã‚¹ã®ã¿æ¤œå‡º")
            print(f"   ã‚ˆã‚Šè‰¯ã„æ€§èƒ½ã«ã¯3ã‚¯ãƒ©ã‚¹å…¨ã¦ãŒæ¨å¥¨ã•ã‚Œã¾ã™")
        
        if sufficient:
            print(f"ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†ï¼åˆ†é¡å™¨å­¦ç¿’ã‚’é–‹å§‹ã§ãã¾ã™ã€‚")
        else:
            if n_classes_present <= 1:
                print(f"ãƒ‡ãƒ¼ã‚¿ä¸è¶³: ã‚ˆã‚Šå¤šæ§˜ãªã‚·ãƒŠãƒªã‚ªã§ãƒ‡ãƒ¼ã‚¿åé›†ãŒå¿…è¦")
            else:
                print(f"ãƒ‡ãƒ¼ã‚¿åé›†ç¶™ç¶šä¸­...")
        
        return sufficient and n_classes_present >= 2
        

    def run_data_collection_session(self, duration_seconds=18000, auto_train=True):
        """
        ãƒ‡ãƒ¼ã‚¿åé›†ã‚»ãƒƒã‚·ãƒ§ãƒ³å®Ÿè¡Œ
        
        Args:
            duration_seconds: æœ€å¤§åé›†æ™‚é–“ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ30åˆ†ï¼‰
            auto_train: ãƒ‡ãƒ¼ã‚¿ååˆ†æ™‚ã«è‡ªå‹•å­¦ç¿’é–‹å§‹
        """
        if not self.start_synchronization_system():
            return None
            
        print(f"ğŸ“š EEGåˆ†é¡å™¨ãƒ‡ãƒ¼ã‚¿åé›†ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹")
        print(f"â±ï¸ æœ€å¤§åé›†æ™‚é–“: {duration_seconds}ç§’ ({duration_seconds//60}åˆ†)")
        print(f"ğŸ¯ å„ã‚¯ãƒ©ã‚¹ç›®æ¨™: {self.min_samples_per_class}ã‚µãƒ³ãƒ—ãƒ«")
        
        start_time = time.time()
        last_check_time = start_time
        
        try:
            while self.is_running:
                elapsed = time.time() - start_time
                
                # 30ç§’ã”ã¨ã«é€²æ—ç¢ºèª
                if elapsed - (last_check_time - start_time) >= 30:
                    sufficient = self.check_data_sufficiency()
                    
                    if sufficient and auto_train:
                        print(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿ååˆ†ï¼è‡ªå‹•å­¦ç¿’é–‹å§‹...")
                        break
                    
                    last_check_time = time.time()
                
                # çµ‚äº†æ¡ä»¶ãƒã‚§ãƒƒã‚¯
                if elapsed >= duration_seconds:
                    print(f"â° åˆ¶é™æ™‚é–“åˆ°é”ï¼ˆ{duration_seconds}ç§’ï¼‰")
                    break
                
                time.sleep(1.0)
                
        except KeyboardInterrupt:
            print(f"\nâš¡ ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¸­æ–­")
        finally:
            print(f"ğŸ”š ãƒ‡ãƒ¼ã‚¿åé›†çµ‚äº†...")
            self.stop_synchronization_system()
            
            # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
            saved = self.save_training_data()
            
            if saved and auto_train:
                # è‡ªå‹•å­¦ç¿’å®Ÿè¡Œ
                return self.train_classifier_from_collected_data()
            
            return saved

    def train_classifier_from_collected_data(self):
        """åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åˆ†é¡å™¨ã‚’å­¦ç¿’"""
        print(f"\nğŸ“ EEGåˆ†é¡å™¨å­¦ç¿’é–‹å§‹...")
        
        if len(self.training_data['eeg_epochs']) == 0:
            print(f"âŒ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return None
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        dataset = EEGClassifierDataset(
            self.training_data['eeg_epochs'],
            self.training_data['feedback_labels']
        )
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        total_size = len(dataset)
        train_size = int(0.7 * total_size)
        val_size = int(0.15 * total_size) 
        test_size = total_size - train_size - val_size
        
        train_dataset, val_dataset, test_dataset = random_split(
            dataset, [train_size, val_size, test_size]
        )
        
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²: Train={train_size}, Val={val_size}, Test={test_size}")
        
        # DataLoaderä½œæˆ
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)
        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
        
        # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
        model = DeepConvNetClassifier(n_channels=32, n_samples=300, n_classes=3)
        trainer = EEGClassifierTrainer(model)
        
        # å­¦ç¿’å®Ÿè¡Œ
        print(f"ğŸš€ å­¦ç¿’é–‹å§‹...")
        best_accuracy = trainer.train_full(
            train_loader, val_loader, 
            epochs=100, early_stopping=15
        )
        
        # æœ€çµ‚è©•ä¾¡
        print(f"\nğŸ” æœ€çµ‚ãƒ†ã‚¹ãƒˆè©•ä¾¡...")
        test_accuracy = trainer.evaluate_final(test_loader)
        
        print(f"\nğŸ‰ å­¦ç¿’å®Œäº†!")
        print(f"   ãƒ™ã‚¹ãƒˆæ¤œè¨¼ç²¾åº¦: {best_accuracy:.1f}%")
        print(f"   æœ€çµ‚ãƒ†ã‚¹ãƒˆç²¾åº¦: {test_accuracy:.1f}%")
        
        # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’è¿”ã™
        return {
            'model_path': 'models/best_eeg_classifier.pth',
            'val_accuracy': best_accuracy,
            'test_accuracy': test_accuracy,
            'total_samples': total_size,
            'class_distribution': dict(self.collection_stats)
        }

class EEGDataAugmentation:
    """
    EEGãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚¯ãƒ©ã‚¹
    å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’åŠ¹æœã‚’æœ€å¤§åŒ–
    """
    
    @staticmethod
    def add_noise(epoch, noise_level=0.05):
        """ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºè¿½åŠ """
        noise = np.random.normal(0, noise_level, epoch.shape)
        return epoch + noise
    
    @staticmethod
    def time_shift(epoch, max_shift_samples=10):
        """æ™‚é–“è»¸ã‚·ãƒ•ãƒˆ"""
        shift = np.random.randint(-max_shift_samples, max_shift_samples + 1)
        if shift > 0:
            shifted = np.zeros_like(epoch)
            shifted[shift:, :] = epoch[:-shift, :]
        elif shift < 0:
            shifted = np.zeros_like(epoch)
            shifted[:shift, :] = epoch[-shift:, :]
        else:
            shifted = epoch
        return shifted
    
    @staticmethod
    def channel_dropout(epoch, dropout_prob=0.1):
        """ãƒãƒ£ãƒ³ãƒãƒ«ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ"""
        mask = np.random.random(epoch.shape[1]) > dropout_prob
        augmented = epoch.copy()
        augmented[:, ~mask] = 0
        return augmented
    
    @staticmethod
    def amplitude_scaling(epoch, scale_range=(0.8, 1.2)):
        """æŒ¯å¹…ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°"""
        scale = np.random.uniform(*scale_range)
        return epoch * scale

def preprocess_eeg_epoch_for_training(epoch_data, sampling_rate=250):
    """
    å­¦ç¿’ç”¨EEGã‚¨ãƒãƒƒã‚¯å‰å‡¦ç†ï¼ˆè«–æ–‡æº–æ‹ ï¼‰
    
    Args:
        epoch_data: (300, 32) EEGã‚¨ãƒãƒƒã‚¯
        sampling_rate: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‘¨æ³¢æ•°
        
    Returns:
        processed_epoch: å‰å‡¦ç†æ¸ˆã¿ã‚¨ãƒãƒƒã‚¯
    """
    # ãƒãƒ³ãƒ‰ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ (2-50Hz)
    sos = signal.butter(5, [2, 50], btype='band', fs=sampling_rate, output='sos')
    filtered_epoch = np.zeros_like(epoch_data)
    
    for ch in range(epoch_data.shape[1]):
        try:
            filtered_epoch[:, ch] = signal.sosfilt(sos, epoch_data[:, ch])
        except:
            filtered_epoch[:, ch] = epoch_data[:, ch]  # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¤±æ•—æ™‚ã¯å…ƒãƒ‡ãƒ¼ã‚¿
    
    # Artifact Subspace Reconstruction (ASR) ã®ç°¡æ˜“ç‰ˆ
    # æ¥µç«¯ãªå¤–ã‚Œå€¤ãƒãƒ£ãƒ³ãƒãƒ«ã®é™¤å»
    for ch in range(filtered_epoch.shape[1]):
        ch_data = filtered_epoch[:, ch]
        if np.std(ch_data) > 0:
            z_scores = np.abs((ch_data - np.mean(ch_data)) / np.std(ch_data))
            if np.max(z_scores) > 5:  # 5Ïƒã‚’è¶…ãˆã‚‹å¤–ã‚Œå€¤
                print(f"âš ï¸ Channel {ch}: å¤–ã‚Œå€¤æ¤œå‡ºã€ã‚¼ãƒ­åŒ–")
                filtered_epoch[:, ch] = 0
    
    # æ­£è¦åŒ– (ãƒãƒ£ãƒ³ãƒãƒ«ã”ã¨Z-score)
    for ch in range(filtered_epoch.shape[1]):
        ch_data = filtered_epoch[:, ch]
        if np.std(ch_data) > 1e-10:  # ã‚¼ãƒ­é™¤ç®—å›é¿
            filtered_epoch[:, ch] = (ch_data - np.mean(ch_data)) / np.std(ch_data)
    
    return filtered_epoch

# ğŸ“ å­¦ç¿’å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
def run_complete_training_pipeline():
    """
    å®Œå…¨ãªå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œ
    1. ãƒ‡ãƒ¼ã‚¿åé›† â†’ 2. å­¦ç¿’ â†’ 3. è©•ä¾¡ â†’ 4. å¼·åŒ–å­¦ç¿’çµ±åˆ
    """
    print("ğŸ“ EEGèªçŸ¥ç«¶åˆåˆ†é¡å™¨ å®Œå…¨å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³")
    print("=" * 70)
    
    # Step 1: ãƒ‡ãƒ¼ã‚¿åé›†
    print("\nğŸ“š Step 1: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åé›†")
    data_collector = EEGClassifierDataCollector(
        tcp_host='127.0.0.1',
        tcp_port=12345,
        lsl_stream_name='MockEEG',
        contact_buffer_duration=1.5,
        min_samples_per_class=50,  # å„ã‚¯ãƒ©ã‚¹50ã‚µãƒ³ãƒ—ãƒ«
        collect_training_data=True
    )
    
    # ãƒ‡ãƒ¼ã‚¿åé›†ã‚»ãƒƒã‚·ãƒ§ãƒ³
    training_result = data_collector.run_data_collection_session(
        duration_seconds=1800,  # 30åˆ†é–“
        auto_train=True
    )
    
    if training_result is None:
        print("âŒ ãƒ‡ãƒ¼ã‚¿åé›†å¤±æ•—")
        return
    
    # Step 2: å­¦ç¿’çµæœç¢ºèª
    print(f"\nâœ… å­¦ç¿’å®Œäº†!")
    print(f"   ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: {training_result['model_path']}")
    print(f"   ãƒ†ã‚¹ãƒˆç²¾åº¦: {training_result['test_accuracy']:.1f}%")
    print(f"   å­¦ç¿’ã‚µãƒ³ãƒ—ãƒ«æ•°: {training_result['total_samples']}")
    
    # Step 3: å¼·åŒ–å­¦ç¿’çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã§å®Ÿç”¨åŒ–
    if training_result['test_accuracy'] > 70:  # 70%ä»¥ä¸Šã§å®Ÿç”¨å¯èƒ½
        print(f"\nğŸ¤– Step 3: å¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ")
        
        eeg_rl_system = EEGReinforcementLearningSystem(
            tcp_host='127.0.0.1',
            tcp_port=12345,
            lsl_stream_name='MockEEG',
            contact_buffer_duration=1.5,
            eeg_model_path=training_result['model_path'],
            enable_eeg_classification=True
        )
        
        print(f"ğŸš€ EEGå¼·åŒ–å­¦ç¿’ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹...")
        eeg_rl_system.run_eeg_reinforcement_learning_session(
            duration_seconds=1200,  # 20åˆ†é–“
            target_episodes=100
        )
    else:
        print(f"âš ï¸ åˆ†é¡ç²¾åº¦ãŒä½ã™ãã¾ã™ ({training_result['test_accuracy']:.1f}%)")
        print(f"   ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿åé›†ãŒå¿…è¦ã§ã™")

if __name__ == "__main__":
    print("ğŸ§  EEGèªçŸ¥ç«¶åˆåˆ†é¡å™¨å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ")
    print("TCPã‹ã‚‰ã®æ˜ç¤ºçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã§LSL EEGãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’")
    
    # é¸æŠãƒ¡ãƒ‹ãƒ¥ãƒ¼
    print("\né¸æŠã—ã¦ãã ã•ã„:")
    print("1. ãƒ‡ãƒ¼ã‚¿åé›†ã®ã¿å®Ÿè¡Œ")
    print("2. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã®ã¿å®Ÿè¡Œ") 
    
    choice = input("é¸æŠ (1/2): ").strip()
    
    if choice == "1":
        # ãƒ‡ãƒ¼ã‚¿åé›†ã®ã¿
        collector = EEGClassifierDataCollector(
            tcp_host='127.0.0.1',
            tcp_port=12345,
            lsl_stream_name='MockEEG',
            contact_buffer_duration=1.5,
            min_samples_per_class=100
        )
        collector.run_data_collection_session(duration_seconds=1800, auto_train=False)
        
    elif choice == "2":
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’
        data_file = input("ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (.pkl): ").strip()
        
        try:
            with open(data_file, 'rb') as f:
                training_data = pickle.load(f)
            
            dataset = EEGClassifierDataset(
                training_data['eeg_epochs'],
                training_data['feedback_labels']
            )
            
            # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ãƒ»å­¦ç¿’å®Ÿè¡Œ
            total_size = len(dataset)
            train_size = int(0.7 * total_size)
            val_size = int(0.15 * total_size)
            test_size = total_size - train_size - val_size
            
            train_dataset, val_dataset, test_dataset = random_split(
                dataset, [train_size, val_size, test_size]
            )
            
            train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
            test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
            
            model = DeepConvNetClassifier()
            trainer = EEGClassifierTrainer(model)
            
            trainer.train_full(train_loader, val_loader, epochs=100)
            trainer.evaluate_final(test_loader)
            
        except Exception as e:
            print(f"âŒ å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
    
    elif choice == "3":
        # å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
        run_complete_training_pipeline()
    
    else:
        print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")

"""
ğŸ§  ä½¿ç”¨æ–¹æ³•:

1. **ãƒ‡ãƒ¼ã‚¿åé›†ãƒ•ã‚§ãƒ¼ã‚º**:
   - Unityå´ã§æ˜ç¤ºçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é€ä¿¡
   - TCP: {"feedback": "success"} / {"feedback": "over_grip"} ç­‰
   - è‡ªå‹•ã§EEGã‚¨ãƒãƒƒã‚¯ã¨ãƒšã‚¢ã«ã—ã¦è“„ç©

2. **å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚º**: 
   - åé›†ãƒ‡ãƒ¼ã‚¿ã§DeepConvNet CNNå­¦ç¿’
   - 70%ä»¥ä¸Šã®ç²¾åº¦é”æˆã‚’ç›®æ¨™

3. **å®Ÿç”¨ãƒ•ã‚§ãƒ¼ã‚º**:
   - å­¦ç¿’æ¸ˆã¿åˆ†é¡å™¨ã§æ–°ã—ã„EEGã‚’è‡ªå‹•åˆ†é¡
   - CC-DDPGå¼·åŒ–å­¦ç¿’ã§ãƒ­ãƒœãƒƒãƒˆåˆ¶å¾¡æœ€é©åŒ–

ğŸ“Š **æœŸå¾…ã•ã‚Œã‚‹ç²¾åº¦**: è«–æ–‡ã§ã¯80-90%ã®åˆ†é¡ç²¾åº¦ã‚’é”æˆ
ğŸ¯ **æœ€å°ãƒ‡ãƒ¼ã‚¿è¦ä»¶**: å„ã‚¯ãƒ©ã‚¹50-100ã‚µãƒ³ãƒ—ãƒ«ï¼ˆåˆè¨ˆ150-300ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰
âš¡ **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½**: æ¥è§¦æ¤œå‡ºã‹ã‚‰50msä»¥å†…ã§åˆ†é¡ãƒ»åˆ¶å¾¡
"""