#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TypeB DDPG LSLãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 

i_ddpg_tcp_feedback.py ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ã¦ã€LSLã‚’ä½¿ã£ãŸTypeBã‚·ã‚¹ãƒ†ãƒ 
e_tcp_lsl_sync_system.py ã§LSLãƒ‡ãƒ¼ã‚¿ã‚’å—ã‘å–ã‚Šã€
h_ddpg_realtime_feedback_system.py ã®åˆ†é¡æ©Ÿã§æŠŠæŒåŠ›ã‚¯ãƒ©ã‚¹åˆ†ã‘ã—ã€
DDPGã§å­¦ç¿’ã—ãªãŒã‚‰æŠŠæŒåŠ›ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’é€ä¿¡

æ©Ÿèƒ½:
1. LSLãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ EEGãƒ‡ãƒ¼ã‚¿å—ä¿¡
2. EEGåˆ†é¡ã«ã‚ˆã‚‹æŠŠæŒåŠ›äºˆæ¸¬
3. DDPGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã‚‹æœ€é©åŒ–å­¦ç¿’
4. Unityå´ã¸ã®æŠŠæŒåŠ›ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é€ä¿¡
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import matplotlib
matplotlib.use("Agg")       
from matplotlib import pyplot as plt
import os
import json
import time
import threading
import queue
import argparse
from datetime import datetime
from collections import deque, namedtuple
from pathlib import Path
from scipy import integrate
import warnings
warnings.filterwarnings('ignore')

# æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from e_tcp_lsl_sync_system import LSLTCPEpisodeCollector, Episode
from g_grip_force_realtime_classifier import RealtimeGripForceClassifier
from c_unity_tcp_interface import EEGTCPInterface

# PyTorchè¨­å®š
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ¯ ãƒ‡ãƒã‚¤ã‚¹: {device}")

# DDPGç”¨ã®çµŒé¨“ãƒãƒƒãƒ•ã‚¡
Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])

class Actor(nn.Module):
    """DDPG Actorãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆæŠŠæŒåŠ›å‡ºåŠ›ï¼‰"""
    def __init__(self, state_dim=7, action_dim=1, hidden_dim=256):
        super().__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)
        
        # Layer Normalization for better stability
        self.ln1 = nn.LayerNorm(hidden_dim)
        self.ln2 = nn.LayerNorm(hidden_dim)
        
        # HeåˆæœŸåŒ–
        nn.init.kaiming_normal_(self.fc1.weight)
        nn.init.kaiming_normal_(self.fc2.weight)
        nn.init.uniform_(self.fc3.weight, -3e-3, 3e-3)
    
    def forward(self, state):
        x = F.relu(self.ln1(self.fc1(state)))
        x = F.relu(self.ln2(self.fc2(x)))
        x = torch.tanh(self.fc3(x))  # [-1, 1]ã®ç¯„å›²
        return x

class Critic(nn.Module):
    """DDPG Criticãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆQå€¤å‡ºåŠ›ï¼‰"""
    def __init__(self, state_dim=7, action_dim=1, hidden_dim=256):
        super().__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim + action_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 1)
        
        # Layer Normalization
        self.ln1 = nn.LayerNorm(hidden_dim)
        self.ln2 = nn.LayerNorm(hidden_dim)
        
        nn.init.kaiming_normal_(self.fc1.weight)
        nn.init.kaiming_normal_(self.fc2.weight)
        nn.init.uniform_(self.fc3.weight, -3e-3, 3e-3)
    
    def forward(self, state, action):
        x = F.relu(self.ln1(self.fc1(state)))
        x = torch.cat([x, action], dim=1)
        x = F.relu(self.ln2(self.fc2(x)))
        q_value = self.fc3(x)
        return q_value

class ReplayBuffer:
    """çµŒé¨“ãƒãƒƒãƒ•ã‚¡"""
    def __init__(self, capacity=100000):
        self.buffer = deque(maxlen=capacity)
        self.capacity = capacity
    
    def push(self, experience):
        self.buffer.append(experience)
    
    def sample(self, batch_size):
        batch = np.random.choice(len(self.buffer), batch_size, replace=False)
        states = torch.FloatTensor([self.buffer[i].state for i in batch]).to(device)
        actions = torch.FloatTensor([self.buffer[i].action for i in batch]).to(device)
        rewards = torch.FloatTensor([self.buffer[i].reward for i in batch]).to(device)
        next_states = torch.FloatTensor([self.buffer[i].next_state for i in batch]).to(device)
        dones = torch.BoolTensor([self.buffer[i].done for i in batch]).to(device)
        
        return states, actions, rewards, next_states, dones
    
    def __len__(self):
        return len(self.buffer)

class OUNoise:
    """Ornstein-Uhlenbeck process noise"""
    def __init__(self, size, mu=0., theta=0.15, sigma=0.2):
        self.mu = mu * np.ones(size)
        self.theta = theta
        self.sigma = sigma
        self.reset()
    
    def reset(self):
        self.state = self.mu.copy()
    
    def sample(self):
        dx = self.theta * (self.mu - self.state) + self.sigma * np.random.randn(len(self.state))
        self.state += dx
        return self.state

class DDPGAgent:
    """DDPG ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    def __init__(self, state_dim=7, action_dim=1, lr_actor=1e-4, lr_critic=1e-3):
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        # ãƒ¡ã‚¤ãƒ³ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.actor = Actor(state_dim, action_dim).to(device)
        self.critic = Critic(state_dim, action_dim).to(device)
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.actor_target = Actor(state_dim, action_dim).to(device)
        self.critic_target = Critic(state_dim, action_dim).to(device)
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®åˆæœŸåŒ–
        self._hard_update(self.actor_target, self.actor)
        self._hard_update(self.critic_target, self.critic)
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)
        
        # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.gamma = 0.99
        self.tau = 0.005  # ã‚½ãƒ•ãƒˆã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆä¿‚æ•°
        
        # çµŒé¨“ãƒãƒƒãƒ•ã‚¡
        self.memory = ReplayBuffer(capacity=100000)
        
        # ãƒã‚¤ã‚º
        self.noise = OUNoise(action_dim, sigma=0.2)
        
    def select_action(self, state, add_noise=True, noise_scale=1.0):
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³é¸æŠ"""
        state = torch.FloatTensor(state).unsqueeze(0).to(device)
        self.actor.eval()
        with torch.no_grad():
            action = self.actor(state).cpu().data.numpy().flatten()
        self.actor.train()
        
        if add_noise:
            action += noise_scale * self.noise.sample()
            action = np.clip(action, -1, 1)
        
        return action
    
    def update(self, batch_size=64):
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ›´æ–°"""
        if len(self.memory) < batch_size:
            return None, None
        
        states, actions, rewards, next_states, dones = self.memory.sample(batch_size)
        
        # Criticæ›´æ–°
        with torch.no_grad():
            next_actions = self.actor_target(next_states)
            target_q = self.critic_target(next_states, next_actions)
            target_q = rewards + (self.gamma * target_q * (~dones))
        
        current_q = self.critic(states, actions)
        critic_loss = F.mse_loss(current_q, target_q)
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()
        
        # Actoræ›´æ–°
        actor_loss = -self.critic(states, self.actor(states)).mean()
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
        
        # ã‚½ãƒ•ãƒˆã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ
        self._soft_update(self.actor_target, self.actor)
        self._soft_update(self.critic_target, self.critic)
        
        return critic_loss.item(), actor_loss.item()
    
    def _soft_update(self, target, source):
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(target_param.data * (1.0 - self.tau) + param.data * self.tau)
    
    def _hard_update(self, target, source):
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(param.data)

class SystemHealthChecker:
    """ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯ã‚¯ãƒ©ã‚¹"""
    def __init__(self):
        self.checks = {}
        self.last_check_results = {}
        
    def register_check(self, name, check_func, critical=False):
        self.checks[name] = {'func': check_func, 'critical': critical}
    
    def run_all_checks(self):
        results = {}
        all_passed = True
        
        for name, check_info in self.checks.items():
            try:
                result = check_info['func']()
                results[name] = result
                self.last_check_results[name] = result
                
                if not result['success'] and check_info['critical']:
                    all_passed = False
                    
            except Exception as e:
                results[name] = {'success': False, 'details': f'ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}'}
                if check_info['critical']:
                    all_passed = False
        
        return {'all_passed': all_passed, 'results': results}

class EEGLSLEnvironment:
    """EEG-LSLãƒ™ãƒ¼ã‚¹ã®ç’°å¢ƒã‚¯ãƒ©ã‚¹"""
    def __init__(self, classifier=None):
        self.classifier = classifier
        self.current_state = None
        self.previous_grip_force = 12.0
        self.episode_reward = 0.0
        
        # å ±é…¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.reward_params = {
            'success_reward': 10.0,
            'error_penalty_coeff': 2.0,
            'damage_penalty': 15.0,
            'contact_bonus': 3.0,
            'classification_bonus': 2.0
        }
        
    def compute_state_from_eeg(self, eeg_data, tcp_data):
        """EEGãƒ‡ãƒ¼ã‚¿ã¨TCPãƒ‡ãƒ¼ã‚¿ã‹ã‚‰çŠ¶æ…‹ã‚’è¨ˆç®—"""
        try:
            state = np.zeros(7)  # 7æ¬¡å…ƒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«
            
            # EEGç‰¹å¾´é‡ã®æŠ½å‡º
            if eeg_data is not None and len(eeg_data) > 0:
                # åŸºæœ¬çš„ãªçµ±è¨ˆç‰¹å¾´é‡
                state[0] = np.mean(eeg_data)  # å¹³å‡
                state[1] = np.std(eeg_data)   # æ¨™æº–åå·®
                state[2] = np.max(eeg_data) - np.min(eeg_data)  # ç¯„å›²
                
                # å‘¨æ³¢æ•°ç‰¹å¾´é‡ï¼ˆç°¡æ˜“FFTï¼‰
                fft = np.fft.fft(eeg_data.flatten()[:512])  # æœ€åˆã®512ã‚µãƒ³ãƒ—ãƒ«
                power_spectrum = np.abs(fft[:256])  # ãƒŠã‚¤ã‚­ã‚¹ãƒˆå‘¨æ³¢æ•°ã¾ã§
                state[3] = np.mean(power_spectrum[:30])   # ä½å‘¨æ³¢æˆåˆ†
                state[4] = np.mean(power_spectrum[30:100])  # ä¸­å‘¨æ³¢æˆåˆ†
            
            # TCPæƒ…å ±ã‹ã‚‰çŠ¶æ…‹ã‚’è¿½åŠ 
            if tcp_data and isinstance(tcp_data, dict):
                state[5] = tcp_data.get('grip_force', self.previous_grip_force) / 30.0  # æ­£è¦åŒ–
                state[6] = tcp_data.get('contact_pressure', 0.0) / 10.0  # æ­£è¦åŒ–
            
            # åˆ†é¡æ©Ÿã«ã‚ˆã‚‹æŠŠæŒåŠ›äºˆæ¸¬ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
            if self.classifier:
                try:
                    predicted_class, confidence = self.classifier.classify_episode_data(eeg_data)
                    # åˆ†é¡çµæœã‚’çŠ¶æ…‹ã«åæ˜ ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                    class_map = {'UnderGrip': -1, 'Success': 0, 'OverGrip': 1}
                    if predicted_class in class_map:
                        # çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®æœ€å¾Œã«åˆ†é¡æƒ…å ±ã‚’è¿½åŠ ã™ã‚‹å ´åˆ
                        pass  # ç¾åœ¨ã¯7æ¬¡å…ƒå›ºå®šã®ãŸã‚ã€è¿½åŠ æƒ…å ±ã¨ã—ã¦åˆ©ç”¨
                except Exception as e:
                    print(f"âš ï¸ EEGåˆ†é¡ã‚¨ãƒ©ãƒ¼: {e}")
            
            self.current_state = state
            return state
            
        except Exception as e:
            print(f"âŒ çŠ¶æ…‹è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return np.zeros(7)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆçŠ¶æ…‹
    
    def compute_reward(self, action, tcp_data, eeg_classification=None):
        """å ±é…¬è¨ˆç®—"""
        reward = 0.0
        
        try:
            if tcp_data and isinstance(tcp_data, dict):
                # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰æŠŠæŒåŠ›ã«å¤‰æ›
                grip_force = self.action_to_grip_force(action[0])
                target_force = tcp_data.get('target_grip_force', 15.0)
                actual_force = tcp_data.get('grip_force', grip_force)
                
                # æŠŠæŒæˆåŠŸå ±é…¬
                force_error = abs(actual_force - target_force)
                if force_error < 2.0:  # è¨±å®¹èª¤å·®å†…
                    reward += self.reward_params['success_reward']
                    reward += self.reward_params['contact_bonus']
                else:
                    reward -= force_error * self.reward_params['error_penalty_coeff']
                
                # ç ´æãƒšãƒŠãƒ«ãƒ†ã‚£
                if actual_force > 25.0:  # éåº¦ãªåŠ›
                    reward -= self.reward_params['damage_penalty']
                
                # åˆ†é¡æ©Ÿã«ã‚ˆã‚‹è¿½åŠ å ±é…¬
                if eeg_classification:
                    predicted_class, confidence = eeg_classification
                    if predicted_class == 'Success' and confidence > 0.8:
                        reward += self.reward_params['classification_bonus']
                
                self.episode_reward += reward
                
        except Exception as e:
            print(f"âš ï¸ å ±é…¬è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            reward = -1.0  # ã‚¨ãƒ©ãƒ¼ãƒšãƒŠãƒ«ãƒ†ã‚£
        
        return reward
    
    def action_to_grip_force(self, action_value):
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å€¤ã‚’æŠŠæŒåŠ›ã«å¤‰æ› [-1,1] -> [5,25]N"""
        return 5.0 + (action_value + 1.0) * 10.0  # [5, 25]N

class TypeBDDPGLSLSystem:
    """TypeB DDPG LSLãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ çµ±åˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, 
                 model_path='models/improved_grip_force_classifier_*.pth',
                 lsl_stream_name='MockEEG',
                 tcp_host='127.0.0.1',
                 tcp_port=12345,
                 feedback_port=12346,
                 experiment_type="B_400"):
        
        self.model_path = model_path
        self.lsl_stream_name = lsl_stream_name
        self.tcp_host = tcp_host
        self.tcp_port = tcp_port
        self.feedback_port = feedback_port
        self.experiment_type = experiment_type
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.init_lsl_data_collector()
        self.init_eeg_classifier()
        self.init_feedback_interface()
        
        # DDPGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
        self.agent = DDPGAgent(state_dim=7, action_dim=1, lr_actor=1e-4, lr_critic=1e-3)
        
        # ç’°å¢ƒ
        self.environment = EEGLSLEnvironment(classifier=self.classifier)
        
        # å®Ÿè¡Œåˆ¶å¾¡
        self.is_running = False
        self.learning_thread = None
        self.data_processing_thread = None
        
        # å­¦ç¿’çµ±è¨ˆ
        self.stats = {
            'total_episodes': 0,
            'successful_episodes': 0,
            'total_rewards': [],
            'episode_rewards': [],
            'classification_accuracy': [],
            'grip_force_history': [],
            'eeg_data_count': 0,
            'learning_updates': 0,
            'start_time': None
        }
        
        # çŠ¶æ…‹ç®¡ç†
        self.previous_state = None
        self.previous_action = None
        self.current_episode_reward = 0.0
        
        # ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¥ãƒ¼ã‚¤ãƒ³ã‚°
        self.eeg_data_queue = queue.Queue(maxsize=1000)
        self.tcp_data_queue = queue.Queue(maxsize=1000)
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.model_save_dir = f"models/ddpg_lsl_typeb_{self.session_id}"
        os.makedirs(self.model_save_dir, exist_ok=True)
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç®¡ç†
        self.target_episodes = 400 if "400" in experiment_type else 1000
        self.episode_count = 0
        
        # å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯
        self.health_checker = SystemHealthChecker()
        self._setup_health_checks()
        
        print(f"ğŸš€ TypeB DDPG LSLãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        print(f"   å®Ÿé¨“ã‚¿ã‚¤ãƒ—: {experiment_type}")
        print(f"   ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {self.session_id}")
        print(f"   ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ: {self.model_save_dir}")
        print(f"   ç›®æ¨™ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {self.target_episodes}")
    
    def init_lsl_data_collector(self):
        """LSLãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        self.data_collector = LSLTCPEpisodeCollector(
            lsl_stream_name=self.lsl_stream_name,
            tcp_host=self.tcp_host,
            tcp_port=self.tcp_port,
            save_to_csv=False,  # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’ã®ãŸã‚ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã¯ç„¡åŠ¹
            enable_state_sharing=True,  # çŠ¶æ…‹å…±æœ‰ã‚’æœ‰åŠ¹
            trigger_on_robot_state=True  # ãƒ­ãƒœãƒƒãƒˆçŠ¶æ…‹ã§ãƒˆãƒªã‚¬ãƒ¼
        )
        
        print(f"âœ… LSLãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def init_eeg_classifier(self):
        """EEGåˆ†é¡æ©ŸåˆæœŸåŒ–"""
        try:
            self.classifier = RealtimeGripForceClassifier(
                model_path=self.model_path,
                lsl_stream_name=self.lsl_stream_name,
                tcp_host=self.tcp_host,
                tcp_port=self.tcp_port
            )
            
            if not self.classifier.load_model():
                print(f"âš ï¸ EEGåˆ†é¡æ©Ÿèª­ã¿è¾¼ã¿å¤±æ•— - åŸºæœ¬æ©Ÿèƒ½ã§ç¶šè¡Œ")
                self.classifier = None
            else:
                print(f"âœ… EEGåˆ†é¡æ©ŸåˆæœŸåŒ–å®Œäº†")
                
        except Exception as e:
            print(f"âš ï¸ EEGåˆ†é¡æ©ŸåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            self.classifier = None
    
    def init_feedback_interface(self):
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é€šä¿¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹åˆæœŸåŒ–"""
        self.feedback_interface = EEGTCPInterface(
            host=self.tcp_host,
            port=self.feedback_port,
            auto_reply=False
        )
        
        # æŠŠæŒåŠ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
        self.feedback_interface.add_message_callback(self.handle_grip_force_request)
        
        print(f"âœ… ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é€šä¿¡åˆæœŸåŒ–å®Œäº† (Port: {self.feedback_port})")
    

    def handle_grip_force_request(self, message_data):
        """Unityå´ã‹ã‚‰ã®æŠŠæŒåŠ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†"""
        try:
            # ç¾åœ¨ã®çŠ¶æ…‹ã‹ã‚‰æœ€é©ãªæŠŠæŒåŠ›ã‚’è¨ˆç®—
            if self.previous_state is not None:
                # DDPGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‹ã‚‰ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å–å¾—ï¼ˆæ¨è«–ç”¨ï¼‰
                action = self.agent.select_action(self.previous_state, add_noise=False, noise_scale=0.0)
                
                # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠŠæŒåŠ›ã«å¤‰æ›
                grip_force = self.environment.action_to_grip_force(action[0])
                
                print(f"ğŸ¯ TypeBæŠŠæŒåŠ›ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯: {grip_force:.2f}N (action: {action[0]:.3f})")
                
            else:
                # çŠ¶æ…‹ãŒãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                grip_force = 12.0
                print(f"âš ï¸ åˆæœŸçŠ¶æ…‹ - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæŠŠæŒåŠ›: {grip_force}N")
            
            # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é€ä¿¡
            response = {
                'type': 'grip_force_command',
                'target_force': float(grip_force),
                'timestamp': time.time(),
                'session_id': f"ddpg_lsl_typeb_{self.session_id}",
                'learning_episode': self.stats['total_episodes'],
                'system_type': 'TypeB_LSL'
            }
            
            self.feedback_interface.send_message(response)
            
            # çµ±è¨ˆæ›´æ–°
            self.stats['grip_force_history'].append(grip_force)
            
        except Exception as e:
            print(f"âŒ æŠŠæŒåŠ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _data_processing_loop(self):
        """ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ«ãƒ¼ãƒ—ï¼ˆåˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ï¼‰"""
        print(f"ğŸ”„ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ«ãƒ¼ãƒ—é–‹å§‹")
        
        last_processed_episode = 0
        
        while self.is_running:
            try:
                # LSLTCPEpisodeCollectorã‹ã‚‰æ–°ã—ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å–å¾—
                if hasattr(self.data_collector, 'episodes') and len(self.data_collector.episodes) > last_processed_episode:
                    # æ–°ã—ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å‡¦ç†
                    for i in range(last_processed_episode, len(self.data_collector.episodes)):
                        episode = self.data_collector.episodes[i]
                        
                        # TCPãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
                        if not self.tcp_data_queue.full():
                            self.tcp_data_queue.put(episode.tcp_data, timeout=0.1)
                        
                        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
                        episode_data = {
                            'lsl_data': episode.lsl_data,
                            'lsl_timestamps': episode.lsl_timestamps,
                            'episode_id': episode.episode_id,
                            'trigger_timestamp': episode.trigger_timestamp
                        }
                        
                        if not self.eeg_data_queue.full():
                            self.eeg_data_queue.put(episode_data, timeout=0.1)
                            self.stats['eeg_data_count'] += 1
                        else:
                            print(f"âš ï¸ EEGãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¥ãƒ¼ãƒ•ãƒ« - ãƒ‡ãƒ¼ã‚¿ã‚’ç ´æ£„")
                    
                    last_processed_episode = len(self.data_collector.episodes)
                
                # EEGãƒ‡ãƒ¼ã‚¿ã¨TCPãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
                if not self.eeg_data_queue.empty() and not self.tcp_data_queue.empty():
                    eeg_episode = self.eeg_data_queue.get(timeout=1.0)
                    tcp_data = self.tcp_data_queue.get(timeout=1.0)
                    
                    # çŠ¶æ…‹è¨ˆç®—
                    state = self.environment.compute_state_from_eeg(
                        eeg_episode.get('lsl_data'), 
                        tcp_data
                    )
                    
                    # EEGåˆ†é¡ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
                    eeg_classification = None
                    if self.classifier and eeg_episode.get('lsl_data') is not None:
                        try:
                            eeg_classification = self.classifier.classify_episode_data(
                                eeg_episode.get('lsl_data')
                            )
                        except Exception as e:
                            print(f"âš ï¸ EEGåˆ†é¡ã‚¨ãƒ©ãƒ¼: {e}")
                    
                    # DDPGå­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—
                    if self.previous_state is not None and self.previous_action is not None:
                        # å ±é…¬è¨ˆç®—
                        reward = self.environment.compute_reward(
                            self.previous_action, tcp_data, eeg_classification
                        )
                        
                        # çµŒé¨“ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
                        experience = Experience(
                            state=self.previous_state,
                            action=self.previous_action,
                            reward=reward,
                            next_state=state,
                            done=False
                        )
                        self.agent.memory.push(experience)
                        
                        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ›´æ–°
                        if len(self.agent.memory) > 64:
                            critic_loss, actor_loss = self.agent.update()
                            if critic_loss is not None:
                                self.stats['learning_updates'] += 1
                    
                    # æ–°ã—ã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³é¸æŠ
                    action = self.agent.select_action(state, add_noise=True)
                    
                    # çŠ¶æ…‹æ›´æ–°
                    self.previous_state = state
                    self.previous_action = action
                    
                    # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ±è¨ˆ
                    self.episode_count += 1
                    self.stats['total_episodes'] = self.episode_count
                    
                    # å®šæœŸçš„ãªé€²æ—è¡¨ç¤º
                    if self.episode_count % 50 == 0:
                        print(f"ğŸ“ˆ é€²æ—: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰{self.episode_count}/{self.target_episodes}, "
                              f"å­¦ç¿’æ›´æ–°{self.stats['learning_updates']}å›")
                
                else:
                    time.sleep(0.1)  # ãƒ‡ãƒ¼ã‚¿å¾…æ©Ÿ
                    
            except queue.Empty:
                continue
            except Exception as e:
                print(f"âŒ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ«ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
                time.sleep(0.1)
    
    def _learning_loop(self):
        """ãƒ¡ã‚¤ãƒ³å­¦ç¿’ãƒ«ãƒ¼ãƒ—"""
        print(f"ğŸ“ TypeBå­¦ç¿’ãƒ«ãƒ¼ãƒ—é–‹å§‹")
        
        start_time = time.time()
        self.stats['start_time'] = start_time
        
        while self.is_running and self.episode_count < self.target_episodes:
            try:
                # å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯
                if self.episode_count % 100 == 0:
                    health_result = self.health_checker.run_all_checks()
                    if not health_result['all_passed']:
                        print(f"âš ï¸ ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯å¤±æ•—")
                        for name, result in health_result['results'].items():
                            if not result['success']:
                                print(f"   {name}: {result['details']}")
                
                # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆå®šæœŸçš„ï¼‰
                if self.episode_count > 0 and self.episode_count % 200 == 0:
                    self.save_models(f"checkpoint_episode_{self.episode_count}")
                
                time.sleep(1.0)  # CPUãƒ­ãƒ¼ãƒ‰è»½æ¸›
                
            except KeyboardInterrupt:
                print(f"\nâ¹ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¸­æ–­")
                break
            except Exception as e:
                print(f"âŒ å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
                time.sleep(1.0)
        
        print(f"âœ… å­¦ç¿’å®Œäº†: {self.episode_count}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰, å®Ÿè¡Œæ™‚é–“: {time.time() - start_time:.1f}ç§’")
    
    def start_learning(self):
        """å­¦ç¿’é–‹å§‹"""
        if self.is_running:
            print(f"âš ï¸ æ—¢ã«å®Ÿè¡Œä¸­")
            return False
        
        try:
            # ã‚µãƒ–ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹
            if not self.data_collector.start_collection():
                print(f"âŒ ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹å¤±æ•—")
                return False
            
            if not self.feedback_interface.start_server():
                print(f"âŒ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é€šä¿¡é–‹å§‹å¤±æ•—")
                return False
            
            self.is_running = True
            
            # ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹
            self.data_processing_thread = threading.Thread(target=self._data_processing_loop)
            self.learning_thread = threading.Thread(target=self._learning_loop)
            
            self.data_processing_thread.start()
            self.learning_thread.start()
            
            print(f"ğŸš€ TypeB DDPGå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
            return True
            
        except Exception as e:
            print(f"âŒ ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹ã‚¨ãƒ©ãƒ¼: {e}")
            self.stop_learning()
            return False
    
    def stop_learning(self):
        """å­¦ç¿’åœæ­¢"""
        print(f"â¹ï¸ ã‚·ã‚¹ãƒ†ãƒ åœæ­¢ä¸­...")
        
        self.is_running = False
        
        # ã‚¹ãƒ¬ãƒƒãƒ‰åœæ­¢å¾…æ©Ÿ
        if self.learning_thread and self.learning_thread.is_alive():
            self.learning_thread.join(timeout=5.0)
        
        if self.data_processing_thread and self.data_processing_thread.is_alive():
            self.data_processing_thread.join(timeout=5.0)
        
        # ã‚µãƒ–ã‚·ã‚¹ãƒ†ãƒ åœæ­¢
        if hasattr(self.data_collector, 'stop_collection'):
            self.data_collector.stop_collection()
        
        if hasattr(self.feedback_interface, 'stop_server'):
            self.feedback_interface.stop_server()
        
        # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        self.save_models("final")
        
        print(f"âœ… ã‚·ã‚¹ãƒ†ãƒ åœæ­¢å®Œäº†")
    
    def save_models(self, suffix=""):
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        try:
            model_path = os.path.join(self.model_save_dir, f"actor_{suffix}.pth")
            torch.save(self.agent.actor.state_dict(), model_path)
            
            critic_path = os.path.join(self.model_save_dir, f"critic_{suffix}.pth")
            torch.save(self.agent.critic.state_dict(), critic_path)
            
            # çµ±è¨ˆæƒ…å ±ä¿å­˜
            stats_path = os.path.join(self.model_save_dir, f"stats_{suffix}.json")
            with open(stats_path, 'w') as f:
                json.dump(self.stats, f, indent=2, default=str)
            
            print(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {suffix}")
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _setup_health_checks(self):
        """å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯é …ç›®ã®è¨­å®š"""
        
        def check_lsl_connection():
            """LSLæ¥ç¶šãƒã‚§ãƒƒã‚¯"""
            return {
                'success': getattr(self.data_collector, 'is_running', True),
                'details': f"LSLãƒ‡ãƒ¼ã‚¿åé›†: {'ç¨¼åƒä¸­' if getattr(self.data_collector, 'is_running', True) else 'åœæ­¢ä¸­'}"
            }
        
        def check_feedback_interface():
            """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é€šä¿¡ãƒã‚§ãƒƒã‚¯"""
            return {
                'success': getattr(self.feedback_interface, 'is_connected', True),
                'details': f"ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é€šä¿¡: {'æ¥ç¶šä¸­' if getattr(self.feedback_interface, 'is_connected', True) else 'æœªæ¥ç¶š'}"
            }
        
        def check_learning_progress():
            """å­¦ç¿’é€²æ—ãƒã‚§ãƒƒã‚¯"""
            buffer_ratio = len(self.agent.memory) / self.agent.memory.capacity
            return {
                'success': True,
                'details': f"ãƒãƒƒãƒ•ã‚¡åˆ©ç”¨ç‡: {buffer_ratio:.1%}, ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {self.episode_count}/{self.target_episodes}"
            }
        
        # ãƒã‚§ãƒƒã‚¯é …ç›®ç™»éŒ²
        self.health_checker.register_check("lsl_connection", check_lsl_connection, critical=True)
        self.health_checker.register_check("feedback_interface", check_feedback_interface, critical=True)
        self.health_checker.register_check("learning_progress", check_learning_progress, critical=False)
    
    def print_status(self):
        """ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º"""
        print(f"\nğŸ¤– TypeB DDPG LSLã‚·ã‚¹ãƒ†ãƒ  ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹")
        print(f"=" * 60)
        print(f"   ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {self.session_id}")
        print(f"   å®Ÿé¨“ã‚¿ã‚¤ãƒ—: {self.experiment_type}")
        print(f"   å®Ÿè¡ŒçŠ¶æ…‹: {'ç¨¼åƒä¸­' if self.is_running else 'åœæ­¢ä¸­'}")
        print(f"   ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é€²æ—: {self.episode_count}/{self.target_episodes}")
        print(f"   å­¦ç¿’æ›´æ–°å›æ•°: {self.stats['learning_updates']}")
        print(f"   EEGãƒ‡ãƒ¼ã‚¿å‡¦ç†å›æ•°: {self.stats['eeg_data_count']}")
        
        if self.stats['grip_force_history']:
            avg_grip_force = np.mean(self.stats['grip_force_history'][-50:])
            print(f"   å¹³å‡æŠŠæŒåŠ›ï¼ˆæœ€æ–°50ï¼‰: {avg_grip_force:.2f}N")
        
        print(f"   çµŒé¨“ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º: {len(self.agent.memory)}")
        
        if self.stats['start_time']:
            uptime = time.time() - self.stats['start_time']
            print(f"   ç¨¼åƒæ™‚é–“: {uptime:.1f}ç§’")
        
        print(f"   ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ: {self.model_save_dir}")
    
    def run_demo(self):
        """ãƒ‡ãƒ¢å®Ÿè¡Œ"""
        print(f"ğŸš€ TypeB DDPG LSLãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ  ãƒ‡ãƒ¢å®Ÿè¡Œ")
        
        if self.start_learning():
            try:
                print(f"\nğŸ’¡ ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒä¸­...")
                print(f"   ğŸ“¡ LSLã‚¹ãƒˆãƒªãƒ¼ãƒ : {self.lsl_stream_name}")
                print(f"   ğŸ“¡ TCPãƒãƒ¼ãƒˆ: {self.tcp_port}")
                print(f"   ğŸ“¡ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒãƒ¼ãƒˆ: {self.feedback_port}")
                print(f"   ğŸ§  EEGåˆ†é¡æ©Ÿ: {'æœ‰åŠ¹' if self.classifier else 'ç„¡åŠ¹'}")
                print(f"   ğŸ“ DDPGãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’å®Ÿè¡Œä¸­")
                print(f"   ç›®æ¨™ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {self.target_episodes}")
                print(f"   Ctrl+C ã§çµ‚äº†")
                
                # ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—
                while self.is_running and self.episode_count < self.target_episodes:
                    time.sleep(5.0)
                    
                    # å®šæœŸçš„ãªé€²æ—è¡¨ç¤º
                    progress_percent = (self.episode_count / self.target_episodes) * 100
                    print(f"ğŸ“ˆ é€²æ—: {progress_percent:.1f}% "
                          f"({self.episode_count}/{self.target_episodes} ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰), "
                          f"å­¦ç¿’æ›´æ–°{self.stats['learning_updates']}å›")
                    
            except KeyboardInterrupt:
                print(f"\nâ¹ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼åœæ­¢")
            finally:
                self.stop_learning()
        else:
            print(f"âŒ ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹å¤±æ•—")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description="TypeB DDPG LSLãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument("--type", choices=["B_400", "B_long"], default="B_400",
                       help="å®Ÿé¨“ã‚¿ã‚¤ãƒ—")
    parser.add_argument("--lsl-stream", default="MockEEG",
                       help="LSLã‚¹ãƒˆãƒªãƒ¼ãƒ å")
    parser.add_argument("--tcp-port", type=int, default=12345,
                       help="TCPãƒãƒ¼ãƒˆ")
    parser.add_argument("--feedback-port", type=int, default=12346,
                       help="ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒãƒ¼ãƒˆ")
    parser.add_argument("--model-path", default="models/improved_grip_force_classifier_*.pth",
                       help="EEGåˆ†é¡æ©Ÿãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹")
    
    args = parser.parse_args()
    
    print(f"ğŸ§  TypeB DDPG LSLãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ")
    print(f"=" * 70)
    print(f"ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆ:")
    print(f"  ğŸ“¡ LSLãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ EEGãƒ‡ãƒ¼ã‚¿å—ä¿¡")
    print(f"  ğŸ§  EEGåˆ†é¡ã«ã‚ˆã‚‹æŠŠæŒåŠ›äºˆæ¸¬")
    print(f"  ğŸ¤– DDPGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã‚‹æœ€é©åŒ–å­¦ç¿’")
    print(f"  ğŸ“¤ Unityå´ã¸ã®æŠŠæŒåŠ›ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é€ä¿¡")
    print(f"=" * 70)
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    system = TypeBDDPGLSLSystem(
        model_path=args.model_path,
        lsl_stream_name=args.lsl_stream,
        tcp_host='127.0.0.1',
        tcp_port=args.tcp_port,
        feedback_port=args.feedback_port,
        experiment_type=args.type
    )
    
    # ãƒ‡ãƒ¢å®Ÿè¡Œ
    system.run_demo()

if __name__ == "__main__":
    main()