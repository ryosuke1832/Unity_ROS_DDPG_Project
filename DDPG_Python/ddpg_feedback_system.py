#!/usr/bin/env python3
"""
DDPGå¼·åŒ–å­¦ç¿’ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ 

çµ±åˆãƒ•ãƒ­ãƒ¼:
1. tcp_lsl_sync_system.py ã§LSLãƒ‡ãƒ¼ã‚¿ã¨TCPãƒ‡ãƒ¼ã‚¿ã‚’å—ä¿¡
2. grip_force_classifier.py ã§åˆ†é¡æ©Ÿã‚’ä½¿ã£ã¦3ã‚¯ãƒ©ã‚¹åˆ†é¡ (UnderGrip/Success/OverGrip)
3. DDPGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒåˆ†é¡çµæœã¨TCP GripForceã‚’ã‚‚ã¨ã«æ¬¡ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®é©åˆ‡ãªæŠŠæŒåŠ›ã‚’å­¦ç¿’
4. unity_tcp_interface.py ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«å¯¾ã—ã¦å­¦ç¿’æ¸ˆã¿æŠŠæŒåŠ›ã‚’å¿œç­”
5. ç¶™ç¶šçš„ã«ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ã§å­¦ç¿’ã‚’é€²ã‚ã‚‹
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import random
import time
import threading
import queue
import json
import os
from collections import deque, namedtuple
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
import pickle

# æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from tcp_lsl_sync_system import LSLTCPEpisodeCollector, Episode
from unity_tcp_interface import EEGTCPInterface
from grip_force_classifier import RealtimeGripForceClassifier

# PyTorchè¨­å®š
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ”§ ãƒ‡ãƒã‚¤ã‚¹: {device}")

# çµŒé¨“ãƒãƒƒãƒ•ã‚¡ç”¨ã®namedtuple
Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])

class Actor(nn.Module):
    """DDPGã‚¢ã‚¯ã‚¿ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆæŠŠæŒåŠ›ã‚’å‡ºåŠ›ï¼‰"""
    
    def __init__(self, state_dim=6, action_dim=1, hidden_dim=128):
        super(Actor, self).__init__()
        
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)
        
        # HeåˆæœŸåŒ–
        nn.init.kaiming_normal_(self.fc1.weight)
        nn.init.kaiming_normal_(self.fc2.weight)
        nn.init.uniform_(self.fc3.weight, -3e-3, 3e-3)
    
    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        x = torch.tanh(self.fc3(x))  # [-1, 1]ã®ç¯„å›²ã«æ­£è¦åŒ–
        return x

class Critic(nn.Module):
    """DDPGã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆQå€¤ã‚’å‡ºåŠ›ï¼‰"""
    
    def __init__(self, state_dim=6, action_dim=1, hidden_dim=128):
        super(Critic, self).__init__()
        
        # State pathway
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        
        # Combined pathway (state + action)
        self.fc2 = nn.Linear(hidden_dim + action_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 1)
        
        # HeåˆæœŸåŒ–
        nn.init.kaiming_normal_(self.fc1.weight)
        nn.init.kaiming_normal_(self.fc2.weight)
        nn.init.uniform_(self.fc3.weight, -3e-3, 3e-3)
    
    def forward(self, state, action):
        x = F.relu(self.fc1(state))
        x = torch.cat([x, action], dim=1)
        x = F.relu(self.fc2(x))
        q_value = self.fc3(x)
        return q_value

class OUNoise:
    """Ornstein-Uhlenbeck ãƒã‚¤ã‚ºï¼ˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ¢ç´¢ç”¨ï¼‰"""
    
    def __init__(self, action_dim, mu=0.0, theta=0.15, sigma=0.2):
        self.action_dim = action_dim
        self.mu = mu
        self.theta = theta
        self.sigma = sigma
        self.state = np.ones(self.action_dim) * self.mu
        self.reset()
    
    def reset(self):
        self.state = np.ones(self.action_dim) * self.mu
    
    def sample(self):
        x = self.state
        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))
        self.state = x + dx
        return self.state

class ReplayBuffer:
    """çµŒé¨“å†ç”Ÿãƒãƒƒãƒ•ã‚¡"""
    
    def __init__(self, capacity=100000):
        self.buffer = deque(maxlen=capacity)
        self.capacity = capacity
    
    def push(self, state, action, reward, next_state, done):
        """çµŒé¨“ã‚’è¿½åŠ """
        experience = Experience(state, action, reward, next_state, done)
        self.buffer.append(experience)
    
    def sample(self, batch_size):
        """ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = map(np.stack, zip(*batch))
        return state, action, reward, next_state, done
    
    def __len__(self):
        return len(self.buffer)

class DDPGAgent:
    """DDPGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self, state_dim=6, action_dim=1, lr_actor=1e-4, lr_critic=1e-3, gamma=0.99, tau=0.001):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.tau = tau
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆæœŸåŒ–
        self.actor = Actor(state_dim, action_dim).to(device)
        self.actor_target = Actor(state_dim, action_dim).to(device)
        self.critic = Critic(state_dim, action_dim).to(device)
        self.critic_target = Critic(state_dim, action_dim).to(device)
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®åˆæœŸåŒ–
        self.hard_update(self.actor_target, self.actor)
        self.hard_update(self.critic_target, self.critic)
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)
        
        # çµŒé¨“å†ç”Ÿãƒãƒƒãƒ•ã‚¡
        self.memory = ReplayBuffer()
        
        # ãƒã‚¤ã‚º
        self.noise = OUNoise(action_dim)
        
        # å­¦ç¿’çµ±è¨ˆ
        self.actor_loss_history = []
        self.critic_loss_history = []
        
    def hard_update(self, target, source):
        """ãƒãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆï¼ˆå®Œå…¨ã‚³ãƒ”ãƒ¼ï¼‰"""
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(param.data)
    
    def soft_update(self, target, source, tau):
        """ã‚½ãƒ•ãƒˆã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆï¼ˆå¾ã€…ã«æ›´æ–°ï¼‰"""
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)
    
    def select_action(self, state, add_noise=True):
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³é¸æŠ"""
        state = torch.FloatTensor(state.reshape(1, -1)).to(device)
        action = self.actor(state).cpu().data.numpy().flatten()
        
        if add_noise:
            action += self.noise.sample()
        
        return np.clip(action, -1.0, 1.0)
    
    def update(self, batch_size=64):
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ›´æ–°"""
        if len(self.memory) < batch_size:
            return
        
        # ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        state, action, reward, next_state, done = self.memory.sample(batch_size)
        
        state = torch.FloatTensor(state).to(device)
        action = torch.FloatTensor(action).to(device)
        reward = torch.FloatTensor(reward).to(device)
        next_state = torch.FloatTensor(next_state).to(device)
        done = torch.FloatTensor(done).to(device)
        
        # Criticã®æ›´æ–°
        next_action = self.actor_target(next_state)
        target_q = self.critic_target(next_state, next_action)
        target_q = reward + (self.gamma * target_q * (1 - done))
        
        current_q = self.critic(state, action)
        critic_loss = F.mse_loss(current_q, target_q.detach())
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()
        
        # Actorã®æ›´æ–°
        actor_loss = -self.critic(state, self.actor(state)).mean()
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ›´æ–°
        self.soft_update(self.actor_target, self.actor, self.tau)
        self.soft_update(self.critic_target, self.critic, self.tau)
        
        # çµ±è¨ˆè¨˜éŒ²
        self.actor_loss_history.append(actor_loss.item())
        self.critic_loss_history.append(critic_loss.item())

class GripForceEnvironment:
    """æŠŠæŒåŠ›ç’°å¢ƒï¼ˆçŠ¶æ…‹ç®¡ç†ãƒ»å ±é…¬è¨ˆç®—ï¼‰"""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        """ç’°å¢ƒãƒªã‚»ãƒƒãƒˆ"""
        self.episode_count = 0
        self.current_classification = None
        self.current_tcp_data = None
        self.previous_grip_force = 10.0
        self.success_count = 0
        self.total_episodes = 0
        
    def create_state(self, classification_result, tcp_data, previous_grip_force):
        """çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ä½œæˆ"""
        # åˆ†é¡çµæœã‚’ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        class_onehot = [0, 0, 0]
        if classification_result is not None:
            class_onehot[classification_result] = 1
        
        # TCP ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡æŠ½å‡º
        grip_force = tcp_data.get('grip_force', 0.0) / 30.0  # æ­£è¦åŒ–
        contact = 1.0 if tcp_data.get('contact', False) else 0.0
        broken = 1.0 if tcp_data.get('broken', False) else 0.0
        
        # çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ä½œæˆ [class_0, class_1, class_2, grip_force, contact, broken]
        state = np.array(class_onehot + [grip_force, contact, broken], dtype=np.float32)
        return state
    
    def calculate_reward(self, classification_result, tcp_data, action_grip_force):
        """å ±é…¬è¨ˆç®—"""
        reward = 0.0
        
        # åˆ†é¡çµæœã«åŸºã¥ãå ±é…¬
        if classification_result == 1:  # Success
            reward += 10.0
            self.success_count += 1
        elif classification_result == 0:  # UnderGrip
            reward -= 5.0
        elif classification_result == 2:  # OverGrip
            reward -= 8.0
        
        # æ¥è§¦æˆåŠŸå ±é…¬
        if tcp_data.get('contact', False):
            reward += 3.0
        
        # ç ´æãƒšãƒŠãƒ«ãƒ†ã‚£
        if tcp_data.get('broken', False):
            reward -= 15.0
        
        # æŠŠæŒåŠ›ã®é©åˆ‡æ€§ï¼ˆç›®æ¨™ç¯„å›²8-15Nï¼‰
        target_min, target_max = 8.0, 15.0
        actual_grip_force = action_grip_force * 15.0 + 15.0  # [-1,1] -> [0,30]N
        
        if target_min <= actual_grip_force <= target_max:
            reward += 2.0
        else:
            # ç¯„å›²å¤–ãƒšãƒŠãƒ«ãƒ†ã‚£
            distance = min(abs(actual_grip_force - target_min), abs(actual_grip_force - target_max))
            reward -= distance * 0.5
        
        return reward

class DDPGFeedbackSystem:
    """DDPGå¼·åŒ–å­¦ç¿’ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ çµ±åˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, 
                 classifier_model_path='models/best_grip_force_classifier.pth',
                 lsl_stream_name='MockEEG',
                 tcp_host='127.0.0.1',
                 tcp_port=12345):
        
        self.classifier_model_path = classifier_model_path
        self.lsl_stream_name = lsl_stream_name
        self.tcp_host = tcp_host
        self.tcp_port = tcp_port
        
        # ã‚µãƒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        self.init_classifier()
        self.init_data_collector()
        self.init_tcp_interface()
        
        # DDPGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
        self.agent = DDPGAgent()
        
        # ç’°å¢ƒ
        self.environment = GripForceEnvironment()
        
        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ç®¡ç†
        self.is_running = False
        self.episode_queue = queue.Queue()
        self.learning_thread = None
        
        # å­¦ç¿’çµ±è¨ˆ
        self.stats = {
            'total_episodes': 0,
            'successful_episodes': 0,
            'total_rewards': [],
            'classification_accuracy': [],
            'grip_force_history': [],
            'start_time': None
        }
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        self.model_save_dir = "models/ddpg_feedback"
        os.makedirs(self.model_save_dir, exist_ok=True)
        
        print(f"ğŸš€ DDPGå¼·åŒ–å­¦ç¿’ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def init_classifier(self):
        """åˆ†é¡æ©ŸåˆæœŸåŒ–"""
        try:
            if os.path.exists(self.classifier_model_path):
                self.classifier = RealtimeGripForceClassifier(
                    model_path=self.classifier_model_path,
                    lsl_stream_name=self.lsl_stream_name,
                    tcp_host=self.tcp_host,
                    tcp_port=self.tcp_port
                )
                print(f"âœ… åˆ†é¡æ©Ÿèª­ã¿è¾¼ã¿æˆåŠŸ: {self.classifier_model_path}")
            else:
                print(f"âš ï¸ åˆ†é¡æ©Ÿãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.classifier_model_path}")
                self.classifier = None
        except Exception as e:
            print(f"âŒ åˆ†é¡æ©ŸåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            self.classifier = None
    
    def init_data_collector(self):
        """ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        self.data_collector = LSLTCPEpisodeCollector(
            lsl_stream_name=self.lsl_stream_name,
            tcp_host=self.tcp_host,
            tcp_port=self.tcp_port,
            save_to_csv=True
        )
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def init_tcp_interface(self):
        """TCPé€šä¿¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹åˆæœŸåŒ–"""
        self.tcp_interface = EEGTCPInterface(
            host=self.tcp_host,
            port=self.tcp_port + 1  # åˆ¥ãƒãƒ¼ãƒˆã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¿œç­”
        )
        
        # æŠŠæŒåŠ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
        self.tcp_interface.add_message_callback(self.handle_grip_force_request)
        
        print(f"âœ… TCPé€šä¿¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹åˆæœŸåŒ–å®Œäº†")
    
    def handle_grip_force_request(self, message_data):
        """Unityã‹ã‚‰ã®æŠŠæŒåŠ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†"""
        try:
            if message_data.get('type') == 'grip_force_request':
                # ç¾åœ¨ã®çŠ¶æ…‹ã‚’å–å¾—ï¼ˆæœ€æ–°ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‹ã‚‰ï¼‰
                if hasattr(self, 'latest_state') and self.latest_state is not None:
                    # DDPGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‹ã‚‰ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å–å¾—
                    action = self.agent.select_action(self.latest_state, add_noise=False)
                    
                    # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠŠæŒåŠ›ã«å¤‰æ› [-1,1] -> [5,25]N
                    grip_force = (action[0] * 10.0) + 15.0
                    grip_force = np.clip(grip_force, 5.0, 25.0)
                    
                    print(f"ğŸ¯ DDPGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã‚‹æŠŠæŒåŠ›å¿œç­”: {grip_force:.2f}N")
                    
                    # TCPå¿œç­”é€ä¿¡
                    response = {
                        'type': 'grip_force_command',
                        'target_force': float(grip_force),
                        'timestamp': time.time(),
                        'session_id': f"ddpg_rl_{int(time.time())}"
                    }
                    self.tcp_interface.send_message(response)
                    
                    # çµ±è¨ˆæ›´æ–°
                    self.stats['grip_force_history'].append(grip_force)
                    
                else:
                    # çŠ¶æ…‹ãŒãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                    default_grip_force = 12.0
                    print(f"âš ï¸ çŠ¶æ…‹ãªã— - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæŠŠæŒåŠ›å¿œç­”: {default_grip_force}N")
                    
                    response = {
                        'type': 'grip_force_command',
                        'target_force': default_grip_force,
                        'timestamp': time.time(),
                        'session_id': f"ddpg_default_{int(time.time())}"
                    }
                    self.tcp_interface.send_message(response)
                    
        except Exception as e:
            print(f"âŒ æŠŠæŒåŠ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    
    def start_feedback_learning(self):
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å­¦ç¿’é–‹å§‹"""
        print(f"ğŸ”´ DDPGãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å­¦ç¿’é–‹å§‹")
        
        if not self.classifier:
            print(f"âŒ åˆ†é¡æ©ŸãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return False
        
        self.is_running = True
        self.stats['start_time'] = time.time()
        self.environment.reset()
        
        # ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹
        if not self.data_collector.start_collection():
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹å¤±æ•—")
            return False
        
        # TCPé€šä¿¡é–‹å§‹
        if not self.tcp_interface.start_server():
            print(f"âŒ TCPé€šä¿¡é–‹å§‹å¤±æ•—")
            return False
        
        # åˆ†é¡æ©Ÿé–‹å§‹
        if not self.classifier.start_classification():
            print(f"âŒ åˆ†é¡æ©Ÿé–‹å§‹å¤±æ•—")
            return False
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹
        self.learning_thread = threading.Thread(target=self._learning_loop, daemon=True)
        self.learning_thread.start()
        
        print(f"âœ… DDPGãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å­¦ç¿’é–‹å§‹å®Œäº†")
        print(f"ğŸ’¡ Unityå´ã§ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        return True
    
    def _learning_loop(self):
        """å­¦ç¿’ãƒ«ãƒ¼ãƒ—ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å®Ÿè¡Œï¼‰"""
        print(f"ğŸ”„ å­¦ç¿’ãƒ«ãƒ¼ãƒ—é–‹å§‹")
        
        previous_state = None
        previous_action = None
        
        while self.is_running:
            try:
                # æ–°ã—ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å¾…æ©Ÿ
                if len(self.data_collector.episodes) > self.stats['total_episodes']:
                    # æœ€æ–°ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å–å¾—
                    latest_episode = self.data_collector.episodes[-1]
                    
                    print(f"ğŸ†• æ–°ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å—ä¿¡: {latest_episode.episode_id}")
                    
                    # EEGåˆ†é¡å®Ÿè¡Œ
                    classification_result = self._classify_episode(latest_episode)
                    
                    # ç¾åœ¨ã®çŠ¶æ…‹ä½œæˆ
                    current_state = self.environment.create_state(
                        classification_result, 
                        latest_episode.tcp_data, 
                        self.environment.previous_grip_force
                    )
                    
                    # å‰ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãŒå­˜åœ¨ã™ã‚‹å ´åˆã€çµŒé¨“ã‚’è¿½åŠ 
                    if previous_state is not None and previous_action is not None:
                        reward = self.environment.calculate_reward(
                            classification_result, 
                            latest_episode.tcp_data, 
                            previous_action[0]
                        )
                        
                        # çµŒé¨“ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
                        done = latest_episode.tcp_data.get('broken', False)
                        self.agent.memory.push(
                            previous_state, 
                            previous_action, 
                            reward, 
                            current_state, 
                            done
                        )
                        
                        # çµ±è¨ˆæ›´æ–°
                        self.stats['total_rewards'].append(reward)
                        if classification_result == 1:  # Success
                            self.stats['successful_episodes'] += 1
                        
                        print(f"ğŸ“ˆ å ±é…¬: {reward:.2f}, åˆ†é¡: {classification_result}")
                        
                        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ›´æ–°
                        if len(self.agent.memory) >= 64:
                            self.agent.update()
                    
                    # æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³é¸æŠ
                    current_action = self.agent.select_action(current_state)
                    
                    # çŠ¶æ…‹æ›´æ–°
                    previous_state = current_state
                    previous_action = current_action
                    self.latest_state = current_state
                    self.stats['total_episodes'] += 1
                    
                    # å®šæœŸçš„ãªãƒ¢ãƒ‡ãƒ«ä¿å­˜
                    if self.stats['total_episodes'] % 50 == 0:
                        self.save_model()
                
                time.sleep(0.1)  # 100mså¾…æ©Ÿ
                
            except Exception as e:
                print(f"âŒ å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
                time.sleep(1.0)
        
        print(f"ğŸ”„ å­¦ç¿’ãƒ«ãƒ¼ãƒ—çµ‚äº†")
    
    def _classify_episode(self, episode: Episode) -> Optional[int]:
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®EEGãƒ‡ãƒ¼ã‚¿ã‚’åˆ†é¡"""
        try:
            if not self.classifier:
                return None
            
            # EEGãƒ‡ãƒ¼ã‚¿ã‚’åˆ†é¡æ©Ÿã®å…¥åŠ›å½¢å¼ã«å¤‰æ›
            eeg_data = episode.lsl_data  # (300, 32)
            
            # åˆ†é¡å®Ÿè¡Œï¼ˆgrip_force_classifierã®é–¢æ•°ã‚’ä½¿ç”¨ï¼‰
            result = self.classifier._classify_eeg_data(eeg_data)
            
            if result:
                classification = result.get('predicted_class', None)
                confidence = result.get('confidence', 0.0)
                
                print(f"ğŸ§  EEGåˆ†é¡çµæœ: ã‚¯ãƒ©ã‚¹{classification}, ä¿¡é ¼åº¦{confidence:.3f}")
                return classification
            
        except Exception as e:
            print(f"âŒ EEGåˆ†é¡ã‚¨ãƒ©ãƒ¼: {e}")
        
        return None
    
    def save_model(self, filepath=None):
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        if filepath is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = os.path.join(self.model_save_dir, f"ddpg_model_{timestamp}.pth")
        
        try:
            torch.save({
                'actor_state_dict': self.agent.actor.state_dict(),
                'critic_state_dict': self.agent.critic.state_dict(),
                'actor_optimizer': self.agent.actor_optimizer.state_dict(),
                'critic_optimizer': self.agent.critic_optimizer.state_dict(),
                'stats': self.stats,
                'episodes': self.stats['total_episodes']
            }, filepath)
            
            print(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {filepath}")
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def load_model(self, filepath):
        """ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        try:
            checkpoint = torch.load(filepath)
            
            self.agent.actor.load_state_dict(checkpoint['actor_state_dict'])
            self.agent.critic.load_state_dict(checkpoint['critic_state_dict'])
            self.agent.actor_optimizer.load_state_dict(checkpoint['actor_optimizer'])
            self.agent.critic_optimizer.load_state_dict(checkpoint['critic_optimizer'])
            
            if 'stats' in checkpoint:
                self.stats.update(checkpoint['stats'])
            
            print(f"ğŸ“‚ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {filepath}")
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def stop_learning(self):
        """å­¦ç¿’åœæ­¢"""
        print(f"â¹ï¸ DDPGãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å­¦ç¿’åœæ­¢ä¸­...")
        
        self.is_running = False
        
        # å„ã‚µãƒ–ã‚·ã‚¹ãƒ†ãƒ åœæ­¢
        if self.classifier:
            self.classifier.stop_classification()
        
        if self.data_collector:
            self.data_collector.stop_collection()
        
        if self.tcp_interface:
            self.tcp_interface.stop_server()
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        self.save_model()
        
        print(f"âœ… DDPGãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å­¦ç¿’åœæ­¢å®Œäº†")
    
    def print_stats(self):
        """å­¦ç¿’çµ±è¨ˆè¡¨ç¤º"""
        print(f"\nğŸ“Š DDPGå­¦ç¿’çµ±è¨ˆ:")
        print(f"   ç·ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°     : {self.stats['total_episodes']}")
        print(f"   æˆåŠŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°   : {self.stats['successful_episodes']}")
        
        if self.stats['total_episodes'] > 0:
            success_rate = self.stats['successful_episodes'] / self.stats['total_episodes'] * 100
            print(f"   æˆåŠŸç‡           : {success_rate:.1f}%")
        
        if self.stats['total_rewards']:
            avg_reward = np.mean(self.stats['total_rewards'][-100:])  # æœ€æ–°100ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®å¹³å‡
            print(f"   å¹³å‡å ±é…¬ï¼ˆæœ€æ–°100ï¼‰: {avg_reward:.2f}")
        
        if self.stats['grip_force_history']:
            avg_grip_force = np.mean(self.stats['grip_force_history'][-100:])
            print(f"   å¹³å‡æŠŠæŒåŠ›ï¼ˆæœ€æ–°100ï¼‰: {avg_grip_force:.2f}N")
        
        if self.stats['start_time']:
            uptime = time.time() - self.stats['start_time']
            print(f"   ç¨¼åƒæ™‚é–“         : {uptime:.1f}ç§’")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print(f"ğŸ§  DDPGå¼·åŒ–å­¦ç¿’ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ ")
    print(f"=" * 60)
    print(f"1. äº‹å‰æº–å‚™: grip_force_classifier.py ã§åˆ†é¡æ©Ÿã‚’å­¦ç¿’")
    print(f"2. LSL/TCPãƒ‡ãƒ¼ã‚¿å—ä¿¡ & EEGåˆ†é¡")
    print(f"3. DDPGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã‚‹æŠŠæŒåŠ›æœ€é©åŒ–")
    print(f"4. Unity TCP ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¸ã®å­¦ç¿’æ¸ˆã¿å¿œç­”")
    print(f"=" * 60)
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    system = DDPGFeedbackSystem(
        classifier_model_path='models/best_grip_force_classifier.pth',
        lsl_stream_name='MockEEG',
        tcp_host='127.0.0.1',
        tcp_port=12345
    )
    
    # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å­¦ç¿’é–‹å§‹
    if system.start_feedback_learning():
        try:
            print(f"\nğŸ’¡ ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒä¸­...")
            print(f"   Unityå´ã§ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
            print(f"   æŠŠæŒåŠ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«è‡ªå‹•å¿œç­”ã—ã¾ã™")
            print(f"   Ctrl+C ã§çµ‚äº†")
            
            while True:
                time.sleep(5.0)
                system.print_stats()
                
        except KeyboardInterrupt:
            print(f"\nâ¹ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼åœæ­¢")
        finally:
            system.stop_learning()
    else:
        print(f"âŒ ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹å¤±æ•—")

if __name__ == "__main__":
    main()


# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆç”¨ã®è¿½åŠ æ©Ÿèƒ½

class DDPGFeedbackSystemTester:
    """DDPGå¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆãƒ»ãƒ‡ãƒãƒƒã‚°ç”¨ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, system: DDPGFeedbackSystem):
        self.system = system
    
    def test_classifier_only(self, csv_dir="DDPG_Python/logs/episodes_latest"):
        """åˆ†é¡æ©Ÿã®ã¿ãƒ†ã‚¹ãƒˆ"""
        print(f"ğŸ§ª åˆ†é¡æ©Ÿå˜ä½“ãƒ†ã‚¹ãƒˆ")
        
        if not self.system.classifier:
            print(f"âŒ åˆ†é¡æ©ŸãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return False
        
        try:
            # ä¿å­˜ã•ã‚ŒãŸCSVãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            from grip_force_classifier import load_csv_data
            eeg_data_list, grip_force_labels = load_csv_data(csv_dir)
            
            if not eeg_data_list:
                print(f"âŒ ãƒ†ã‚¹ãƒˆç”¨CSVãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_dir}")
                return False
            
            print(f"ğŸ“‚ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(eeg_data_list)}ä»¶")
            
            # å„ãƒ‡ãƒ¼ã‚¿ã§åˆ†é¡ãƒ†ã‚¹ãƒˆ
            correct_predictions = 0
            total_predictions = len(eeg_data_list)
            
            for i, (eeg_data, true_label) in enumerate(zip(eeg_data_list, grip_force_labels)):
                result = self.system._classify_episode_data(eeg_data)
                predicted_label = result if result is not None else -1
                
                if predicted_label == true_label:
                    correct_predictions += 1
                
                if i < 5:  # æœ€åˆã®5ä»¶ã‚’è©³ç´°è¡¨ç¤º
                    print(f"   ãƒ†ã‚¹ãƒˆ{i+1}: çœŸå€¤={true_label}, äºˆæ¸¬={predicted_label}")
            
            accuracy = correct_predictions / total_predictions * 100
            print(f"âœ… åˆ†é¡ç²¾åº¦: {accuracy:.1f}% ({correct_predictions}/{total_predictions})")
            
            return accuracy > 50  # 50%ä»¥ä¸Šã§åˆæ ¼
            
        except Exception as e:
            print(f"âŒ åˆ†é¡æ©Ÿãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def test_ddpg_agent(self, num_episodes=10):
        """DDPGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å‹•ä½œãƒ†ã‚¹ãƒˆ"""
        print(f"ğŸ§ª DDPGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå˜ä½“ãƒ†ã‚¹ãƒˆ ({num_episodes}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰)")
        
        try:
            # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ†ã‚¹ãƒˆ
            for episode in range(num_episodes):
                # ãƒ©ãƒ³ãƒ€ãƒ çŠ¶æ…‹ä½œæˆ
                state = np.random.rand(6).astype(np.float32)
                
                # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³é¸æŠ
                action = self.system.agent.select_action(state)
                
                # æŠŠæŒåŠ›å¤‰æ›
                grip_force = (action[0] * 10.0) + 15.0
                grip_force = np.clip(grip_force, 5.0, 25.0)
                
                # ãƒ€ãƒŸãƒ¼å ±é…¬
                reward = np.random.uniform(-5.0, 10.0)
                
                # æ¬¡çŠ¶æ…‹
                next_state = np.random.rand(6).astype(np.float32)
                done = np.random.choice([True, False], p=[0.1, 0.9])
                
                # çµŒé¨“è¿½åŠ 
                self.system.agent.memory.push(state, action, reward, next_state, done)
                
                print(f"   ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰{episode+1}: æŠŠæŒåŠ›={grip_force:.2f}N, å ±é…¬={reward:.2f}")
                
                # å­¦ç¿’æ›´æ–°ï¼ˆååˆ†ãªçµŒé¨“ãŒã‚ã‚‹å ´åˆï¼‰
                if len(self.system.agent.memory) >= 64:
                    self.system.agent.update()
            
            print(f"âœ… DDPGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ†ã‚¹ãƒˆå®Œäº†")
            print(f"   çµŒé¨“ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º: {len(self.system.agent.memory)}")
            
            return True
            
        except Exception as e:
            print(f"âŒ DDPGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def test_tcp_communication(self, test_duration=10):
        """TCPé€šä¿¡ãƒ†ã‚¹ãƒˆ"""
        print(f"ğŸ§ª TCPé€šä¿¡ãƒ†ã‚¹ãƒˆ ({test_duration}ç§’)")
        
        try:
            # TCP ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹é–‹å§‹
            if not self.system.tcp_interface.start_server():
                print(f"âŒ TCPã‚µãƒ¼ãƒãƒ¼é–‹å§‹å¤±æ•—")
                return False
            
            print(f"ğŸ”— TCPã‚µãƒ¼ãƒãƒ¼å¾…æ©Ÿä¸­...")
            print(f"   å¤–éƒ¨ã‹ã‚‰æ¥ç¶šã—ã¦ãƒ†ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡ã—ã¦ãã ã•ã„")
            
            start_time = time.time()
            initial_message_count = self.system.tcp_interface.stats['messages_received']
            
            while time.time() - start_time < test_duration:
                current_message_count = self.system.tcp_interface.stats['messages_received']
                
                if current_message_count > initial_message_count:
                    print(f"ğŸ“¥ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å—ä¿¡: {current_message_count - initial_message_count}ä»¶")
                
                time.sleep(1.0)
            
            final_message_count = self.system.tcp_interface.stats['messages_received']
            total_received = final_message_count - initial_message_count
            
            print(f"âœ… TCPé€šä¿¡ãƒ†ã‚¹ãƒˆå®Œäº†")
            print(f"   å—ä¿¡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°: {total_received}ä»¶")
            
            self.system.tcp_interface.stop_server()
            return total_received >= 0  # 0ä»¶ä»¥ä¸Šã§åˆæ ¼ï¼ˆæ¥ç¶šãŒãªãã¦ã‚‚å•é¡Œãªã—ï¼‰
            
        except Exception as e:
            print(f"âŒ TCPé€šä¿¡ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def run_all_tests(self):
        """å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        print(f"ğŸ§ª DDPGå¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ  å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
        print(f"=" * 50)
        
        results = {}
        
        # 1. åˆ†é¡æ©Ÿãƒ†ã‚¹ãƒˆ
        results['classifier'] = self.test_classifier_only()
        print()
        
        # 2. DDPGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ†ã‚¹ãƒˆ
        results['ddpg'] = self.test_ddpg_agent()
        print()
        
        # 3. TCPé€šä¿¡ãƒ†ã‚¹ãƒˆ
        results['tcp'] = self.test_tcp_communication()
        print()
        
        # çµæœã‚µãƒãƒªãƒ¼
        print(f"ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼:")
        for test_name, result in results.items():
            status = "âœ… åˆæ ¼" if result else "âŒ ä¸åˆæ ¼"
            print(f"   {test_name:12}: {status}")
        
        all_passed = all(results.values())
        print(f"\nğŸ¯ ç·åˆçµæœ: {'âœ… å…¨ãƒ†ã‚¹ãƒˆåˆæ ¼' if all_passed else 'âŒ ä¸€éƒ¨ãƒ†ã‚¹ãƒˆä¸åˆæ ¼'}")
        
        return all_passed


def run_comprehensive_demo():
    """åŒ…æ‹¬çš„ãªãƒ‡ãƒ¢å®Ÿè¡Œ"""
    print(f"ğŸš€ DDPGå¼·åŒ–å­¦ç¿’ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ  åŒ…æ‹¬ãƒ‡ãƒ¢")
    print(f"=" * 60)
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    system = DDPGFeedbackSystem(
        classifier_model_path='models/best_grip_force_classifier.pth',
        lsl_stream_name='MockEEG',
        tcp_host='127.0.0.1',
        tcp_port=12345
    )
    
    # ãƒ†ã‚¹ã‚¿ãƒ¼åˆæœŸåŒ–
    tester = DDPGFeedbackSystemTester(system)
    
    print(f"Phase 1: ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ")
    print(f"-" * 30)
    
    # å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test_success = tester.run_all_tests()
    
    if not test_success:
        print(f"âš ï¸ ä¸€éƒ¨ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸã€‚ç¶šè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/n)")
        user_input = input().strip().lower()
        if user_input != 'y':
            print(f"âŒ ãƒ‡ãƒ¢ä¸­æ­¢")
            return
    
    print(f"\nPhase 2: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å­¦ç¿’ãƒ‡ãƒ¢")
    print(f"-" * 30)
    
    # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å­¦ç¿’é–‹å§‹
    if system.start_feedback_learning():
        try:
            print(f"\nğŸ’¡ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒä¸­...")
            print(f"   Unityå´ã§ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
            print(f"   a2cClient.SendGripForceRequest() ã§æŠŠæŒåŠ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¯èƒ½")
            print(f"   EPISODE_ENDãƒˆãƒªã‚¬ãƒ¼ã§å­¦ç¿’å®Ÿè¡Œ")
            print(f"   'q' + Enter ã§çµ‚äº†")
            
            # éãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°å…¥åŠ›å¾…æ©Ÿ
            import select
            import sys
            
            while True:
                # çµ±è¨ˆè¡¨ç¤ºï¼ˆ5ç§’ã”ã¨ï¼‰
                system.print_stats()
                
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ãƒã‚§ãƒƒã‚¯
                if sys.stdin in select.select([sys.stdin], [], [], 0)[0]:
                    user_input = sys.stdin.readline().strip()
                    if user_input.lower() == 'q':
                        break
                
                time.sleep(5.0)
                
        except KeyboardInterrupt:
            print(f"\nâ¹ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼åœæ­¢")
        except Exception as e:
            print(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        finally:
            system.stop_learning()
    else:
        print(f"âŒ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å­¦ç¿’é–‹å§‹å¤±æ•—")
    
    print(f"\nğŸ¯ ãƒ‡ãƒ¢å®Œäº†")


def run_training_mode():
    """å­¦ç¿’å°‚ç”¨ãƒ¢ãƒ¼ãƒ‰"""
    print(f"ğŸ“ DDPGå­¦ç¿’å°‚ç”¨ãƒ¢ãƒ¼ãƒ‰")
    print(f"=" * 40)
    
    # æ—¢å­˜ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    model_files = []
    model_dir = "models/ddpg_feedback"
    if os.path.exists(model_dir):
        model_files = [f for f in os.listdir(model_dir) if f.endswith('.pth')]
    
    system = DDPGFeedbackSystem()
    
    # æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    if model_files:
        print(f"ğŸ“‚ æ—¢å­˜ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ç™ºè¦‹:")
        for i, model_file in enumerate(model_files):
            print(f"   {i+1}. {model_file}")
        
        print(f"   0. æ–°è¦å­¦ç¿’é–‹å§‹")
        
        try:
            choice = int(input(f"é¸æŠ (0-{len(model_files)}): "))
            
            if 1 <= choice <= len(model_files):
                model_path = os.path.join(model_dir, model_files[choice-1])
                if system.load_model(model_path):
                    print(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {model_files[choice-1]}")
                else:
                    print(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—ã€æ–°è¦å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™")
        except (ValueError, IndexError):
            print(f"âš ï¸ ç„¡åŠ¹ãªé¸æŠã€æ–°è¦å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™")
    
    # å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    print(f"\nâš™ï¸ å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š:")
    try:
        target_episodes = int(input(f"ç›®æ¨™ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1000): ") or "1000")
        save_interval = int(input(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜é–“éš” (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 50): ") or "50")
    except ValueError:
        target_episodes = 1000
        save_interval = 50
        print(f"âš ï¸ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨: {target_episodes}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰, {save_interval}é–“éš”ä¿å­˜")
    
    # å­¦ç¿’é–‹å§‹
    if system.start_feedback_learning():
        try:
            print(f"\nğŸ“ å­¦ç¿’é–‹å§‹ (ç›®æ¨™: {target_episodes}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰)")
            print(f"ğŸ’¡ Unityå´ã§ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
            print(f"   é€²æ—ã¯ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§è¡¨ç¤ºã•ã‚Œã¾ã™")
            print(f"   Ctrl+C ã§æ—©æœŸçµ‚äº†")
            
            while system.stats['total_episodes'] < target_episodes:
                time.sleep(10.0)  # 10ç§’ã”ã¨ã«çµ±è¨ˆè¡¨ç¤º
                system.print_stats()
                
                # ä¿å­˜é–“éš”ãƒã‚§ãƒƒã‚¯
                if (system.stats['total_episodes'] % save_interval == 0 and 
                    system.stats['total_episodes'] > 0):
                    system.save_model()
                    print(f"ğŸ’¾ ä¸­é–“ä¿å­˜å®Œäº† (ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ {system.stats['total_episodes']})")
            
            print(f"\nğŸ‰ ç›®æ¨™ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°é”æˆ!")
            
        except KeyboardInterrupt:
            print(f"\nâ¹ï¸ å­¦ç¿’æ—©æœŸçµ‚äº†")
        finally:
            system.stop_learning()
            print(f"ğŸ“Š æœ€çµ‚çµ±è¨ˆ:")
            system.print_stats()
    else:
        print(f"âŒ å­¦ç¿’é–‹å§‹å¤±æ•—")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        mode = sys.argv[1].lower()
        
        if mode == "test":
            # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰
            system = DDPGFeedbackSystem()
            tester = DDPGFeedbackSystemTester(system)
            tester.run_all_tests()
            
        elif mode == "demo":
            # ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰
            run_comprehensive_demo()
            
        elif mode == "train":
            # å­¦ç¿’å°‚ç”¨ãƒ¢ãƒ¼ãƒ‰
            run_training_mode()
            
        else:
            print(f"âŒ ç„¡åŠ¹ãªãƒ¢ãƒ¼ãƒ‰: {mode}")
            print(f"ä½¿ç”¨å¯èƒ½ãªãƒ¢ãƒ¼ãƒ‰: test, demo, train")
            print(f"ä¾‹: python ddpg_feedback_system.py demo")
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: é€šå¸¸å®Ÿè¡Œ
        main()